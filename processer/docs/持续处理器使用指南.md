# 持续处理器使用指南

## 📚 概述

持续处理器是数据管道的核心组件，负责：
- 从 Redis DB1 读取 Cleaner 清洗后的数据
- 进行词频分析、热词提取、情感分析
- 生成可视化所需的各种数据格式
- 将结果发布到 Redis DB2 供前端使用

## 🏗️ 架构

```
┌─────────┐    ┌─────────┐    ┌──────────┐    ┌──────────────┐
│ Scraper │───>│ Cleaner │───>│Processor │───>│Visualization │
└─────────┘    └─────────┘    └──────────┘    └──────────────┘
                   │                │                 │
                   v                v                 v
              DB1: queue      Process Data       DB2: keys
```

### 数据流详情

**输入 (DB1)**
- 队列名: `clean_data_queue`
- 数据库: Redis DB1
- 数据格式: JSON (清洗后的新闻数据)

**输出 (DB2)**
- 键前缀: `processed_data:*`
- 数据库: Redis DB2
- 输出键:
  - `processed_data:trending_keywords` - 热词排行
  - `processed_data:word_cloud` - 词云数据
  - `processed_data:history_data` - 24小时趋势数据
  - `processed_data:news_feed` - 新闻流
  - `processed_data:metadata` - 元数据 (包括新闻来源统计)

## 🚀 快速开始

### 1. 测试 Redis 连接

运行测试脚本，检查 Redis 连接和数据状态：

```cmd
test_redis.bat
```

或直接运行 Python 脚本:

```cmd
python test_pipeline.py
```

测试会显示：
- ✅ Redis DB1/DB2 连接状态
- 📥 输入队列中的数据量
- 📤 输出键的存在情况
- 💡 下一步建议

### 2. 单次执行（测试）

如果队列中有数据，可以先执行一次看看效果：

```cmd
run_once.bat
```

或使用命令:

```cmd
cd processer\Analysis
python continuous_processor.py --once
```

这会：
- 从队列读取数据
- 执行完整的处理流程
- 输出结果到 Redis DB2
- 保存结果到本地 `output_data.json`

### 3. 启动持续服务

启动后将持续运行，每隔一定时间自动处理新数据：

```cmd
start_processor.bat
```

或使用命令:

```cmd
cd processer\Analysis
python continuous_processor.py
```

**默认配置**:
- 处理间隔: 300 秒 (5 分钟)
- 自动检测队列中的新数据
- 按 `Ctrl+C` 停止服务

### 4. 自定义处理间隔

```cmd
# 每 60 秒处理一次
python continuous_processor.py --interval 60

# 每 10 分钟处理一次
python continuous_processor.py --interval 600
```

## ⚙️ 配置

配置文件: `processer/Analysis/config.py`

### 核心配置项

```python
CONFIG = {
    # 处理间隔（秒）
    "process_interval_seconds": 300,
    
    # Redis 配置
    "redis": {
        "host": "localhost",
        "port": 6379,
        "input_db": 1,              # 输入数据库
        "output_db": 2,             # 输出数据库
        "input_queue": "clean_data_queue",    # 输入队列名
        "output_prefix": "processed_data",    # 输出键前缀
        "key_ttl_seconds": 86400,   # 键过期时间（24小时）
    },
    
    # 分析参数
    "trending_keywords_count": 10,  # 热词数量
    "word_cloud_count": 20,         # 词云词数
    "history_hours": 24,            # 历史数据小时数
    "history_interval_minutes": 30, # 历史数据采样间隔
}
```

## 📊 运行状态监控

### 查看运行日志

服务运行时会显示详细的处理日志：

```
🔄 开始处理任务 - 2025-01-15 10:30:00
============================================================
📥 队列中有 1234 条数据待处理
✅ 成功加载 1234 条数据
✓ 当前窗口数据: 567 条
✓ 历史窗口数据: 667 条

🔍 执行文本分析...
📊 生成热词排行榜...
☁️  生成词云数据...
📈 生成历史数据...
📰 生成新闻流...
📊 统计新闻来源分布...

💾 生成输出数据...
✅ 本地文件已更新: output_data.json
📤 发布到 Redis...
✅ 数据已成功发布到 Redis

✨ 处理完成！
============================================================
⏳ 下次处理时间: 10:35:00
```

### 检查 Redis 数据

使用测试脚本查看 Redis 中的数据状态：

```cmd
test_redis.bat
```

## 🔧 故障排除

### 问题 1: Redis 连接失败

**症状**: 
```
❌ Redis连接失败，无法启动服务
```

**解决方法**:
1. 确认 Redis 服务已启动
2. 在 scraper 目录运行: `start_redis.bat`
3. 或手动启动 Redis

### 问题 2: 队列无数据

**症状**:
```
ℹ️  队列中暂无新数据
⚠️  无可处理数据，跳过本次处理
```

**解决方法**:
1. 确认 Scraper 正在运行并抓取数据
2. 确认 Cleaner 正在运行并清洗数据
3. 检查 Cleaner 是否正确推送到 `clean_data_queue`

### 问题 3: 处理出错

**症状**:
```
❌ 处理过程中发生错误: ...
```

**解决方法**:
1. 查看完整的错误堆栈
2. 检查数据格式是否正确
3. 确认配置文件中的参数正确
4. 检查本地文件是否有写入权限

### 问题 4: 数据未发布到 Redis

**症状**:
```
⚠️  数据发布到 Redis 失败
```

**解决方法**:
1. 确认 Redis DB2 连接正常
2. 检查 Redis 内存是否充足
3. 查看 `redis_manager.py` 中的发布逻辑

## 🔄 完整数据管道启动顺序

1. **启动 Redis** (在 scraper 目录)
   ```cmd
   cd scraper
   start_redis.bat
   ```

2. **启动 Scraper** (抓取数据)
   ```cmd
   cd scraper
   run.bat
   ```

3. **启动 Cleaner** (清洗数据)
   ```cmd
   cd cleaner
   python data_cleaner_module.py
   ```

4. **启动 Processor** (处理数据) ← **本服务**
   ```cmd
   cd processer
   start_processor.bat
   ```

5. **启动 Visualization** (可视化)
   ```cmd
   cd visualization
   启动前后端服务...
   ```

## 📝 命令参考

### 测试相关
```cmd
# 测试 Redis 连接和数据流
test_redis.bat
python test_pipeline.py

# 单次执行测试
run_once.bat
python continuous_processor.py --once
```

### 服务启动
```cmd
# 使用默认配置启动
start_processor.bat
python continuous_processor.py

# 自定义处理间隔
python continuous_processor.py --interval 60
python continuous_processor.py --interval 600
```

### 停止服务
- 按 `Ctrl+C` 优雅停止
- 服务会在当前处理完成后退出

## 📈 性能建议

1. **处理间隔**: 根据数据量调整
   - 数据量大: 缩短间隔 (60-120秒)
   - 数据量小: 延长间隔 (300-600秒)

2. **批处理大小**: 修改 `load_data_from_redis()` 中的 `batch_size`
   - 默认: 1000 条/批
   - 数据量特别大时可增加

3. **Redis 内存**: 确保足够的内存
   - 监控 Redis 内存使用
   - 必要时增加 TTL 时间

4. **日志级别**: 生产环境可减少详细输出

## 🆘 获取帮助

运行时遇到问题:
1. 查看详细的错误堆栈信息
2. 运行 `test_pipeline.py` 诊断问题
3. 检查各模块的日志文件
4. 确认所有依赖服务正常运行
