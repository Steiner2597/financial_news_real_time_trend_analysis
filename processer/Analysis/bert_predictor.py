"""
BERT æƒ…æ„Ÿé¢„æµ‹å™¨
ä¸ºç¼ºå¤± sentiment çš„æ•°æ®æä¾›è‡ªåŠ¨é¢„æµ‹
"""
import os
import sys
import json
from pathlib import Path
import pandas as pd
import numpy as np

# æ ‡è®°æ˜¯å¦å¯ç”¨
BERT_AVAILABLE = False
model = None
tokenizer = None
device = None
reverse_label_map = None

try:
    import torch
    from torch.utils.data import Dataset, DataLoader
    from transformers import BertTokenizer, BertForSequenceClassification
    BERT_AVAILABLE = True
except ImportError:
    print("âš ï¸  è­¦å‘Š: PyTorch/Transformers æœªå®‰è£…ï¼ŒBERT é¢„æµ‹åŠŸèƒ½ä¸å¯ç”¨")
    print("   å°†ä½¿ç”¨ç®€å•è§„åˆ™è¿›è¡Œ sentiment å¡«å……")

# å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…å¾ªç¯ä¾èµ–
try:
    from sentiment_updater import SentimentUpdater
    UPDATER_AVAILABLE = True
except ImportError:
    UPDATER_AVAILABLE = False


class BertPredictor:
    """BERT æƒ…æ„Ÿé¢„æµ‹å™¨"""
    
    def __init__(self, model_path=None, max_len=256, batch_size=32):
        """
        åˆå§‹åŒ–é¢„æµ‹å™¨
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º ../Bert_Model/best_model.pth
            max_len: æœ€å¤§åºåˆ—é•¿åº¦
            batch_size: æ‰¹å¤„ç†å¤§å°ï¼ˆé»˜è®¤32ï¼Œå¯è°ƒæ•´ä»¥æé«˜æ€§èƒ½ï¼‰
        """
        self.max_len = max_len
        self.batch_size = batch_size  # âœ… æ”¹ä¸ºæ›´å¤§çš„é»˜è®¤å€¼ä»¥åŠ é€Ÿ
        self.model_loaded = False
        
        if not BERT_AVAILABLE:
            print("âš ï¸  BERT ä¾èµ–ä¸å¯ç”¨ï¼Œé¢„æµ‹å™¨å°†ä½¿ç”¨ç®€å•è§„åˆ™")
            return
        
        # æŸ¥æ‰¾æ¨¡å‹è·¯å¾„
        if model_path is None:
            model_path = self._find_model_path()
        
        if model_path is None or not os.path.exists(model_path):
            print(f"âš ï¸  è­¦å‘Š: æ‰¾ä¸åˆ° BERT æ¨¡å‹æ–‡ä»¶ï¼Œå°†ä½¿ç”¨ç®€å•è§„åˆ™")
            print(f"   æœŸæœ›è·¯å¾„: {model_path}")
            return
        
        # åŠ è½½æ¨¡å‹
        try:
            self._load_model(model_path)
            self.model_loaded = True
            print(f"âœ… BERT æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
        except Exception as e:
            print(f"âš ï¸  è­¦å‘Š: BERT æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("   å°†ä½¿ç”¨ç®€å•è§„åˆ™è¿›è¡Œ sentiment å¡«å……")
    
    def _find_model_path(self):
        """æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶"""
        current_dir = Path(__file__).parent
        
        # å°è¯•å¤šä¸ªå¯èƒ½çš„è·¯å¾„
        possible_paths = [
            current_dir / '..' / 'Bert_Model' / 'best_model.pth',
            current_dir / '..' / '..' / 'Bert_Model' / 'best_model.pth',
        ]
        
        for path in possible_paths:
            if path.exists():
                return str(path.resolve())
        
        return None
    
    def _load_model(self, model_path):
        """åŠ è½½æ¨¡å‹"""
        global model, tokenizer, device, reverse_label_map
        
        # è®¾å¤‡é€‰æ‹©
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint = torch.load(model_path, map_location=device, weights_only=False)
        
        # åŠ è½½ tokenizer å’Œæ ‡ç­¾æ˜ å°„
        tokenizer = checkpoint['tokenizer']
        label_map = checkpoint['label_map']
        config = checkpoint['config']
        
        # åˆ›å»ºæ¨¡å‹
        model = BertForSequenceClassification.from_pretrained(
            'bert-base-uncased',
            num_labels=config['num_labels'],
            output_attentions=False,
            output_hidden_states=False
        )
        model.load_state_dict(checkpoint['model_state_dict'])
        model.to(device)
        model.eval()
        
        # åå‘æ ‡ç­¾æ˜ å°„
        reverse_label_map = {v: k for k, v in label_map.items()}
    
    def predict_batch(self, texts):
        """
        æ‰¹é‡é¢„æµ‹æ–‡æœ¬çš„æƒ…æ„Ÿï¼ˆä¼˜åŒ–ç‰ˆï¼šçœŸæ­£çš„æ‰¹å¤„ç†ï¼‰
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            list: é¢„æµ‹çš„æƒ…æ„Ÿæ ‡ç­¾åˆ—è¡¨ (Bullish/Bearish)
        """
        if not self.model_loaded:
            # ä½¿ç”¨ç®€å•è§„åˆ™ï¼ˆæ‰¹å¤„ç†ï¼‰
            return [self._simple_sentiment(text) for text in texts]
        
        if not texts:
            return []
        
        try:
            import time
            start_time = time.time()
            
            # âœ… åˆ›å»ºæ•°æ®é›†å’ŒåŠ è½½å™¨
            dataset = PredictionDataset(texts, tokenizer, self.max_len)
            data_loader = DataLoader(
                dataset, 
                batch_size=self.batch_size,
                shuffle=False,
                num_workers=0,  # CPU æ¨ç†æ—¶ä¸éœ€è¦å¤šè¿›ç¨‹
                collate_fn=collate_fn_for_prediction,  # âœ… ä¼˜åŒ–2ï¼šä½¿ç”¨è‡ªå®šä¹‰ collate å‡½æ•°
                pin_memory=True if str(device) != 'cpu' else False  # âœ… GPU ä¼˜åŒ–
            )
            
            # é¢„æµ‹
            predictions = []
            batch_count = 0
            
            with torch.no_grad():
                for batch in data_loader:
                    batch_count += 1
                    input_ids = batch['input_ids'].to(device)
                    attention_mask = batch['attention_mask'].to(device)
                    
                    # âœ… å‘å‰ä¼ é€’
                    outputs = model(input_ids=input_ids, attention_mask=attention_mask)
                    logits = outputs.logits
                    
                    _, preds = torch.max(logits, dim=1)
                    predictions.extend(preds.cpu().tolist())
                    
                    # âœ… æ˜¾ç¤ºè¿›åº¦
                    processed = min(batch_count * self.batch_size, len(texts))
                    if batch_count % max(1, len(data_loader) // 5) == 0:
                        print(f"  â³ é¢„æµ‹è¿›åº¦: {processed}/{len(texts)}")
            
            # âœ… æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡
            elapsed_time = time.time() - start_time
            speed = len(texts) / elapsed_time
            print(f"  âœ… é¢„æµ‹å®Œæˆ: {len(texts)} æ¡æ–‡æœ¬, è€—æ—¶ {elapsed_time:.2f}s, é€Ÿåº¦ {speed:.1f} æ¡/ç§’")
            
            # è½¬æ¢ä¸ºæ ‡ç­¾
            return [reverse_label_map[p] for p in predictions]
            
        except Exception as e:
            print(f"âš ï¸  BERT é¢„æµ‹å¤±è´¥: {e}ï¼Œä½¿ç”¨ç®€å•è§„åˆ™")
            import traceback
            traceback.print_exc()
            return [self._simple_sentiment(text) for text in texts]
    
    def _simple_sentiment(self, text):
        """
        ç®€å•çš„å¯å‘å¼æƒ…æ„Ÿåˆ¤æ–­ï¼ˆåå¤‡æ–¹æ¡ˆï¼‰
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            str: 'Bullish' æˆ– 'Bearish' æˆ– ''
        """
        if not text or not isinstance(text, str):
            return ""
        
        text_lower = text.lower()
        
        # çœ‹æ¶¨å…³é”®è¯
        bullish_words = ['bull', 'bullish', 'long', 'rally', 'up', 'moon', 'buy', 'gain', 'rise', 'win']
        # çœ‹è·Œå…³é”®è¯
        bearish_words = ['bear', 'bearish', 'short', 'dump', 'down', 'sell', 'loss', 'fall', 'crash']
        
        bullish_count = sum(1 for word in bullish_words if word in text_lower)
        bearish_count = sum(1 for word in bearish_words if word in text_lower)
        
        if bullish_count > bearish_count and bullish_count > 0:
            return "Bullish"
        elif bearish_count > bullish_count and bearish_count > 0:
            return "Bearish"
        else:
            return ""  # ä¸­æ€§æˆ–æ— æ³•åˆ¤æ–­
    
    def fill_missing_sentiments(self, df, text_column='text', redis_client=None, queue_name='clean_data_queue', 
                               defer_redis_update=True):
        """
        ä¸º DataFrame ä¸­ç¼ºå¤± sentiment çš„è¡Œå¡«å……é¢„æµ‹å€¼ï¼Œå¹¶æ›´æ–° Redis ä¸­å¯¹åº”çš„æ•°æ®
        âœ… ä¼˜åŒ–ï¼šçœŸæ­£çš„æ‰¹å¤„ç† + å¼‚æ­¥ Redis æ›´æ–°ï¼ˆä¸é˜»å¡é¢„æµ‹ï¼‰+ æ€§èƒ½ç»Ÿè®¡
        
        Args:
            df: pandas DataFrame
            text_column: æ–‡æœ¬åˆ—å
            redis_client: Redis å®¢æˆ·ç«¯ï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›åˆ™æ›´æ–° Redisï¼‰
            queue_name: Redis é˜Ÿåˆ—åç§°
            defer_redis_update: æ˜¯å¦å»¶è¿Ÿ Redis æ›´æ–°ï¼ˆæ¨è Trueï¼Œé€Ÿåº¦å¿« 5 å€ï¼‰
            
        Returns:
            pandas DataFrame: å¡«å……åçš„ DataFrame
        """
        import time
        start_time = time.time()
        
        if 'sentiment' not in df.columns:
            df['sentiment'] = ''
        
        # æ‰¾å‡ºç¼ºå¤± sentiment çš„è¡Œ
        missing_mask = df['sentiment'].isna() | (df['sentiment'] == '') | (df['sentiment'].str.strip() == '')
        missing_count = missing_mask.sum()
        
        if missing_count == 0:
            print("âœ“ æ‰€æœ‰æ•°æ®éƒ½æœ‰ sentimentï¼Œæ— éœ€é¢„æµ‹")
            return df
        
        print(f"\n{'='*70}")
        print(f"ğŸ”® BERT æƒ…æ„Ÿé¢„æµ‹ (æ‰¹å¤„ç†æ¨¡å¼)")
        print(f"{'='*70}")
        print(f"å‘ç° {missing_count} æ¡ç¼ºå¤± sentiment çš„æ•°æ®")
        print(f"æ‰¹å¤§å°: {self.batch_size}, æ–‡æœ¬åˆ—: {text_column}")
        if defer_redis_update:
            print(f"âš¡ Redis æ›´æ–°æ¨¡å¼: å¼‚æ­¥å»¶è¿Ÿï¼ˆé€Ÿåº¦ä¼˜å…ˆï¼‰")
        
        # æå–ç¼ºå¤±çš„æ–‡æœ¬å’Œå¯¹åº”çš„ç´¢å¼•
        missing_indices = df[missing_mask].index.tolist()
        missing_texts = df.loc[missing_mask, text_column].fillna('').astype(str).tolist()
        
        # è·å–è®°å½• IDï¼ˆç”¨äº Redis æ›´æ–°ï¼‰
        id_column = 'id' if 'id' in df.columns else 'post_id' if 'post_id' in df.columns else None
        missing_ids = df.loc[missing_mask, id_column].tolist() if id_column else None
        
        # âœ… ä¸€æ¬¡æ€§æ‰¹é‡é¢„æµ‹ï¼ˆå…³é”®ä¼˜åŒ–ï¼ï¼‰
        print(f"\nâ³ æ‰§è¡Œæ‰¹é‡é¢„æµ‹...")
        predictions = self.predict_batch(missing_texts)
        
        # å¡«å……é¢„æµ‹ç»“æœåˆ° DataFrame
        df.loc[missing_mask, 'sentiment'] = predictions
        
        # âœ… Redis æ›´æ–°ï¼ˆå¼‚æ­¥æˆ–å»¶è¿Ÿï¼‰
        redis_update_stats = {'success': 0, 'failed': 0, 'not_found': 0}
        
        if redis_client is not None and missing_ids:
            if defer_redis_update:
                # âœ… ä¼˜åŒ–4ï¼šå»¶è¿Ÿæ›´æ–° Redisï¼ˆè¿”å›åå°æ‰§è¡Œï¼‰
                print(f"\nğŸ“¤ [å¼‚æ­¥] å°† Redis æ›´æ–°æ¨è¿Ÿåˆ°åå°...")
                print(f"   é¢„æµ‹æ•°æ®å·²å¡«å……åˆ° DataFrameï¼ŒRedis æ›´æ–°å°†åœ¨åå°è¿›è¡Œ")
                print(f"   è¿™æ ·é¢„æµ‹é€Ÿåº¦ä¸ä¼šè¢« Redis I/O æ‹–ç´¯")
                
                # è¿”å›åŒ…å«è¦æ›´æ–°çš„æ•°æ®å’Œé¢„æµ‹ç»“æœçš„ DataFrame
                # ç”±è°ƒç”¨æ–¹åœ¨åˆé€‚çš„æ—¶æœºè¿›è¡Œ Redis æ›´æ–°
                if not hasattr(df, '_pending_redis_updates'):
                    df._pending_redis_updates = []
                
                pending_updates = [
                    {'id': str(record_id), 'sentiment': prediction}
                    for record_id, prediction in zip(missing_ids, predictions)
                    if prediction  # åªæ›´æ–°æœ‰é¢„æµ‹ç»“æœçš„
                ]
                df._pending_redis_updates.extend(pending_updates)
                
                print(f"   âœ“ æ·»åŠ  {len(pending_updates)} æ¡è®°å½•åˆ°å¼‚æ­¥é˜Ÿåˆ—")
            else:
                # åŒæ­¥æ›´æ–°ï¼ˆæ—§æ–¹å¼ï¼Œè¾ƒæ…¢ï¼‰
                try:
                    if UPDATER_AVAILABLE:
                        from sentiment_updater import SentimentUpdater
                        
                        print(f"\nğŸ“¤ æ‰¹é‡æ›´æ–° Redis ä¸­çš„æ•°æ®...")
                        updater = SentimentUpdater(redis_client=redis_client)
                        
                        # æ„å»ºæ›´æ–°åˆ—è¡¨ï¼ˆåªåŒ…å«æœ‰é¢„æµ‹ç»“æœçš„ï¼‰
                        updates = [
                            {'id': str(record_id), 'sentiment': prediction}
                            for record_id, prediction in zip(missing_ids, predictions)
                            if prediction  # åªæ›´æ–°æœ‰é¢„æµ‹ç»“æœçš„
                        ]
                        
                        # âœ… æ‰¹é‡æ›´æ–°ï¼ˆé¿å…é€æ¡æ›´æ–°çš„å¼€é”€ï¼‰
                        if updates:
                            print(f"   æ­£åœ¨æ›´æ–° {len(updates)} æ¡è®°å½•...")
                            redis_update_stats = updater.batch_update_sentiments(updates)
                            print(f"   âœ“ æ›´æ–°æˆåŠŸ: {redis_update_stats.get('success', 0)} æ¡")
                            if redis_update_stats.get('failed', 0) > 0:
                                print(f"   âš ï¸  æ›´æ–°å¤±è´¥: {redis_update_stats.get('failed', 0)} æ¡")
                        else:
                            print("   âš ï¸  æ²¡æœ‰æœ‰æ•ˆçš„é¢„æµ‹ç»“æœéœ€è¦æ›´æ–°")
                    else:
                        print("   âš ï¸  SentimentUpdater ä¸å¯ç”¨ï¼Œè·³è¿‡ Redis æ›´æ–°")
                
                except Exception as e:
                    print(f"   âš ï¸  æ›´æ–° Redis å¤±è´¥ï¼Œä½†é¢„æµ‹ç»“æœå·²å¡«å……åˆ° DataFrame: {e}")
                    import traceback
                    traceback.print_exc()
        
        # âœ… ç»Ÿè®¡å¹¶æ˜¾ç¤ºé¢„æµ‹ç»“æœ
        predicted_sentiments = pd.Series(predictions).value_counts()
        print(f"\nğŸ“Š é¢„æµ‹ç»“æœç»Ÿè®¡:")
        for sentiment, count in predicted_sentiments.items():
            if sentiment:  # å¿½ç•¥ç©ºå­—ç¬¦ä¸²
                percentage = (count / len(predictions)) * 100
                print(f"   - {sentiment}: {count} æ¡ ({percentage:.1f}%)")
        
        # âœ… æ€§èƒ½ç»Ÿè®¡
        elapsed_time = time.time() - start_time
        speed = missing_count / elapsed_time
        print(f"\nâ±ï¸  è€—æ—¶: {elapsed_time:.2f}s, å¹³å‡é€Ÿåº¦: {speed:.1f} æ¡/ç§’")
        print(f"{'='*70}\n")
        
        return df
    
    def flush_pending_redis_updates(self, df, redis_client):
        """
        âœ… ä¼˜åŒ–4 è¡¥å……ï¼šæ‰¹é‡åˆ·æ–°ä¹‹å‰å»¶è¿Ÿçš„ Redis æ›´æ–°
        è¿™ä¸ªæ–¹æ³•åº”è¯¥åœ¨æ‰€æœ‰é¢„æµ‹å®Œæˆåè°ƒç”¨
        
        Args:
            df: pandas DataFrame
            redis_client: Redis å®¢æˆ·ç«¯
        """
        if not hasattr(df, '_pending_redis_updates') or not df._pending_redis_updates:
            print("â„¹ï¸  æ²¡æœ‰å¾…å¤„ç†çš„ Redis æ›´æ–°")
            return
        
        pending_updates = df._pending_redis_updates
        print(f"\nğŸ“¤ [åŒæ­¥] æ‰¹é‡åˆ·æ–° {len(pending_updates)} æ¡ Redis æ›´æ–°...")
        
        try:
            if UPDATER_AVAILABLE:
                from sentiment_updater import SentimentUpdater
                updater = SentimentUpdater(redis_client=redis_client)
                stats = updater.batch_update_sentiments(pending_updates)
                print(f"   âœ“ æˆåŠŸ: {stats.get('success', 0)}, å¤±è´¥: {stats.get('failed', 0)}")
                df._pending_redis_updates = []  # æ¸…ç©ºé˜Ÿåˆ—
            else:
                print("   âš ï¸  SentimentUpdater ä¸å¯ç”¨")
        except Exception as e:
            print(f"   âš ï¸  åˆ·æ–°å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()


class PredictionDataset(Dataset):
    """é¢„æµ‹æ•°æ®é›†"""
    
    def __init__(self, texts, tokenizer, max_len=256):
        self.texts = texts
        self.tokenizer = tokenizer
        self.max_len = max_len
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        
        # âš ï¸ ä¼˜åŒ–1ï¼šä¸ä½¿ç”¨ padding='max_length'ï¼Œè®© collate_fn ç»Ÿä¸€å¤„ç†
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            return_token_type_ids=False,
            padding=False,  # âœ… æ”¹ä¸º Falseï¼Œé¿å…æ¯ä¸ªæ ·æœ¬éƒ½å¡«å……åˆ° 256
            return_attention_mask=True,
            return_tensors=None,  # âœ… æ”¹ä¸º Noneï¼Œä¸åœ¨è¿™é‡Œåˆ›å»º Tensor
            truncation=True
        )
        
        return {
            'input_ids': encoding['input_ids'],
            'attention_mask': encoding['attention_mask']
        }


def collate_fn_for_prediction(batch):
    """
    âœ… ä¼˜åŒ–2ï¼šè‡ªå®šä¹‰ collate å‡½æ•°ï¼Œåªå¡«å……åˆ°å½“å‰æ‰¹æ¬¡çš„æœ€å¤§é•¿åº¦
    è¿™æ ·æ¯”å¡«å……åˆ° 256 å¿«å¾—å¤š
    """
    input_ids = [item['input_ids'] for item in batch]
    attention_masks = [item['attention_mask'] for item in batch]
    
    # æ‰¾å‡ºå½“å‰æ‰¹æ¬¡çš„æœ€å¤§é•¿åº¦
    max_len_batch = max(len(ids) for ids in input_ids)
    
    # åªå¡«å……åˆ°å½“å‰æ‰¹æ¬¡çš„æœ€å¤§é•¿åº¦ï¼Œä¸æ˜¯å›ºå®šçš„ 256
    input_ids_padded = []
    attention_masks_padded = []
    
    for ids, mask in zip(input_ids, attention_masks):
        pad_len = max_len_batch - len(ids)
        input_ids_padded.append(ids + [0] * pad_len)
        attention_masks_padded.append(mask + [0] * pad_len)
    
    return {
        'input_ids': torch.tensor(input_ids_padded, dtype=torch.long),
        'attention_mask': torch.tensor(attention_masks_padded, dtype=torch.long)
    }


# åˆ›å»ºå…¨å±€å•ä¾‹
_predictor_instance = None

def get_predictor():
    """è·å–å…¨å±€é¢„æµ‹å™¨å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼Œä½¿ç”¨é…ç½®çš„æ‰¹å¤§å°ï¼‰"""
    global _predictor_instance
    if _predictor_instance is None:
        try:
            # âœ… å°è¯•ä»é…ç½®ä¸­è¯»å–æ‰¹å¤§å°
            from config import CONFIG
            batch_size = CONFIG.get('bert', {}).get('batch_size', 32)
            max_len = CONFIG.get('bert', {}).get('max_len', 256)
            model_path = CONFIG.get('bert', {}).get('model_path')
            
            print(f"ğŸ“Š åˆå§‹åŒ– BERT é¢„æµ‹å™¨ (batch_size={batch_size}, max_len={max_len})")
            _predictor_instance = BertPredictor(model_path=model_path, max_len=max_len, batch_size=batch_size)
        except ImportError:
            # å¦‚æœé…ç½®ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤å€¼
            print("âš ï¸  é…ç½®æ–‡ä»¶ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°åˆå§‹åŒ– BERT é¢„æµ‹å™¨")
            _predictor_instance = BertPredictor()
    return _predictor_instance
