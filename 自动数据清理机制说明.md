# 自动数据清理机制说明

## 📋 概述

为了满足 **24 小时数据保留** 的需求，系统实现了自动数据清理机制。每个模块（Scraper 和 Cleaner）在完成数据处理后，都会自动清理超过 24 小时的旧数据，然后再发送完成通知给下一个模块。

## 🔄 数据流程

```
Scraper 爬取数据
    ↓
写入 Redis DB0 (data_queue)
    ↓
🗑️ 清理 DB0 中超过 24 小时的旧数据
    ↓
📢 发送 crawler_complete 通知
    ↓
Cleaner 清洗数据
    ↓
写入 Redis DB1 (clean_data_queue)
    ↓
🗑️ 清理 DB1 中超过 24 小时的旧数据
    ↓
📢 发送 cleaner_complete 通知
    ↓
Processor 处理数据
    ↓
写入 Redis DB2 (processed_data)
```

## ⏰ 清理策略

### 触发时机
- **Scraper**: 每次爬取任务完成后，清理 DB0
- **Cleaner**: 每次清洗任务完成后，清理 DB1

### 保留时间
- 默认保留 **24 小时** 的数据
- 可配置参数：`hours=24`

### 清理逻辑
1. 计算截止时间戳：`cutoff = 当前时间 - 24小时`
2. 从队列尾部（最旧）开始检查数据
3. 比较每条数据的 `timestamp` 字段
4. 删除所有 `timestamp < cutoff` 的数据
5. 遇到新数据就停止（因为队列按时间顺序排列）
6. 使用 `LTRIM` 批量删除尾部旧数据

## 📝 实现细节

### Scraper 清理实现

**文件**: `scraper/control_center.py`

```python
def _clean_old_data(self, hours=24):
    """清理超过指定时间的旧数据"""
    # 连接到 DB0
    r = redis.Redis(
        host=self.config['redis']['host'],
        port=self.config['redis']['port'],
        db=self.config['redis']['db'],
        decode_responses=True
    )
    
    # 计算截止时间
    cutoff_timestamp = time.time() - (hours * 3600)
    
    # 从队列尾部开始检查，删除旧数据
    # ... (详细实现见代码)
```

**调用位置**: `_send_completion_notification()` 方法中
- 先清理数据
- 再发送通知

### Cleaner 清理实现

**文件**: `cleaner/event_driven/cleaner.py`

```python
def _clean_old_data(self, redis_conn, queue_name, hours=24):
    """清理超过指定时间的旧数据"""
    # 计算截止时间
    cutoff_timestamp = time.time() - (hours * 3600)
    
    # 获取队列长度
    queue_length = redis_conn.llen(queue_name)
    
    # 从尾部向头部遍历
    for i in range(queue_length - 1, -1, -1):
        data_str = redis_conn.lindex(queue_name, i)
        data = json.loads(data_str)
        
        if data['timestamp'] < cutoff_timestamp:
            # 标记为旧数据
            removed_count += 1
        else:
            # 遇到新数据，停止
            break
    
    # 使用 LTRIM 批量删除
    redis_conn.ltrim(queue_name, 0, keep_count - 1)
```

**调用位置**: `_run_cleaning()` 方法中
- 先清理数据
- 再发送通知

## 🎯 关键优化

### 1. 非消费模式
- 使用 `LRANGE` 代替 `LPOP/RPOP`
- 数据读取后不删除
- 保留 24 小时历史数据供趋势分析使用

### 2. 高效删除
- 从队列尾部（最旧）开始检查
- 遇到新数据立即停止（利用时间顺序）
- 使用 `LTRIM` 批量删除，避免逐条删除

### 3. 统计信息
每次清理都会记录：
- `checked`: 检查的数据条数
- `removed`: 删除的数据条数
- `remaining`: 剩余的数据条数

## 📊 使用场景

### 趋势分析
Processor 可以比较：
- **最近 30 分钟** 的数据
- **过去 24 小时** 的数据

用于检测：
- 热度变化趋势
- 情绪波动
- 话题演进

### 内存管理
- 自动清理旧数据，防止 Redis 内存无限增长
- 保持数据新鲜度
- 避免手动干预

## 🔧 配置说明

### 修改保留时间

如果需要修改保留时间（例如改为 48 小时）：

**Scraper**:
```python
# scraper/control_center.py, line ~365
self._clean_old_data(hours=48)  # 改为 48
```

**Cleaner**:
```python
# cleaner/event_driven/cleaner.py, line ~205
clean_result = self._clean_old_data(r_out, QUEUE_OUT, hours=48)  # 改为 48
```

### 禁用自动清理

如果临时不需要自动清理，可以注释掉清理调用：

**Scraper**:
```python
# self._clean_old_data(hours=24)  # 注释掉
```

**Cleaner**:
```python
# clean_result = self._clean_old_data(r_out, QUEUE_OUT, hours=24)  # 注释掉
```

## 🚀 测试建议

### 1. 功能测试
```bash
# 1. 清空 Redis
redis-cli flushall

# 2. 运行 Scraper（会自动清理）
cd scraper
python control_center.py

# 3. 观察日志
# 应该看到类似输出：
# 🗑️  开始清理超过 24 小时的旧数据...
# 队列 data_queue 当前长度: 150
# ✅ 清理完成:
#    - 检查了 50 条数据
#    - 删除了 50 条旧数据
#    - 剩余 100 条数据
```

### 2. 时间验证
```bash
# 检查 Redis 中数据的时间戳
redis-cli -n 0 lrange data_queue 0 -1 | jq '.timestamp'

# 验证所有数据都在 24 小时内
python -c "
import redis, json, time
r = redis.Redis(db=0, decode_responses=True)
data = r.lrange('data_queue', 0, -1)
cutoff = time.time() - 24*3600
for d in data:
    obj = json.loads(d)
    if obj['timestamp'] < cutoff:
        print(f'发现超过 24 小时的数据: {obj}')
"
```

### 3. 性能测试
```bash
# 测试清理大量数据的性能
# 1. 插入 10000 条旧数据
python -c "
import redis, json, time
r = redis.Redis(db=0, decode_responses=True)
old_time = time.time() - 25*3600  # 25小时前
for i in range(10000):
    r.rpush('data_queue', json.dumps({
        'text': f'test {i}',
        'timestamp': old_time,
        'source': 'test'
    }))
print('插入完成')
"

# 2. 运行清理
# 应该快速删除所有旧数据
```

## 📈 监控指标

建议监控以下指标：
- Redis 内存使用量
- 队列长度变化
- 清理耗时
- 删除数据量

可以在日志中看到这些信息：
```
🗑️  开始清理超过 24 小时的旧数据...
队列 data_queue 当前长度: 1500
已检查 100 条数据，发现 50 条旧数据
已检查 200 条数据，发现 150 条旧数据
正在删除 200 条旧数据...
✅ 清理完成:
   - 检查了 250 条数据
   - 删除了 200 条旧数据
   - 剩余 1300 条数据
```

## ⚠️ 注意事项

1. **时间戳格式**: 所有数据必须包含 `timestamp` 字段（Unix 时间戳，秒级）
2. **数据顺序**: 假设队列中数据按时间顺序排列（旧数据在尾部）
3. **并发安全**: 清理过程中不应有其他进程写入队列
4. **性能影响**: 大量数据清理可能需要一定时间，建议在数据量较小时执行

## 🆚 与手动清理的对比

### 手动清理脚本
- 文件: `clean_old_data.py`
- 用途: 一次性清理，适合维护或测试
- 优点: 灵活控制
- 缺点: 需要手动执行

### 自动清理机制
- 位置: 集成在 Scraper 和 Cleaner 中
- 用途: 持续自动清理，适合生产环境
- 优点: 无需人工干预，自动维护
- 缺点: 需要确保逻辑正确

## 📚 相关文件

- `scraper/control_center.py` - Scraper 清理实现
- `cleaner/event_driven/cleaner.py` - Cleaner 清理实现
- `utils/redis_cleaner.py` - 清理工具函数
- `clean_old_data.py` - 手动清理脚本

## 🎉 总结

自动数据清理机制确保了：
- ✅ 24 小时数据窗口始终可用
- ✅ Redis 内存使用可控
- ✅ 数据保持新鲜
- ✅ 无需人工干预
- ✅ 性能优化（批量删除，智能停止）

系统现在可以持续运行，自动维护 24 小时的滚动数据窗口，支持实时趋势分析！
