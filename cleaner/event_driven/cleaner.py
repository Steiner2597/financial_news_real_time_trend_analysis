"""
äº‹ä»¶é©±åŠ¨æ¸…æ´—å™¨ä¸»ç±»
æ•´åˆå„ä¸ªç»„ä»¶ï¼Œæä¾›å®Œæ•´çš„äº‹ä»¶é©±åŠ¨æ¸…æ´—åŠŸèƒ½
"""
import logging
import time
from typing import Dict, Any
from pathlib import Path
import sys

# ä»é…ç½®æ–‡ä»¶åŠ è½½é…ç½®
import yaml

config_path = Path(__file__).parent.parent / "config_processing.yaml"
with open(config_path, 'r', encoding='utf-8') as f:
    CONFIG = yaml.safe_load(f)

REDIS_HOST = CONFIG['redis']['host']
REDIS_PORT = CONFIG['redis']['port']
DB_IN = CONFIG['redis']['db_in']
DB_OUT = CONFIG['redis']['db_out']
QUEUE_IN = CONFIG['redis']['queue_in']
QUEUE_OUT = CONFIG['redis']['queue_out']
ID_CACHE_KEY = CONFIG['redis']['id_cache']
LOG_DIR = Path(__file__).parent.parent / "logs"
LOG_DIR.mkdir(exist_ok=True)

# å¯¼å…¥æœ¬æ¨¡å—çš„ç»„ä»¶
from .redis_manager import RedisConnectionManager
from .notification_handler import NotificationHandler
from .cache_manager import CacheManager
from .signal_handler import SignalHandler
from .single_pass_cleaner import SinglePassCleaner

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler(LOG_DIR / "event_driven_cleaner.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class EventDrivenCleaner:
    """äº‹ä»¶é©±åŠ¨çš„æ¸…æ´—å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ¸…æ´—å™¨"""
        self.config = CONFIG
        
        # ç›‘å¬é…ç½®ï¼ˆä» Scraper æ¥æ”¶ï¼‰
        self.notification_listen = self.config.get('redis', {}).get('notification_listen', {})
        self.listen_enabled = self.notification_listen.get('enabled', False)
        self.listen_channel = self.notification_listen.get('channel', 'crawler_complete')
        self.mode = self.notification_listen.get('mode', 'event_driven')
        
        # å‘é€é…ç½®ï¼ˆå‘é€ç»™ Processorï¼‰
        self.notification_send = self.config.get('redis', {}).get('notification_send', {})
        self.send_enabled = self.notification_send.get('enabled', False)
        self.send_channel = self.notification_send.get('channel', 'cleaner_complete')
        
        # å»é‡é…ç½®
        self.dedup_config = self.config.get('deduplication', {})
        
        # è¿è¡ŒçŠ¶æ€
        self.running = True
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.redis_manager = RedisConnectionManager(REDIS_HOST, REDIS_PORT)
        self.signal_handler = SignalHandler(self._stop)
        self.notification_handler = None  # ç¨ååˆå§‹åŒ–
        self.cache_manager = None  # ç¨ååˆå§‹åŒ–
        
        # è®¾ç½®ä¿¡å·å¤„ç†
        self.signal_handler.setup()
        
        # æ‰“å°åˆå§‹åŒ–ä¿¡æ¯
        self._log_initialization()
    
    def _log_initialization(self):
        """è®°å½•åˆå§‹åŒ–ä¿¡æ¯"""
        logger.info("=" * 70)
        logger.info("äº‹ä»¶é©±åŠ¨æ¸…æ´—å™¨åˆå§‹åŒ–")
        logger.info("=" * 70)
        logger.info(f"æ¨¡å¼: {self.mode}")
        logger.info(f"ç›‘å¬é¢‘é“: {self.listen_channel} (å¯ç”¨: {self.listen_enabled})")
        logger.info(f"å‘é€é¢‘é“: {self.send_channel} (å¯ç”¨: {self.send_enabled})")
        logger.info("-" * 70)
        logger.info(f"å»é‡æ¨¡å¼: {self.dedup_config.get('mode', 'permanent')}")
        if self.dedup_config.get('mode') == 'time_window':
            logger.info(f"æ—¶é—´çª—å£: {self.dedup_config.get('window_hours', 24)} å°æ—¶")
        logger.info(f"å¯åŠ¨æ—¶æ¸…ç©º: {'æ˜¯' if self.dedup_config.get('clear_on_start', False) else 'å¦'}")
        logger.info("=" * 70)
    
    def _stop(self):
        """åœæ­¢è¿è¡Œ"""
        self.running = False
    
    def _connect_redis(self):
        """è¿æ¥ Redis"""
        # è¿æ¥è®¢é˜…
        self.redis_manager.connect_subscribe(DB_IN, self.listen_channel)
        
        # è¿æ¥å‘å¸ƒï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.send_enabled:
            publish_client = self.redis_manager.connect_publish(DB_OUT)
        else:
            publish_client = None
        
        # åˆå§‹åŒ–é€šçŸ¥å¤„ç†å™¨
        self.notification_handler = NotificationHandler(
            publish_client,
            self.send_enabled,
            self.send_channel
        )
        
        # åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨ï¼ˆåˆ›å»ºä¸€ä¸ªç®€å•çš„è¿æ¥å™¨å¯¹è±¡ï¼‰
        import redis
        class SimpleConnector:
            def __init__(self, host, port, db):
                self.r = redis.Redis(host=host, port=port, db=db, decode_responses=True)
        
        r_out = SimpleConnector(REDIS_HOST, REDIS_PORT, DB_OUT)
        self.cache_manager = CacheManager(r_out, ID_CACHE_KEY, self.dedup_config)
        
        # å¦‚æœé…ç½®è¦æ±‚ï¼Œæ¸…ç©º ID ç¼“å­˜
        if self.dedup_config.get('clear_on_start', False):
            self.cache_manager.clear_cache()
    
    def _run_cleaning(self) -> int:
        """
        æ‰§è¡Œæ¸…æ´—ä»»åŠ¡ï¼ˆå•æ¬¡å¤„ç†ï¼‰
        
        Returns:
            æ¸…æ´—çš„æ•°æ®é‡
        """
        try:
            # åˆ›å»ºå•æ¬¡æ¸…æ´—å™¨
            cleaner = SinglePassCleaner(
                redis_host=REDIS_HOST,
                redis_port=REDIS_PORT,
                db_in=DB_IN,
                db_out=DB_OUT,
                queue_in=QUEUE_IN,
                queue_out=QUEUE_OUT,
                id_cache_key=ID_CACHE_KEY
            )
            
            # æ‰§è¡Œå•æ¬¡æ¸…æ´—
            stats = cleaner.clean_once(batch_size=100)
            
            # å¯¼å‡ºåˆ°æ–‡ä»¶
            if stats['cleaned'] > 0:
                logger.info("\nğŸ“¦ å¯¼å‡ºæ¸…æ´—ç»“æœåˆ°æ–‡ä»¶...")
                output_dir = Path(__file__).parent.parent / "output"
                cleaner.export_to_file(output_dir)
            
            # å…³é—­æ¸…æ´—å™¨
            cleaner.close()
            
            return stats['cleaned']
            
        except Exception as e:
            logger.error(f"æ¸…æ´—è¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return 0
    
    def _process_notification(self, message: Dict[str, Any]):
        """
        å¤„ç†æ”¶åˆ°çš„é€šçŸ¥æ¶ˆæ¯
        
        Args:
            message: é€šçŸ¥æ¶ˆæ¯
        """
        try:
            # æ˜¾ç¤ºæ¸…æ´—å‰çš„ç¼“å­˜çŠ¶æ€
            self.cache_manager.log_cache_status("æ¸…æ´—å‰")
            
            # å¤„ç†æ¶ˆæ¯å¹¶æ‰§è¡Œæ¸…æ´—
            cleaned_count = self.notification_handler.process_message(
                message,
                lambda msg: self._run_cleaning()
            )
            
            # æ˜¾ç¤ºæ¸…æ´—åçš„ç¼“å­˜çŠ¶æ€
            self.cache_manager.log_cache_status("æ¸…æ´—å")
            
            # å‘é€å®Œæˆé€šçŸ¥ç»™ Processor
            logger.info("ğŸ“¤ å‡†å¤‡å‘é€æ¸…æ´—å®Œæˆé€šçŸ¥...")
            # è·å–è¾“å‡ºé˜Ÿåˆ—é•¿åº¦
            import redis
            r_out = redis.Redis(
                host=REDIS_HOST,
                port=REDIS_PORT,
                db=DB_OUT,
                decode_responses=True
            )
            
            # æ¸…ç†è¶…è¿‡ 24 å°æ—¶çš„æ—§æ•°æ®
            logger.info("\nğŸ§¹ æ¸…ç†è¶…è¿‡ 24 å°æ—¶çš„æ—§æ•°æ®...")
            clean_result = self._clean_old_data(r_out, QUEUE_OUT, hours=24)
            
            queue_length = r_out.llen(QUEUE_OUT)
            r_out.close()
            crawler_stats = message.get('statistics', {})
            
            self.notification_handler.send_completion_notification(
                cleaned_count,
                queue_length,
                crawler_stats
            )
            
            logger.info("=" * 70)
            logger.info("âœ¨ æ•°æ®æ¸…æ´—å®Œæˆ")
            logger.info("=" * 70 + "\n")
            
        except Exception as e:
            logger.error(f"å¤„ç†é€šçŸ¥æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
    
    def _clean_old_data(self, redis_conn, queue_name, hours=24):
        """
        æ¸…ç†è¶…è¿‡æŒ‡å®šæ—¶é—´çš„æ—§æ•°æ®
        
        Args:
            redis_conn: Redis è¿æ¥å¯¹è±¡ï¼ˆRedis å®ä¾‹ï¼‰
            queue_name: é˜Ÿåˆ—åç§°
            hours: ä¿ç•™æ—¶é—´ï¼ˆå°æ—¶ï¼‰ï¼Œé»˜è®¤ 24 å°æ—¶
            
        Returns:
            dict: æ¸…ç†ç»“æœç»Ÿè®¡
        """
        logger.info(f"\nğŸ—‘ï¸  å¼€å§‹æ¸…ç†è¶…è¿‡ {hours} å°æ—¶çš„æ—§æ•°æ®...")
        
        try:
            import json
            import time
            
            cutoff_timestamp = time.time() - (hours * 3600)
            removed_count = 0
            checked_count = 0
            
            # è·å–é˜Ÿåˆ—é•¿åº¦
            queue_length = redis_conn.llen(queue_name)
            logger.info(f"é˜Ÿåˆ— {queue_name} å½“å‰é•¿åº¦: {queue_length}")
            
            if queue_length == 0:
                logger.info("é˜Ÿåˆ—ä¸ºç©ºï¼Œæ— éœ€æ¸…ç†")
                return {
                    'removed': 0,
                    'checked': 0,
                    'remaining': 0
                }
            
            # ä»é˜Ÿåˆ—å°¾éƒ¨ï¼ˆæœ€æ—§çš„æ•°æ®ï¼‰å¼€å§‹æ£€æŸ¥
            # ä½¿ç”¨ LINDEX é€ä¸ªæ£€æŸ¥ï¼Œé‡åˆ°æ–°æ•°æ®å°±åœæ­¢
            items_to_remove = []
            
            for i in range(queue_length - 1, -1, -1):  # ä»å°¾éƒ¨å‘å¤´éƒ¨éå†
                try:
                    # è·å–é˜Ÿåˆ—ä¸­çš„æ•°æ®
                    data_str = redis_conn.lindex(queue_name, i)
                    if not data_str:
                        continue
                    
                    checked_count += 1
                    data = json.loads(data_str)
                    
                    # æ£€æŸ¥æ—¶é—´æˆ³
                    if 'timestamp' not in data:
                        logger.warning(f"æ•°æ®ç¼ºå°‘ timestamp å­—æ®µï¼Œè·³è¿‡: {data_str[:100]}")
                        continue
                    
                    timestamp = data['timestamp']
                    
                    # å¦‚æœæ˜¯æ—§æ•°æ®ï¼Œæ ‡è®°åˆ é™¤
                    if timestamp < cutoff_timestamp:
                        items_to_remove.append(i)
                        removed_count += 1
                    else:
                        # é‡åˆ°æ–°æ•°æ®ï¼Œåœæ­¢æ£€æŸ¥ï¼ˆå› ä¸ºé˜Ÿåˆ—æ˜¯æŒ‰æ—¶é—´é¡ºåºçš„ï¼‰
                        break
                    
                    # æ¯æ£€æŸ¥ 100 æ¡æ•°æ®è¾“å‡ºä¸€æ¬¡è¿›åº¦
                    if checked_count % 100 == 0:
                        logger.info(f"å·²æ£€æŸ¥ {checked_count} æ¡æ•°æ®ï¼Œå‘ç° {removed_count} æ¡æ—§æ•°æ®")
                
                except json.JSONDecodeError as e:
                    logger.error(f"JSON è§£æå¤±è´¥: {e}")
                    continue
                except Exception as e:
                    logger.error(f"å¤„ç†æ•°æ®æ—¶å‡ºé”™: {e}")
                    continue
            
            # åˆ é™¤æ—§æ•°æ®ï¼ˆä»åå¾€å‰åˆ é™¤ï¼Œé¿å…ç´¢å¼•å˜åŒ–ï¼‰
            if items_to_remove:
                logger.info(f"æ­£åœ¨åˆ é™¤ {len(items_to_remove)} æ¡æ—§æ•°æ®...")
                
                # ä½¿ç”¨ LTRIM åˆ é™¤å°¾éƒ¨æ—§æ•°æ®
                # å› ä¸ºæ—§æ•°æ®åœ¨å°¾éƒ¨ï¼Œæˆ‘ä»¬åªéœ€è¦ä¿ç•™å‰é¢çš„æ–°æ•°æ®
                keep_count = queue_length - removed_count
                if keep_count > 0:
                    redis_conn.ltrim(queue_name, 0, keep_count - 1)
                else:
                    # å…¨éƒ¨æ˜¯æ—§æ•°æ®ï¼Œæ¸…ç©ºé˜Ÿåˆ—
                    redis_conn.delete(queue_name)
            
            remaining = redis_conn.llen(queue_name)
            
            logger.info(f"âœ… æ¸…ç†å®Œæˆ:")
            logger.info(f"   - æ£€æŸ¥äº† {checked_count} æ¡æ•°æ®")
            logger.info(f"   - åˆ é™¤äº† {removed_count} æ¡æ—§æ•°æ®")
            logger.info(f"   - å‰©ä½™ {remaining} æ¡æ•°æ®")
            
            return {
                'removed': removed_count,
                'checked': checked_count,
                'remaining': remaining
            }
            
        except Exception as e:
            logger.error(f"æ¸…ç†æ—§æ•°æ®æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return {
                'removed': 0,
                'checked': 0,
                'remaining': redis_conn.llen(queue_name),
                'error': str(e)
            }
    
    def run_event_driven(self):
        """äº‹ä»¶é©±åŠ¨æ¨¡å¼ï¼šç­‰å¾…é€šçŸ¥"""
        logger.info("\n" + "=" * 70)
        logger.info("ğŸ§ äº‹ä»¶é©±åŠ¨æ•°æ®æ¸…æ´—å™¨å·²å°±ç»ª")
        logger.info("=" * 70)
        logger.info("ç›‘å¬é¢‘é“: %s", self.listen_channel)
        logger.info("å‘é€é¢‘é“: %s", self.send_channel)
        logger.info("æŒ‰ Ctrl+C åœæ­¢")
        logger.info("=" * 70 + "\n")
        
        # è¿æ¥ Redis
        self._connect_redis()
        
        # è·å– pubsub å¯¹è±¡
        pubsub = self.redis_manager.pubsub
        
        # ç›‘å¬æ¶ˆæ¯ï¼ˆä½¿ç”¨è¶…æ—¶ä»¥æ”¯æŒ Ctrl+Cï¼‰
        try:
            while self.running:
                try:
                    # ä½¿ç”¨ get_message() å¸¦è¶…æ—¶ï¼Œè€Œä¸æ˜¯ listen()
                    raw_message = pubsub.get_message(timeout=1.0)
                    
                    if raw_message is None:
                        # æ²¡æœ‰æ¶ˆæ¯ï¼Œç»§ç»­ç­‰å¾…
                        continue
                    
                    # è§£ææ¶ˆæ¯
                    message_data = self.notification_handler.parse_message(raw_message)
                    if message_data is None:
                        continue
                    
                    # å¤„ç†æ¶ˆæ¯
                    self._process_notification(message_data)
                
                except KeyboardInterrupt:
                    logger.info("\nâš ï¸  æ”¶åˆ°é”®ç›˜ä¸­æ–­")
                    break
                except Exception as e:
                    if self.running:  # åªåœ¨è¿è¡Œæ—¶æ‰“å°é”™è¯¯
                        logger.error(f"æ¥æ”¶æ¶ˆæ¯æ—¶å‡ºé”™: {e}")
                    time.sleep(1)
        
        except KeyboardInterrupt:
            logger.info("\nâš ï¸  æ”¶åˆ°ä¸­æ–­ä¿¡å·")
        finally:
            self._cleanup()
    
    def run_continuous(self):
        """æŒç»­æ¨¡å¼ï¼šå®šæœŸæ£€æŸ¥é˜Ÿåˆ—ï¼ˆå…¼å®¹æ—§æ¨¡å¼ï¼‰"""
        logger.info("\n" + "=" * 70)
        logger.info("ğŸ”„ æŒç»­è½®è¯¢æ¨¡å¼å¯åŠ¨")
        logger.info("æŒ‰ Ctrl+C åœæ­¢")
        logger.info("=" * 70 + "\n")
        
        try:
            import time
            # ä½¿ç”¨å•æ¬¡æ¸…æ´—çš„è½®è¯¢æ¨¡å¼
            while self.running:
                try:
                    # æ‰§è¡Œå•æ¬¡æ¸…æ´—
                    cleaned_count = self._run_cleaning()
                    
                    if cleaned_count > 0:
                        logger.info(f"âœ… æœ¬è½®æ¸…æ´—å®Œæˆï¼Œå¤„ç†äº† {cleaned_count} æ¡æ•°æ®")
                    
                    # ç­‰å¾…ä¸€æ®µæ—¶é—´å†æ£€æŸ¥
                    time.sleep(5)
                    
                except Exception as e:
                    logger.error(f"è½®è¯¢è¿‡ç¨‹å‡ºé”™: {e}")
                    time.sleep(5)
                    
        except KeyboardInterrupt:
            logger.info("\nâš ï¸  æ”¶åˆ°ä¸­æ–­ä¿¡å·")
        finally:
            self._cleanup()
    
    def _cleanup(self):
        """æ¸…ç†èµ„æº"""
        logger.info("\n" + "=" * 70)
        logger.info("ğŸ§¹ æ¸…ç†èµ„æº...")
        
        # æ¸…ç† Redis è¿æ¥
        self.redis_manager.cleanup(self.listen_channel)
        
        # æ¢å¤ä¿¡å·å¤„ç†å™¨
        self.signal_handler.restore()
        
        logger.info("ğŸ‘‹ æ¸…æ´—å™¨å·²åœæ­¢")
        logger.info("=" * 70)
    
    def run(self):
        """æ ¹æ®é…ç½®è¿è¡Œ"""
        if not self.listen_enabled:
            logger.warning("âš ï¸  é€šçŸ¥åŠŸèƒ½æœªå¯ç”¨ï¼Œä½¿ç”¨æŒç»­æ¨¡å¼")
            self.run_continuous()
        elif self.mode == 'event_driven':
            self.run_event_driven()
        else:
            self.run_continuous()


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='äº‹ä»¶é©±åŠ¨æ•°æ®æ¸…æ´—å™¨')
    parser.add_argument(
        '--mode',
        choices=['event_driven', 'continuous', 'once'],
        default=None,
        help='è¿è¡Œæ¨¡å¼ (é»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®)'
    )
    args = parser.parse_args()
    
    cleaner = EventDrivenCleaner()
    
    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®
    if args.mode:
        cleaner.mode = args.mode
    
    if args.mode == 'once':
        # å•æ¬¡è¿è¡Œ
        logger.info("ğŸ”„ å•æ¬¡è¿è¡Œæ¨¡å¼")
        cleaner._run_cleaning()
    else:
        # æ ¹æ®é…ç½®è¿è¡Œ
        cleaner.run()


if __name__ == "__main__":
    main()
