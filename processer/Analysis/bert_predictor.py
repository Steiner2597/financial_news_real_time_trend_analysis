"""
BERT æƒ…æ„Ÿé¢„æµ‹å™¨
ä¸ºç¼ºå¤± sentiment çš„æ•°æ®æä¾›è‡ªåŠ¨é¢„æµ‹
"""
import os
import sys
from pathlib import Path
import pandas as pd
import numpy as np

# æ ‡è®°æ˜¯å¦å¯ç”¨
BERT_AVAILABLE = False
model = None
tokenizer = None
device = None
reverse_label_map = None

try:
    import torch
    from torch.utils.data import Dataset, DataLoader
    from transformers import BertTokenizer, BertForSequenceClassification
    BERT_AVAILABLE = True
except ImportError:
    print("âš ï¸  è­¦å‘Š: PyTorch/Transformers æœªå®‰è£…ï¼ŒBERT é¢„æµ‹åŠŸèƒ½ä¸å¯ç”¨")
    print("   å°†ä½¿ç”¨ç®€å•è§„åˆ™è¿›è¡Œ sentiment å¡«å……")


class BertPredictor:
    """BERT æƒ…æ„Ÿé¢„æµ‹å™¨"""
    
    def __init__(self, model_path=None, max_len=256, batch_size=16):
        """
        åˆå§‹åŒ–é¢„æµ‹å™¨
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º ../Bert_Model/best_model.pth
            max_len: æœ€å¤§åºåˆ—é•¿åº¦
            batch_size: æ‰¹å¤„ç†å¤§å°
        """
        self.max_len = max_len
        self.batch_size = batch_size
        self.model_loaded = False
        
        if not BERT_AVAILABLE:
            print("âš ï¸  BERT ä¾èµ–ä¸å¯ç”¨ï¼Œé¢„æµ‹å™¨å°†ä½¿ç”¨ç®€å•è§„åˆ™")
            return
        
        # æŸ¥æ‰¾æ¨¡å‹è·¯å¾„
        if model_path is None:
            model_path = self._find_model_path()
        
        if model_path is None or not os.path.exists(model_path):
            print(f"âš ï¸  è­¦å‘Š: æ‰¾ä¸åˆ° BERT æ¨¡å‹æ–‡ä»¶ï¼Œå°†ä½¿ç”¨ç®€å•è§„åˆ™")
            print(f"   æœŸæœ›è·¯å¾„: {model_path}")
            return
        
        # åŠ è½½æ¨¡å‹
        try:
            self._load_model(model_path)
            self.model_loaded = True
            print(f"âœ… BERT æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
        except Exception as e:
            print(f"âš ï¸  è­¦å‘Š: BERT æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("   å°†ä½¿ç”¨ç®€å•è§„åˆ™è¿›è¡Œ sentiment å¡«å……")
    
    def _find_model_path(self):
        """æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶"""
        current_dir = Path(__file__).parent
        
        # å°è¯•å¤šä¸ªå¯èƒ½çš„è·¯å¾„
        possible_paths = [
            current_dir / '..' / 'Bert_Model' / 'best_model.pth',
            current_dir / '..' / '..' / 'Bert_Model' / 'best_model.pth',
        ]
        
        for path in possible_paths:
            if path.exists():
                return str(path.resolve())
        
        return None
    
    def _load_model(self, model_path):
        """åŠ è½½æ¨¡å‹"""
        global model, tokenizer, device, reverse_label_map
        
        # è®¾å¤‡é€‰æ‹©
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint = torch.load(model_path, map_location=device, weights_only=False)
        
        # åŠ è½½ tokenizer å’Œæ ‡ç­¾æ˜ å°„
        tokenizer = checkpoint['tokenizer']
        label_map = checkpoint['label_map']
        config = checkpoint['config']
        
        # åˆ›å»ºæ¨¡å‹
        model = BertForSequenceClassification.from_pretrained(
            'bert-base-uncased',
            num_labels=config['num_labels'],
            output_attentions=False,
            output_hidden_states=False
        )
        model.load_state_dict(checkpoint['model_state_dict'])
        model.to(device)
        model.eval()
        
        # åå‘æ ‡ç­¾æ˜ å°„
        reverse_label_map = {v: k for k, v in label_map.items()}
    
    def predict_batch(self, texts):
        """
        æ‰¹é‡é¢„æµ‹æ–‡æœ¬çš„æƒ…æ„Ÿ
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            list: é¢„æµ‹çš„æƒ…æ„Ÿæ ‡ç­¾åˆ—è¡¨ (Bullish/Bearish)
        """
        if not self.model_loaded:
            # ä½¿ç”¨ç®€å•è§„åˆ™
            return [self._simple_sentiment(text) for text in texts]
        
        try:
            # åˆ›å»ºæ•°æ®é›†å’ŒåŠ è½½å™¨
            dataset = PredictionDataset(texts, tokenizer, self.max_len)
            data_loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False)
            
            # é¢„æµ‹
            predictions = []
            with torch.no_grad():
                for batch in data_loader:
                    input_ids = batch['input_ids'].to(device)
                    attention_mask = batch['attention_mask'].to(device)
                    
                    outputs = model(input_ids=input_ids, attention_mask=attention_mask)
                    logits = outputs.logits
                    
                    _, preds = torch.max(logits, dim=1)
                    predictions.extend(preds.cpu().tolist())
            
            # è½¬æ¢ä¸ºæ ‡ç­¾
            return [reverse_label_map[p] for p in predictions]
            
        except Exception as e:
            print(f"âš ï¸  BERT é¢„æµ‹å¤±è´¥: {e}ï¼Œä½¿ç”¨ç®€å•è§„åˆ™")
            return [self._simple_sentiment(text) for text in texts]
    
    def _simple_sentiment(self, text):
        """
        ç®€å•çš„å¯å‘å¼æƒ…æ„Ÿåˆ¤æ–­ï¼ˆåå¤‡æ–¹æ¡ˆï¼‰
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            str: 'Bullish' æˆ– 'Bearish' æˆ– ''
        """
        if not text or not isinstance(text, str):
            return ""
        
        text_lower = text.lower()
        
        # çœ‹æ¶¨å…³é”®è¯
        bullish_words = ['bull', 'bullish', 'long', 'rally', 'up', 'moon', 'buy', 'gain', 'rise', 'win']
        # çœ‹è·Œå…³é”®è¯
        bearish_words = ['bear', 'bearish', 'short', 'dump', 'down', 'sell', 'loss', 'fall', 'crash']
        
        bullish_count = sum(1 for word in bullish_words if word in text_lower)
        bearish_count = sum(1 for word in bearish_words if word in text_lower)
        
        if bullish_count > bearish_count and bullish_count > 0:
            return "Bullish"
        elif bearish_count > bullish_count and bearish_count > 0:
            return "Bearish"
        else:
            return ""  # ä¸­æ€§æˆ–æ— æ³•åˆ¤æ–­
    
    def fill_missing_sentiments(self, df, text_column='text'):
        """
        ä¸º DataFrame ä¸­ç¼ºå¤± sentiment çš„è¡Œå¡«å……é¢„æµ‹å€¼
        
        Args:
            df: pandas DataFrame
            text_column: æ–‡æœ¬åˆ—å
            
        Returns:
            pandas DataFrame: å¡«å……åçš„ DataFrame
        """
        if 'sentiment' not in df.columns:
            df['sentiment'] = ''
        
        # æ‰¾å‡ºç¼ºå¤± sentiment çš„è¡Œ
        missing_mask = df['sentiment'].isna() | (df['sentiment'] == '') | (df['sentiment'].str.strip() == '')
        missing_count = missing_mask.sum()
        
        if missing_count == 0:
            print("âœ“ æ‰€æœ‰æ•°æ®éƒ½æœ‰ sentimentï¼Œæ— éœ€é¢„æµ‹")
            return df
        
        print(f"ğŸ”® å‘ç° {missing_count} æ¡ç¼ºå¤± sentiment çš„æ•°æ®ï¼Œå¼€å§‹é¢„æµ‹...")
        
        # æå–ç¼ºå¤±çš„æ–‡æœ¬
        missing_texts = df.loc[missing_mask, text_column].fillna('').astype(str).tolist()
        
        # æ‰¹é‡é¢„æµ‹
        predictions = self.predict_batch(missing_texts)
        
        # å¡«å……é¢„æµ‹ç»“æœ
        df.loc[missing_mask, 'sentiment'] = predictions
        
        # ç»Ÿè®¡é¢„æµ‹ç»“æœ
        predicted_sentiments = pd.Series(predictions).value_counts()
        print(f"âœ“ é¢„æµ‹å®Œæˆ:")
        for sentiment, count in predicted_sentiments.items():
            if sentiment:  # å¿½ç•¥ç©ºå­—ç¬¦ä¸²
                print(f"  - {sentiment}: {count} æ¡")
        
        return df


class PredictionDataset(Dataset):
    """é¢„æµ‹æ•°æ®é›†"""
    
    def __init__(self, texts, tokenizer, max_len=256):
        self.texts = texts
        self.tokenizer = tokenizer
        self.max_len = max_len
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            return_token_type_ids=False,
            padding='max_length',
            return_attention_mask=True,
            return_tensors='pt',
            truncation=True
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten()
        }


# åˆ›å»ºå…¨å±€å•ä¾‹
_predictor_instance = None

def get_predictor():
    """è·å–å…¨å±€é¢„æµ‹å™¨å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _predictor_instance
    if _predictor_instance is None:
        _predictor_instance = BertPredictor()
    return _predictor_instance
