# 清洗流程对比：阻塞 vs 非阻塞

## 📊 原流程（阻塞模式）

```
┌────────────────────────────────────────────────────────────────┐
│                         时间线                                   │
└────────────────────────────────────────────────────────────────┘

T1  Scraper 爬取数据
    │
    ├─→ 推送 1000 条数据到 Redis DB0
    │
    └─→ 发送 'crawler_complete' 通知
         │
         ▼
T2  Cleaner 收到通知
    │
    └─→ 调用 cursor_loop()
         │
         ├─→ while True:           ← ❌ 无限循环开始
         │       wait for data     ← ❌ 一直等待
         │       clean data
         │       ...
         │
         └─→ 永远不会退出          ← ❌ 永远卡在这里
         
T3  ❌ 无法发送 'cleaner_complete' 通知
    
T4  ❌ Processor 一直等待通知
    
T5  ❌ 整个流程停滞
    
    
═══════════════════════════════════════════════════════════════════
    问题总结:
    ❌ Cleaner 陷入无限循环
    ❌ 无法通知 Processor
    ❌ 数据流程中断
    ❌ 资源持续占用
═══════════════════════════════════════════════════════════════════
```

## ✨ 新流程（非阻塞模式）

```
┌────────────────────────────────────────────────────────────────┐
│                         时间线                                   │
└────────────────────────────────────────────────────────────────┘

T1  Scraper 爬取数据
    │
    ├─→ 推送 1000 条数据到 Redis DB0
    │
    └─→ 发送 'crawler_complete' 通知
         │
         ▼
T2  Cleaner 收到通知
    │
    └─→ 调用 clean_once()
         │
         ├─→ 获取队列长度: 1000 条   ✅ 明确边界
         │
         ├─→ 批量处理 (100条/批)
         │   ├─ 批次 1: 100 条
         │   ├─ 批次 2: 100 条
         │   ├─ ...
         │   └─ 批次 10: 100 条
         │
         ├─→ 统计: 清洗 850, 去重 150  ✅ 详细统计
         │
         └─→ 返回结果                ✅ 立即返回
         
T3  ✅ 发送 'cleaner_complete' 通知
    │
    └─→ 通知内容: {cleaned: 850, queue_length: 850}
         │
         ▼
T4  Processor 收到通知
    │
    └─→ 开始处理 850 条清洗后的数据
         │
         ▼
T5  ✅ Cleaner 继续待命
    │
    └─→ 等待下一次 'crawler_complete' 通知


═══════════════════════════════════════════════════════════════════
    优势总结:
    ✅ Cleaner 单次处理，不阻塞
    ✅ 及时通知 Processor
    ✅ 数据流程顺畅
    ✅ 资源按需使用
    ✅ 支持多轮处理
═══════════════════════════════════════════════════════════════════
```

## 🔄 多轮处理流程

```
第一轮爬取
═══════════════════════════════════════════════════════════════════
T1: Scraper 爬取 → 1000 条数据
T2: Cleaner 清洗 → 850 条数据
T3: Processor 处理 → 分析完成
T4: Cleaner 待命 ✅


第二轮爬取（30分钟后）
═══════════════════════════════════════════════════════════════════
T5: Scraper 爬取 → 800 条数据
T6: Cleaner 清洗 → 650 条数据（去重150）
T7: Processor 处理 → 分析完成
T8: Cleaner 待命 ✅


第三轮爬取（1小时后）
═══════════════════════════════════════════════════════════════════
T9:  Scraper 爬取 → 1200 条数据
T10: Cleaner 清洗 → 1000 条数据
T11: Processor 处理 → 分析完成
T12: Cleaner 待命 ✅

✅ 每一轮都顺畅完成，不会卡住！
```

## 📈 性能对比图

### 处理时间对比

```
原流程:
爬取  清洗开始 ────────────────────────────→ ❌ 永不结束
│     │
0秒   5秒                                   ∞

新流程:
爬取  清洗  完成  通知  处理  待命
│     │     │     │     │     │
0秒   5秒   15秒  16秒  20秒  待命状态 ✅
```

### 资源占用对比

```
原流程:
CPU:  ████████████████████████████████████ (持续高占用)
内存: ████████████████████████████████████ (持续占用)

新流程:
CPU:  ████████░░░░░░░░░░░░░░░░░░░░░░░░░░░ (按需使用)
内存: ████████░░░░░░░░░░░░░░░░░░░░░░░░░░░ (及时释放)
```

## 🎯 关键差异

| 方面 | 原流程 (cursor_loop) | 新流程 (clean_once) |
|-----|---------------------|-------------------|
| **循环方式** | `while True` 无限循环 | 处理固定数量的数据 |
| **处理边界** | ❌ 无边界，一直等待 | ✅ 明确边界，队列长度 |
| **完成时间** | ❌ 永不完成 | ✅ 10-20 秒 |
| **通知发送** | ❌ 无法发送 | ✅ 及时发送 |
| **下游影响** | ❌ Processor 永久等待 | ✅ Processor 正常工作 |
| **多轮支持** | ❌ 卡在第一轮 | ✅ 支持无限轮 |
| **资源管理** | ❌ 持续占用 | ✅ 按需分配 |

## 💡 代码对比

### 原代码（阻塞）

```python
def cursor_loop():
    """无限循环，永不退出"""
    while True:  # ❌ 无限循环
        # 等待数据
        data = redis.brpop('queue', timeout=0)  # ❌ 阻塞等待
        
        # 处理数据
        clean(data)
        
        # 永远不会退出这个循环
```

### 新代码（非阻塞）

```python
def clean_once(self, batch_size=100):
    """处理当前队列数据，然后返回"""
    # 1. 获取当前队列长度
    queue_length = redis.llen('queue')  # ✅ 明确边界
    
    # 2. 只处理这些数据
    processed = 0
    while processed < queue_length:  # ✅ 有限循环
        batch = get_batch(batch_size)
        process_batch(batch)
        processed += len(batch)
    
    # 3. 处理完成，返回统计
    return stats  # ✅ 立即返回
```

## 🔍 详细执行流程

### 单次清洗的执行细节

```
clean_once() 调用
    │
    ├─ 步骤 1: 获取队列信息
    │  ├─ queue_length = redis.llen('data_queue')
    │  └─ 结果: 1000 条
    │
    ├─ 步骤 2: 批量处理循环
    │  │
    │  ├─ 批次 1 (0-100)
    │  │  ├─ 弹出 100 条数据
    │  │  ├─ 验证、去重、清洗
    │  │  └─ 推送到 clean_data_queue
    │  │
    │  ├─ 批次 2 (100-200)
    │  │  └─ ... 同上
    │  │
    │  └─ ... (共 10 批)
    │
    ├─ 步骤 3: 生成统计
    │  ├─ total_processed: 1000
    │  ├─ cleaned: 850
    │  ├─ duplicates: 100
    │  └─ invalid: 50
    │
    └─ 步骤 4: 返回
       └─ return stats  ← ✅ 在这里返回，不再循环
```

## 🎉 总结

### 核心改进

```
┌─────────────────────────────────────────────┐
│  原流程: 永不停止的清洗循环                   │
│  ❌ 阻塞                                     │
│  ❌ 无法继续                                 │
│  ❌ 资源浪费                                 │
└─────────────────────────────────────────────┘
                    ↓
            【重构为】
                    ↓
┌─────────────────────────────────────────────┐
│  新流程: 单次处理，快速响应                   │
│  ✅ 非阻塞                                   │
│  ✅ 流程顺畅                                 │
│  ✅ 资源高效                                 │
└─────────────────────────────────────────────┘
```

### 实际效果

- ✅ Scraper → Cleaner → Processor 完整流程顺畅运行
- ✅ 支持多轮爬取和清洗
- ✅ 每轮独立完成，互不影响
- ✅ 资源按需使用，不浪费

**现在整个数据管道可以正常工作了！** 🎊
