# æ•°æ®æµè¡”æ¥æ£€æŸ¥æŠ¥å‘Š

## ğŸ“‹ æ£€æŸ¥æ—¥æœŸ
2025-01-20

## ğŸ” ç³»ç»Ÿæ¦‚è§ˆ

æœ¬ç³»ç»ŸåŒ…å«4ä¸ªæ¨¡å—çš„æ•°æ®æµï¼š
```
Scraper (çˆ¬è™«) â†’ Cleaner (æ¸…æ´—) â†’ Processer (å¤„ç†) â†’ Visualization (å¯è§†åŒ–)
   |                |              |                    |
   DB0           DB0â†’DB1         DB0              DB0 è¯»å–
```

---

## âš ï¸ å‘ç°çš„é—®é¢˜

### é—®é¢˜ 1ï¸âƒ£: **Processer æ¨¡å—æ•°æ®æºé…ç½®é”™è¯¯**

**ç°çŠ¶ï¼š**
- Processer æ¨¡å—ä½¿ç”¨ **æœ¬åœ° CSV æ–‡ä»¶** ä½œä¸ºè¾“å…¥æº
- `config.py` ä¸­ï¼š`"input_file": "input_data.csv"`
- `main.py` ä¸­ï¼šç›´æ¥è°ƒç”¨ `processor.process(input_file=CONFIG["input_file"], ...)`
- **æœªä» Redis è¯»å– Cleaner å¤„ç†åçš„æ•°æ®**

**é—®é¢˜å½±å“ï¼š**
- âŒ å®Œå…¨ç»•è¿‡äº† Cleaner æ¨¡å—çš„æ¸…æ´—è¿‡ç¨‹
- âŒ æ•°æ®æµæ–­è£‚ï¼šCleaner â†’ Processer ç¯èŠ‚ä¸é€š
- âŒ æ— æ³•å®ç°å®æ—¶å¤„ç†ï¼ˆä½¿ç”¨é™æ€ CSV æ–‡ä»¶ï¼‰
- âŒ ç³»ç»Ÿä¸èƒ½å½¢æˆå®Œæ•´çš„æ•°æ®ç®¡é“

**é¢„æœŸæµç¨‹ï¼š**
```
Scraper å†™å…¥ (DB0:data_queue)
        â†“
Cleaner è¯»å– (DB0:data_queue) â†’ æ¸…æ´— â†’ å†™å…¥ (DB1:clean_data_queue)
        â†“
Processer åº”è¯¥è¯»å– (DB1:clean_data_queue) â†’ å¤„ç† â†’ å†™å…¥ (DB0:processed_data:*)
        â†“
Visualization è¯»å– (DB0:processed_data:*)
```

**å½“å‰é”™è¯¯æµç¨‹ï¼š**
```
Processer å¿½ç•¥ Redis
        â†“
ç›´æ¥ä½¿ç”¨æœ¬åœ° CSV æ–‡ä»¶ (input_data.csv)
        â†“
Cleaner æ•°æ®æ— æ³•è¢«åˆ©ç”¨
```

---

### é—®é¢˜ 2ï¸âƒ£: **Processer è¾“å‡ºå­˜å‚¨ä½ç½®æ­£ç¡®ï¼Œä½†è¾“å…¥æºæœ‰é—®é¢˜**

**å½“å‰æƒ…å†µï¼š**
- âœ… è¾“å‡ºéƒ¨åˆ†ï¼šå°†å¤„ç†ç»“æœå­˜å…¥ Redis æ­£ç¡®
  - ä½¿ç”¨ Redis Hash ç»“æ„å­˜å‚¨åˆ° `processed_data` é”®
  - å­—æ®µåŒ…æ‹¬ï¼šmetadata, trending_keywords, word_cloud, news_feed
  - å†å²æ•°æ®å­˜å…¥ `history_data` å“ˆå¸Œ

**å­˜å‚¨ä»£ç åˆ†æï¼ˆredis_manager.pyï¼‰ï¼š**
```python
# âœ… æ­£ç¡®çš„è¾“å‡ºé€»è¾‘
self.r.hset(self.processed_data_key, "metadata", json.dumps(...))
self.r.hset(self.processed_data_key, "trending_keywords", json.dumps(...))
self.r.hset(self.processed_data_key, "word_cloud", json.dumps(...))
self.r.hset(self.processed_data_key, "news_feed", json.dumps(...))
```

**é—®é¢˜ï¼š** 
- âŒ Redis è¾“å‡ºä½¿ç”¨ Hash ç»“æ„ (`hset`)
- âŒ Visualization è¯»å–ä½¿ç”¨ String ç»“æ„ (`get`)

---

### é—®é¢˜ 3ï¸âƒ£: **Redis æ•°æ®ç»“æ„ä¸åŒ¹é…**

**Processer è¾“å‡ºï¼š**
```python
# ä½¿ç”¨ Redis Hash å­˜å‚¨
self.r.hset("processed_data", "metadata", json_string)
self.r.hset("processed_data", "trending_keywords", json_string)
self.r.hset("processed_data", "word_cloud", json_string)
self.r.hset("processed_data", "news_feed", json_string)
```

**Visualization è¯»å–ï¼š**
```python
# æœŸæœ›ä½¿ç”¨ String ç»“æ„è¯»å–
metadata_json = self.redis_client.get("processed_data:metadata")  # è¯»å–ä¸åˆ°ï¼
trending_json = self.redis_client.get("processed_data:trending_keywords")  # è¯»å–ä¸åˆ°ï¼
```

**æ ¹æœ¬åŸå› ï¼š**
```
Processer å†™å…¥:     hset("processed_data", "metadata", ...)
                    â†“
                    Redis Hash ç»“æ„: {processed_data: {metadata: "...", trending_keywords: "..."}}

Visualization å°è¯•è¯»å–:  get("processed_data:metadata")
                    â†“
                    æŸ¥æ‰¾ String é”®åä¸º "processed_data:metadata" çš„å€¼
                    âŒ æ‰¾ä¸åˆ°ï¼å› ä¸ºå®é™…å­˜å‚¨çš„æ˜¯ Hashï¼Œä¸æ˜¯ String
```

---

### é—®é¢˜ 4ï¸âƒ£: **Cleaner æ¨¡å—è¾“å‡ºç›®æ ‡ä¸æ˜ç¡®**

**å½“å‰æƒ…å†µï¼š**
- Cleaner æ¨¡å—é…ç½®ä¸­ï¼š
  - `DB_IN = 0`ï¼ˆä» DB0 è¯»å–åŸå§‹æ•°æ®ï¼‰
  - `DB_OUT = 1`ï¼ˆè¾“å‡ºåˆ° DB1ï¼‰
  - `QUEUE_OUT = "clean_data_queue"`ï¼ˆè¾“å‡ºé˜Ÿåˆ—åï¼‰

**é—®é¢˜ï¼š**
- âŒ ä¸ç¡®å®šæ˜¯å¦çœŸçš„å‘ Redis å†™å…¥æ•°æ®
- âŒ å¤„ç†é€»è¾‘ä¸­å†™å…¥ä»€ä¹ˆæ•°æ®ã€ç”¨ä»€ä¹ˆ Redis ç»“æ„ï¼ˆString/Hash/Listï¼‰æ²¡æœ‰æ˜ç¡®ä»£ç å®ç°

**éœ€è¦éªŒè¯ï¼š**
```python
# Cleaner åº”è¯¥å®ç°ç±»ä¼¼çš„è¾“å‡ºé€»è¾‘
redis_client.lpush("clean_data_queue", json.dumps(cleaned_item))
# æˆ–
redis_client.set("cleaned_data:item_id", json.dumps(cleaned_item))
```

---

### é—®é¢˜ 5ï¸âƒ£: **DB åˆ†é…ä¸æ¸…æ™°**

**é…ç½®ä¸­çš„ DB åˆ†é…ï¼š**
| æ¨¡å— | è¾“å…¥ DB | è¾“å‡º DB | ç”¨é€” |
|------|--------|--------|------|
| Scraper | N/A | DB0 | å°†åŸå§‹æ•°æ®å†™å…¥ `data_queue` |
| Cleaner | DB0 | DB1 | ä» data_queue è¯»å–ï¼Œå¤„ç†åå†™å…¥ clean_data_queue |
| Processer | DB1 | DB0 | **åº”è¯¥**ä» clean_data_queue è¯»å–ï¼Œå¤„ç†åå†™å…¥ processed_data |
| Visualization | DB0 | N/A | åªè¯» processed_data |

**å½“å‰é—®é¢˜ï¼š**
- âŒ Processer é…ç½®çš„æ˜¯ DB0ï¼Œä½†åº”è¯¥ä¼˜å…ˆè¯» DB1
- âŒ Processer æ—¢è¯» DB0 åˆå†™ DB0ï¼Œä¸ Cleaner çš„ DB éš”ç¦»è®¾è®¡ä¸ç¬¦

---

## âœ… éœ€è¦ä¿®å¤çš„å†…å®¹

### ä¿®å¤ 1: æ›´æ–° Processer é…ç½®æ–‡ä»¶

**æ–‡ä»¶ï¼š** `processer/Analysis/config.py`

```python
# ä¿®æ”¹ Redis é…ç½®éƒ¨åˆ†
"redis": {
    "host": "localhost",
    "port": 6379,
    "db": 1,  # æ”¹æˆ 1ï¼Œä» Cleaner çš„è¾“å‡º DB è¯»å–
    "password": None,
    # æ–°å¢é…ç½®
    "input_queue": "clean_data_queue",  # ä» Cleaner çš„è¾“å‡ºé˜Ÿåˆ—è¯»å–
    "input_db": 1,  # Cleaner çš„è¾“å‡º DB
    "output_db": 0,  # è¾“å‡ºåˆ° DB0ï¼Œä¾› Visualization è¯»å–
    "output_channel": "processed_data",  # è¾“å‡ºé”®å‰ç¼€
}
```

### ä¿®å¤ 2: ä¿®æ”¹ Processer æ•°æ®åŠ è½½æ–¹å¼

**æ–‡ä»¶ï¼š** `processer/Analysis/data_loader.py`

éœ€è¦ä» Redis é˜Ÿåˆ—è¯»å–æ•°æ®è€Œä¸æ˜¯æœ¬åœ° CSVï¼š

```python
import redis
from config import CONFIG

class DataLoader:
    def __init__(self):
        self.config = CONFIG
        # è¿æ¥åˆ° DB1ï¼ˆCleaner è¾“å‡º DBï¼‰
        self.redis_client = redis.Redis(
            host=CONFIG["redis"]["host"],
            port=CONFIG["redis"]["port"],
            db=CONFIG["redis"]["input_db"],
            decode_responses=True
        )
    
    def load_data_from_redis(self) -> pd.DataFrame:
        """ä» Redis é˜Ÿåˆ—è¯»å–æ¸…æ´—åçš„æ•°æ®"""
        queue_name = CONFIG["redis"]["input_queue"]
        data_list = []
        
        # ä»é˜Ÿåˆ—è¯»å–æ‰€æœ‰é¡¹ç›®
        while True:
            item_json = self.redis_client.lpop(queue_name)
            if not item_json:
                break
            data_list.append(json.loads(item_json))
        
        if not data_list:
            print(f"âš ï¸  è­¦å‘Šï¼šRedis é˜Ÿåˆ— {queue_name} ä¸­æ²¡æœ‰æ•°æ®")
            return pd.DataFrame()
        
        # è½¬æ¢ä¸º DataFrame
        df = pd.DataFrame(data_list)
        return df
    
    def load_data(self, file_path: str = None) -> pd.DataFrame:
        """
        åŠ è½½æ•°æ®ï¼ˆä¼˜å…ˆä» Redisï¼Œå›é€€åˆ°æœ¬åœ°æ–‡ä»¶ï¼‰
        """
        # ä¼˜å…ˆå°è¯•ä» Redis è¯»å–
        df_redis = self.load_data_from_redis()
        if not df_redis.empty:
            print(f"âœ… ä» Redis é˜Ÿåˆ—åŠ è½½äº† {len(df_redis)} æ¡æ¸…æ´—åçš„æ•°æ®")
            return df_redis
        
        # å›é€€ï¼šä»æœ¬åœ° CSV è¯»å–
        if file_path:
            print(f"ğŸ“‚ ä»æœ¬åœ°æ–‡ä»¶åŠ è½½æ•°æ®: {file_path}")
            df = pd.read_csv(file_path)
            return df
        
        # éƒ½æ²¡æœ‰åˆ™è¿”å›ç©º DataFrame
        return pd.DataFrame()
```

### ä¿®å¤ 3: ç»Ÿä¸€ Redis æ•°æ®ç»“æ„

**å»ºè®®æ–¹æ¡ˆï¼šä½¿ç”¨ String ç»“æ„è€Œé Hash**

ä¿®æ”¹ `processer/Analysis/redis_manager.py`ï¼š

```python
def publish_processed_data(self, output_file_path=None):
    """å°†å¤„ç†åçš„æ•°æ®å‘å¸ƒåˆ° Redisï¼ˆä½¿ç”¨ String ç»“æ„ï¼‰"""
    if not self.r:
        print("Redisæœªè¿æ¥ï¼Œæ— æ³•å‘å¸ƒæ•°æ®")
        return False

    try:
        with open(output_file_path, 'r', encoding='utf-8') as f:
            processed_data = json.load(f)

        # ä¿®æ”¹ä¸º String ç»“æ„ï¼Œä½¿ç”¨ç»Ÿä¸€çš„å‰ç¼€
        # è¿™æ · Visualization èƒ½æ­£ç¡®è¯»å–
        
        # 1. å­˜å‚¨ä¸»è¦æ•°æ®åˆ° processed_data:* é”®
        self.r.set(
            "processed_data:metadata",
            json.dumps(processed_data['metadata'], ensure_ascii=False)
        )
        self.r.set(
            "processed_data:trending_keywords",
            json.dumps(processed_data['trending_keywords'], ensure_ascii=False)
        )
        self.r.set(
            "processed_data:word_cloud",
            json.dumps(processed_data['word_cloud'], ensure_ascii=False)
        )
        self.r.set(
            "processed_data:news_feed",
            json.dumps(processed_data['news_feed'], ensure_ascii=False)
        )

        # 2. å­˜å‚¨å†å²æ•°æ®åˆ°å•ç‹¬çš„é”®
        history_data = processed_data.get('history_data', {})
        for keyword, data in history_data.items():
            self.r.set(
                f"processed_data:history_data:{keyword}",
                json.dumps(data, ensure_ascii=False)
            )

        # è®¾ç½®è¿‡æœŸæ—¶é—´ï¼ˆ24å°æ—¶ï¼‰
        self.r.expire("processed_data:metadata", 24 * 60 * 60)
        # ... å…¶ä»–é”®ä¹Ÿè®¾ç½®è¿‡æœŸæ—¶é—´

        print(f"âœ… å¤„ç†åçš„æ•°æ®å·²å‘å¸ƒåˆ° Redis String ç»“æ„")
        return True

    except Exception as e:
        print(f"âŒ å‘å¸ƒæ•°æ®æ—¶å‡ºé”™: {e}")
        return False
```

### ä¿®å¤ 4: å¢å¼º Cleaner è¾“å‡ºéªŒè¯

éœ€è¦ç¡®è®¤ `cleaner/data_cleaner_module.py` ä¸­ç¡®å®å‘ Redis å†™å…¥æ•°æ®ã€‚

å»ºè®®æ·»åŠ éªŒè¯ä»£ç åˆ° Cleaner çš„ä¸»å‡½æ•°ï¼š

```python
def verify_cleaner_output(redis_client, queue_name):
    """éªŒè¯ Cleaner æ˜¯å¦æˆåŠŸå†™å…¥æ•°æ®"""
    queue_len = redis_client.llen(queue_name)
    if queue_len > 0:
        print(f"âœ… Cleaner å·²å‘ Redis é˜Ÿåˆ—å†™å…¥ {queue_len} æ¡æ•°æ®")
        # æŸ¥çœ‹æ ·æœ¬
        sample = redis_client.lindex(queue_name, 0)
        print(f"ğŸ“ æ ·æœ¬æ•°æ®: {sample[:100]}...")
    else:
        print(f"âŒ Cleaner è¾“å‡ºé˜Ÿåˆ— {queue_name} ä¸ºç©º")
    
    return queue_len
```

---

## ğŸ“Š ä¿®å¤åçš„æ•°æ®æµ

```
âœ… å®Œæ•´çš„å®æ—¶æ•°æ®ç®¡é“ï¼š

1ï¸âƒ£  Scraper (control_center.py)
    â†“
    å†™å…¥: redis.lpush("data_queue", cleaned_item)  [DB0]
    
2ï¸âƒ£  Cleaner (data_cleaner_module.py)
    â†“
    è¯»å–: clean_item = redis.lpop("data_queue")  [DB0]
    æ¸…æ´—å¤„ç†...
    å†™å…¥: redis.lpush("clean_data_queue", cleaned_item)  [DB1]
    
3ï¸âƒ£  Processer (main.py)
    â†“
    è¯»å–: cleaned_item = redis.lpop("clean_data_queue")  [DB1]
    â†“ å¤„ç†æˆè¶‹åŠ¿æ•°æ®...
    â†“
    å†™å…¥: redis.set("processed_data:*", json_data)  [DB0]
    
4ï¸âƒ£  Visualization (redis_client.py)
    â†“
    è¯»å–: trend_data = redis.get("processed_data:metadata")  [DB0]
    â†“ é€šè¿‡ WebSocket æ¨é€ç»™å‰ç«¯
```

---

## ğŸ”§ ä¿®å¤ä¼˜å…ˆçº§

| ä¼˜å…ˆçº§ | ä¿®å¤é¡¹ | å½±å“èŒƒå›´ |
|-------|--------|---------|
| ğŸ”´ é«˜ | Processer æ•°æ®æºæ”¹ä¸º Redis | ç³»ç»Ÿæ— æ³•å·¥ä½œ |
| ğŸ”´ é«˜ | ç»Ÿä¸€ Redis æ•°æ®ç»“æ„ (String vs Hash) | Visualization è¯»å–å¤±è´¥ |
| ğŸŸ¡ ä¸­ | éªŒè¯ Cleaner è¾“å‡ºåŠŸèƒ½ | æ•°æ®æµä¸­æ®µå¯èƒ½æ–­è£‚ |
| ğŸŸ¡ ä¸­ | DB åˆ†é…è°ƒæ•´ | æ•°æ®éš”ç¦»ä¸æ¸…æ™° |
| ğŸŸ¢ ä½ | æ·»åŠ æ•°æ®æµç›‘æ§å’Œæ—¥å¿— | è°ƒè¯•å’Œç»´æŠ¤æ•ˆç‡ |

---

## âœ¨ å»ºè®®çš„æµ‹è¯•æ–¹æ¡ˆ

ä¿®å¤åï¼ŒæŒ‰ç…§è¿™ä¸ªé¡ºåºæµ‹è¯•ï¼š

```bash
# 1. å¯åŠ¨ Redis
redis-server

# 2. æµ‹è¯• Scraper â†’ Redis
cd scraper
python control_center.py
# æ£€æŸ¥ DB0:data_queue æ˜¯å¦æœ‰æ•°æ®

# 3. æµ‹è¯• Cleaner â†’ Redis
cd cleaner
python data_cleaner_module.py
# æ£€æŸ¥ DB1:clean_data_queue æ˜¯å¦æœ‰æ¸…æ´—åçš„æ•°æ®

# 4. æµ‹è¯• Processer â†’ Redis
cd processer
python Analysis/main.py
# æ£€æŸ¥ DB0:processed_data:* æ˜¯å¦æœ‰å¤„ç†åçš„æ•°æ®

# 5. æµ‹è¯• Visualization è¯»å–
cd visualization/backend
python -m uvicorn app.main:app --reload
# è®¿é—® http://localhost:8000/api/v1/trendsï¼Œæ£€æŸ¥æ˜¯å¦èƒ½è¯»å–æ•°æ®

# 6. å¯åŠ¨å‰ç«¯
cd visualization/frontend
npm run dev
# æ£€æŸ¥å‰ç«¯æ˜¯å¦èƒ½å®æ—¶æ˜¾ç¤ºæ•°æ®
```

---

## ğŸ“ æ€»ç»“

ç³»ç»Ÿçš„ä¸»è¦æ•°æ®è¡”æ¥é—®é¢˜æ˜¯ï¼š

1. **Processer æ¨¡å—å®Œå…¨å¿½è§†äº† Redisï¼Œä½¿ç”¨æœ¬åœ° CSV æ–‡ä»¶** âš ï¸ æœ€ä¸¥é‡
2. **Redis æ•°æ®ç»“æ„ä¸ç»Ÿä¸€ï¼šHash vs String** âŒ å¯¼è‡´è¯»å–å¤±è´¥
3. **æ•°æ®åº“åˆ†é…é€»è¾‘ä¸æ¸…æ™°** âš ï¸ æ˜“ç»´æŠ¤æ€§å·®

å»ºè®®ç«‹å³ä¿®å¤**é—®é¢˜ 1 å’Œé—®é¢˜ 3**ï¼Œå¦åˆ™ç³»ç»Ÿæ— æ³•ä½œä¸ºå®æ—¶æ•°æ®ç®¡é“æ­£å¸¸è¿ä½œã€‚
