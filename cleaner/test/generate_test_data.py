"""
ç”Ÿæˆæµ‹è¯•æ•°æ®è„šæœ¬
ä¸º Cleaner ç”Ÿæˆ 100 æ¡æµ‹è¯•æ•°æ®åˆ° Redis DB0
"""
import redis
import json
import random
import time
from datetime import datetime, timedelta
from pathlib import Path
import yaml

# åŠ è½½é…ç½®ï¼ˆé…ç½®æ–‡ä»¶åœ¨ä¸Šä¸€å±‚ç›®å½•ï¼‰
config_path = Path(__file__).parent.parent / "config_processing.yaml"
with open(config_path, 'r', encoding='utf-8') as f:
    CONFIG = yaml.safe_load(f)

REDIS_HOST = CONFIG['redis']['host']
REDIS_PORT = CONFIG['redis']['port']
DB_IN = CONFIG['redis']['db_in']
QUEUE_IN = CONFIG['redis']['queue_in']

# æµ‹è¯•æ•°æ®æ¨¡æ¿
SOURCES = ["alphavantage", "newsapi", "reddit", "stocktwits", "twitter", "rss"]
SYMBOLS = ["AAPL", "MSFT", "GOOGL", "AMZN", "TSLA", "NVDA", "META", "AMD", "INTC", "CRM"]
DOMAINS = [
    "www.benzinga.com",
    "www.cnbc.com", 
    "www.reuters.com",
    "www.bloomberg.com",
    "finance.yahoo.com",
    "seekingalpha.com",
    "reddit.com",
    "twitter.com"
]

TITLES = [
    "Bulls And Bears: Microsoft, Joby Aviation, Meta - And Nvidia Tops $5 Trillion",
    "Stock Market Surge: Tech Giants Lead Rally",
    "Fed Rate Decision Impact on Market Volatility",
    "Earnings Report: Apple Beats Expectations",
    "Crypto Market Rebounds After Correction",
    "Market Correction Signals Economic Uncertainty",
    "Tech IPO Boom Continues in Q4",
    "Oil Prices Hit New High Amid Geopolitical Tensions",
    "Real Estate Market Shows Signs of Recovery",
    "Inflation Data Triggers Market Reaction",
    "Dividend Announcement Boosts Stock Price",
    "Merger Deal Between Two Tech Giants",
    "Bitcoin Surge Past $100K Milestone",
    "Healthcare Stock Rally on New Drug Approval",
    "Financial Sector Outperforms Market Average",
]

SUMMARIES = [
    "Wall Street extended its record-setting rally as Nvidia Corp. ( NASDAQ:NVDA ) crossed the $5 trillion market-cap milestone - a first in history.",
    "Market analysts predict continued gains in technology sector over the coming quarter.",
    "Federal Reserve signals potential interest rate cuts in 2024.",
    "Major corporation reports better-than-expected quarterly earnings.",
    "Cryptocurrency market shows signs of recovery after recent downturn.",
    "Economic indicators suggest slower growth ahead for the economy.",
    "New product launch drives consumer enthusiasm for major retailer.",
    "Supply chain improvements lead to higher profit margins.",
    "Market volatility expected as earnings season concludes.",
    "International trade developments impact commodity prices.",
    "Tech startups attract record venture capital investments.",
    "Banking sector shows resilience amid economic headwinds.",
    "Real estate market benefits from lower mortgage rates.",
    "Energy sector transforms with renewable energy investments.",
    "Retail sales data indicates strong consumer spending.",
]

TEXT_SAMPLES = [
    "Bulls And Bears: Microsoft, Joby Aviation, Meta - And Nvidia Tops $5 Trillion - Apple ( NASDAQ:AAPL ), Amazon.com ( NASDAQ:AMZN ). Benzinga examined the prospects for many investors' favorite stocks over the last week - here's a look at some of our top stories. Wall Street extended its record-setting rally as Nvidia Corp. ( NASDAQ:NVDA ) crossed the $5 trillion market-cap milestone - a first in history.",
    "The stock market continues to show strength as major indices reach new all-time highs. Investors remain optimistic about corporate earnings and economic growth prospects. Technology stocks lead the gains with strong performance from cloud computing and artificial intelligence companies.",
    "Financial markets respond positively to recent economic data showing resilience in consumer spending and business investment. Market analysts maintain their bullish outlook for the remainder of the year despite some lingering concerns about inflation.",
    "Trading volumes remain elevated as institutional investors continue to rotate into growth stocks. The semiconductor sector shows particular strength driven by increased demand for AI chips and computing infrastructure upgrades.",
    "Market sentiment improves on dovish Fed commentary suggesting patience with interest rate policy. Bond markets rally on expectations of potential rate cuts in the coming year, supporting equity valuations.",
]

URLS = [
    "https://www.benzinga.com/markets/market-summary/25/11/48578014/benzinga-bulls-and-bears-microsoft-joby-aviation-meta-and-nvidia-tops-5-trillion",
    "https://www.cnbc.com/markets/stocks/",
    "https://www.reuters.com/finance/",
    "https://finance.yahoo.com/",
    "https://seekingalpha.com/market-news/",
    "https://reddit.com/r/stocks/",
    "https://www.bloomberg.com/quote/",
]

AUTHORS = [
    "Benzinga Senior Editor",
    "Financial Times Reporter",
    "Reuters Market Correspondent",
    "Bloomberg Analyst",
    "MarketWatch Columnist",
    "Seeking Alpha Contributor",
    "CNBC Producer",
    "Financial News Desk",
]

def generate_test_data(count: int = 500) -> list:
    """
    ç”Ÿæˆå…¨é¢çš„æµ‹è¯•æ•°æ®ï¼ŒåŒ…å«ï¼š
    - é‡å¤æ•°æ®ï¼ˆæµ‹è¯•å»é‡åŠŸèƒ½ï¼‰
    - ç¼ºå°‘å¿…è¦å­—æ®µçš„æ•°æ®ï¼ˆæµ‹è¯•éªŒè¯åŠŸèƒ½ï¼‰
    - è¶…è¿‡24å°æ—¶çš„æ•°æ®ï¼ˆæµ‹è¯•æ—¶é—´çª—å£æ¸…ç†ï¼‰
    - æ­£å¸¸æ•°æ®
    
    Args:
        count: ç”Ÿæˆçš„æ•°æ®æ¡æ•°
        
    Returns:
        æµ‹è¯•æ•°æ®åˆ—è¡¨
    """
    test_data = []
    
    # æ—¶é—´æˆ³èŒƒå›´
    now = time.time()
    week_ago = now - (7 * 24 * 3600)  # 7å¤©å‰
    hours_25_ago = now - (25 * 3600)  # 25å°æ—¶å‰ï¼ˆè¶…è¿‡24å°æ—¶ï¼‰
    hours_1_ago = now - (1 * 3600)    # 1å°æ—¶å‰ï¼ˆåœ¨24å°æ—¶å†…ï¼‰
    
    print(f"ğŸ“Š å¼€å§‹ç”Ÿæˆ {count} æ¡å…¨é¢æµ‹è¯•æ•°æ®...")
    print(f"  - æ—¶é—´èŒƒå›´: {datetime.fromtimestamp(week_ago)} ~ {datetime.fromtimestamp(now)}")
    print(f"  - 24å°æ—¶åˆ†ç•Œçº¿: {datetime.fromtimestamp(hours_25_ago)}")
    
    # æ•°æ®åˆ†å¸ƒè®¡åˆ’
    normal_count = int(count * 0.60)      # 60% æ­£å¸¸æ•°æ®
    duplicate_count = int(count * 0.15)   # 15% é‡å¤æ•°æ®
    invalid_count = int(count * 0.10)     # 10% æ— æ•ˆæ•°æ®ï¼ˆç¼ºå°‘å¿…è¦å­—æ®µï¼‰
    old_count = int(count * 0.15)         # 15% è¶…è¿‡24å°æ—¶çš„æ•°æ®
    
    print(f"  - æ­£å¸¸æ•°æ®: {normal_count} æ¡")
    print(f"  - é‡å¤æ•°æ®: {duplicate_count} æ¡")
    print(f"  - æ— æ•ˆæ•°æ®: {invalid_count} æ¡")
    print(f"  - è¶…æ—¶æ•°æ®: {old_count} æ¡")
    print()
    
    # 1. ç”Ÿæˆæ­£å¸¸æ•°æ®
    print("ğŸ”§ ç”Ÿæˆæ­£å¸¸æ•°æ®...")
    for i in range(normal_count):
        timestamp = random.uniform(hours_1_ago, now)  # æœ€è¿‘1å°æ—¶
        dt = datetime.fromtimestamp(timestamp)
        
        data = {
            "id": f"normal_{i+1:05d}",
            "text": random.choice(TEXT_SAMPLES),
            "source": random.choice(SOURCES),
            "timestamp": int(timestamp),
            "url": f"{random.choice(URLS)}?id=normal_{i+1}",
            "symbol": random.choice(SYMBOLS),
            "title": f"{random.choice(TITLES)} #normal_{i+1}",
            "summary": random.choice(SUMMARIES),
            "source_domain": random.choice(DOMAINS),
            "authors": random.choice(AUTHORS),
            "published_at": dt.strftime("%Y%m%dT%H%M%S"),
            "score": random.randint(0, 1000),
            "comments": random.randint(0, 500),
            "created_at": dt.isoformat() + "Z",
            "tags": random.sample(["tech", "market", "stocks", "crypto", "finance"], k=random.randint(1, 3)),
        }
        
        # éšæœºæ·»åŠ å­—æ®µå˜åŒ–
        if random.random() < 0.3:
            data["sentiment"] = random.choice(["Bullish", "Bearish", "Neutral"])
        if random.random() < 0.3:
            data["content"] = data["text"]
        if random.random() < 0.2:
            data["post_id"] = data["id"]
        if random.random() < 0.2:
            data["tweet_id"] = data["id"]
        
        test_data.append(data)
    
    # 2. ç”Ÿæˆé‡å¤æ•°æ®ï¼ˆåŸºäºå‰é¢çš„æ­£å¸¸æ•°æ®ï¼‰
    print("ğŸ”„ ç”Ÿæˆé‡å¤æ•°æ®...")
    base_data_for_dups = test_data[:min(20, normal_count)]  # ç”¨å‰20æ¡ä½œä¸ºé‡å¤çš„åŸºç¡€
    
    for i in range(duplicate_count):
        # é€‰æ‹©ä¸€ä¸ªåŸºç¡€æ•°æ®è¿›è¡Œ"é‡å¤"
        base_data = random.choice(base_data_for_dups).copy()
        
        # ä¿®æ”¹ä¸€äº›ä¸å½±å“å»é‡åˆ¤æ–­çš„å­—æ®µ
        base_data["timestamp"] = int(random.uniform(hours_1_ago, now))
        base_data["score"] = random.randint(0, 1000)
        base_data["comments"] = random.randint(0, 500)
        
        # é‡å¤ç±»å‹ï¼š
        if i % 3 == 0:
            # å®Œå…¨ç›¸åŒçš„ID
            pass  # ä¿æŒåŸIDä¸å˜
        elif i % 3 == 1:
            # ç›¸åŒçš„title+sourceç»„åˆï¼ˆä¼šç”Ÿæˆç›¸åŒçš„å“ˆå¸Œï¼‰
            base_data.pop("id", None)  # åˆ é™¤IDï¼Œè®©ç³»ç»Ÿç”¨title+sourceç”Ÿæˆå“ˆå¸Œ
        else:
            # ç›¸åŒçš„URL
            base_data["id"] = f"dup_{i+1:05d}_different_id"  # ä¸åŒID
            # ä½†ä¿æŒç›¸åŒçš„ title å’Œ sourceï¼Œä¼šç”Ÿæˆç›¸åŒå“ˆå¸Œ
        
        test_data.append(base_data)
    
    # 3. ç”Ÿæˆæ— æ•ˆæ•°æ®ï¼ˆç¼ºå°‘å¿…è¦å­—æ®µï¼‰
    print("âŒ ç”Ÿæˆæ— æ•ˆæ•°æ®...")
    for i in range(invalid_count):
        timestamp = random.uniform(hours_1_ago, now)
        dt = datetime.fromtimestamp(timestamp)
        
        # éšæœºç¼ºå°‘å¿…è¦å­—æ®µ
        data = {
            "id": f"invalid_{i+1:05d}",
            "timestamp": int(timestamp),
            "url": f"{random.choice(URLS)}?id=invalid_{i+1}",
            "published_at": dt.strftime("%Y%m%dT%H%M%S"),
        }
        
        invalid_type = i % 4
        if invalid_type == 0:
            # ç¼ºå°‘ source å­—æ®µ
            data["text"] = random.choice(TEXT_SAMPLES)
            data["title"] = f"No Source Data #{i+1}"
        elif invalid_type == 1:
            # ç¼ºå°‘æ‰€æœ‰æ–‡æœ¬å­—æ®µï¼ˆtext, content, titleï¼‰
            data["source"] = random.choice(SOURCES)
        elif invalid_type == 2:
            # source ä¸ºç©º
            data["source"] = ""
            data["text"] = random.choice(TEXT_SAMPLES)
        else:
            # æ–‡æœ¬å­—æ®µä¸ºç©º
            data["source"] = random.choice(SOURCES)
            data["text"] = ""
            data["content"] = ""
            data["title"] = ""
        
        test_data.append(data)
    
    # 4. ç”Ÿæˆè¶…è¿‡24å°æ—¶çš„æ—§æ•°æ®
    print("â° ç”Ÿæˆè¶…è¿‡24å°æ—¶çš„æ—§æ•°æ®...")
    for i in range(old_count):
        # ç”Ÿæˆ25å°æ—¶åˆ°7å¤©å‰çš„æ—¶é—´æˆ³
        timestamp = random.uniform(week_ago, hours_25_ago)
        dt = datetime.fromtimestamp(timestamp)
        
        data = {
            "id": f"old_{i+1:05d}",
            "text": f"è¿™æ˜¯è¶…è¿‡24å°æ—¶çš„æ—§æ•°æ® #{i+1}: {random.choice(TEXT_SAMPLES)}",
            "source": random.choice(SOURCES),
            "timestamp": int(timestamp),
            "url": f"{random.choice(URLS)}?id=old_{i+1}",
            "symbol": random.choice(SYMBOLS),
            "title": f"[æ—§æ•°æ®] {random.choice(TITLES)} #old_{i+1}",
            "summary": f"[{dt.strftime('%Y-%m-%d %H:%M')}] {random.choice(SUMMARIES)}",
            "source_domain": random.choice(DOMAINS),
            "authors": random.choice(AUTHORS),
            "published_at": dt.strftime("%Y%m%dT%H%M%S"),
            "score": random.randint(0, 1000),
            "comments": random.randint(0, 500),
            "created_at": dt.isoformat() + "Z",
            "tags": ["old_data", "test"] + random.sample(["tech", "market", "stocks"], k=random.randint(0, 2)),
        }
        
        test_data.append(data)
    
    # 5. è¡¥é½åˆ°æŒ‡å®šæ•°é‡ï¼ˆå¦‚æœæœ‰ç¼ºå°‘çš„ï¼‰
    remaining = count - len(test_data)
    if remaining > 0:
        print(f"ğŸ”§ è¡¥é½å‰©ä½™ {remaining} æ¡æ•°æ®...")
        for i in range(remaining):
            timestamp = random.uniform(hours_1_ago, now)
            dt = datetime.fromtimestamp(timestamp)
            
            data = {
                "id": f"extra_{i+1:05d}",
                "text": random.choice(TEXT_SAMPLES),
                "source": random.choice(SOURCES),
                "timestamp": int(timestamp),
                "url": f"{random.choice(URLS)}?id=extra_{i+1}",
                "title": f"Extra Data #{i+1}",
                "created_at": dt.isoformat() + "Z",
            }
            test_data.append(data)
    
    # æ‰“ä¹±é¡ºåº
    random.shuffle(test_data)
    
    print(f"âœ… ç”Ÿæˆå®Œæˆï¼æ€»å…± {len(test_data)} æ¡æ•°æ®")
    print()
    
    return test_data

def push_to_redis(test_data: list) -> dict:
    """
    æ¨é€æµ‹è¯•æ•°æ®åˆ° Redis
    
    Args:
        test_data: æµ‹è¯•æ•°æ®åˆ—è¡¨
        
    Returns:
        æ¨é€ç»“æœç»Ÿè®¡
    """
    try:
        r = redis.Redis(
            host=REDIS_HOST,
            port=REDIS_PORT,
            db=DB_IN,
            decode_responses=True
        )
        
        stats = {
            'total': len(test_data),
            'pushed': 0,
            'failed': 0,
            'errors': []
        }
        
        # æ¸…ç©ºç°æœ‰é˜Ÿåˆ—ï¼ˆå¯é€‰ï¼‰
        old_count = r.llen(QUEUE_IN)
        if old_count > 0:
            r.delete(QUEUE_IN)
            print(f"âœ“ æ¸…ç©ºæ—§æ•°æ®: {old_count} æ¡")
        
        # æ¨é€æ–°æ•°æ®
        for idx, data in enumerate(test_data, 1):
            try:
                json_str = json.dumps(data, ensure_ascii=False)
                r.rpush(QUEUE_IN, json_str)
                stats['pushed'] += 1
                
                if idx % 20 == 0:
                    print(f"  å·²æ¨é€: {idx}/{len(test_data)}")
                    
            except Exception as e:
                stats['failed'] += 1
                stats['errors'].append(f"æ•°æ® #{idx}: {str(e)}")
        
        return stats
        
    except Exception as e:
        print(f"âŒ Redis è¿æ¥å¤±è´¥: {e}")
        return None

def send_crawler_complete_notification() -> bool:
    """
    å‘é€çˆ¬è™«å®Œæˆé€šçŸ¥ç»™ Cleaner
    æ¨¡æ‹Ÿ Scraper çš„é€šçŸ¥æ¶ˆæ¯
    
    Returns:
        æ˜¯å¦å‘é€æˆåŠŸ
    """
    try:
        r = redis.Redis(
            host=REDIS_HOST,
            port=REDIS_PORT,
            db=DB_IN,
            decode_responses=True
        )
        
        # è·å–é˜Ÿåˆ—é•¿åº¦
        queue_length = r.llen(QUEUE_IN)
        
        # æ„é€ é€šçŸ¥æ¶ˆæ¯ï¼ˆä¸ Scraper çš„æ¶ˆæ¯æ ¼å¼ä¸€è‡´ï¼‰
        notification = {
            "message": "crawler_complete",
            "timestamp": int(time.time()),
            "statistics": {
                "total_items": queue_length,
                "timestamp": int(time.time())
            }
        }
        
        # å‘å¸ƒé€šçŸ¥åˆ° crawler_complete é¢‘é“
        channel = "crawler_complete"
        result = r.publish(channel, json.dumps(notification, ensure_ascii=False))
        
        return result > 0
        
    except Exception as e:
        print(f"âŒ å‘é€é€šçŸ¥å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("ğŸ”§ Cleaner å…¨é¢æµ‹è¯•æ•°æ®ç”Ÿæˆå·¥å…·")
    print("=" * 80)
    print("æœ¬å·¥å…·å°†ç”ŸæˆåŒ…å«ä»¥ä¸‹ç±»å‹çš„æµ‹è¯•æ•°æ®:")
    print("âœ“ 60% æ­£å¸¸æ•°æ®ï¼ˆæœ‰æ•ˆä¸”åœ¨24å°æ—¶å†…ï¼‰")
    print("âœ“ 15% é‡å¤æ•°æ®ï¼ˆæµ‹è¯•å»é‡åŠŸèƒ½ï¼‰")
    print("âœ“ 10% æ— æ•ˆæ•°æ®ï¼ˆç¼ºå°‘å¿…è¦å­—æ®µï¼‰")
    print("âœ“ 15% è¶…æ—¶æ•°æ®ï¼ˆè¶…è¿‡24å°æ—¶ï¼Œæµ‹è¯•æ¸…ç†åŠŸèƒ½ï¼‰")
    print()
    
    # 1. ç”Ÿæˆæµ‹è¯•æ•°æ®
    print("ğŸ“Š ç”Ÿæˆæµ‹è¯•æ•°æ®...")
    print("-" * 80)
    test_data = generate_test_data(count=500)  # ç”Ÿæˆ500æ¡æ•°æ®
    
    # ç»Ÿè®¡æ•°æ®ç±»å‹ï¼ˆå®‰å…¨åœ°æ£€æŸ¥IDå­—æ®µï¼‰
    normal_data = [d for d in test_data if d.get('id', '').startswith('normal_')]
    duplicate_data = [d for d in test_data if 'dup_' in d.get('id', '') or 
                     any(base_d for base_d in normal_data[:20] 
                         if (d.get('id') == base_d.get('id') and d != base_d) or
                            (d.get('title') == base_d.get('title') and 
                             d.get('source') == base_d.get('source') and d != base_d))]
    invalid_data = [d for d in test_data if d.get('id', '').startswith('invalid_')]
    old_data = [d for d in test_data if d.get('id', '').startswith('old_')]
    
    print(f"âœ… æ•°æ®ç”Ÿæˆå®Œæˆï¼")
    print(f"  - æ€»æ•°: {len(test_data)} æ¡")
    print(f"  - æ­£å¸¸æ•°æ®: {len(normal_data)} æ¡")
    print(f"  - é‡å¤æ•°æ®: {len(duplicate_data)} æ¡")
    print(f"  - æ— æ•ˆæ•°æ®: {len(invalid_data)} æ¡") 
    print(f"  - è¶…æ—¶æ•°æ®: {len(old_data)} æ¡")
    print()
    
    # æ˜¾ç¤ºæ•°æ®æ ·æœ¬
    print("ğŸ“ æ•°æ®æ ·æœ¬:")
    print("-" * 80)
    
    # æ˜¾ç¤ºæ­£å¸¸æ•°æ®æ ·æœ¬
    if normal_data:
        print("ğŸŸ¢ æ­£å¸¸æ•°æ®æ ·æœ¬:")
        sample = normal_data[0]
        print(f"  ID: {sample.get('id', 'N/A')}")
        print(f"  Source: {sample.get('source', 'N/A')}")
        print(f"  Title: {sample.get('title', 'N/A')[:50]}...")
        print(f"  Text: {sample.get('text', 'N/A')[:50]}...")
        print(f"  Timestamp: {sample.get('timestamp', 'N/A')} ({datetime.fromtimestamp(sample['timestamp']) if sample.get('timestamp') else 'N/A'})")
        print()
    
    # æ˜¾ç¤ºæ— æ•ˆæ•°æ®æ ·æœ¬
    if invalid_data:
        print("ğŸ”´ æ— æ•ˆæ•°æ®æ ·æœ¬:")
        sample = invalid_data[0]
        print(f"  ID: {sample.get('id', 'âŒ ç¼ºå¤±')}")
        print(f"  Source: {sample.get('source', 'âŒ ç¼ºå¤±')}")
        text_value = sample.get('text', 'âŒ ç¼ºå¤±')
        print(f"  Text: {text_value[:30] if text_value != 'âŒ ç¼ºå¤±' else text_value}...")
        title_value = sample.get('title', 'âŒ ç¼ºå¤±')
        print(f"  Title: {title_value[:30] if title_value != 'âŒ ç¼ºå¤±' else title_value}...")
        print()
    
    # æ˜¾ç¤ºè¶…æ—¶æ•°æ®æ ·æœ¬
    if old_data:
        print("ğŸ• è¶…æ—¶æ•°æ®æ ·æœ¬:")
        sample = old_data[0]
        timestamp = sample.get('timestamp')
        if timestamp:
            dt = datetime.fromtimestamp(timestamp)
            hours_ago = (time.time() - timestamp) / 3600
            print(f"  ID: {sample.get('id', 'N/A')}")
            print(f"  Source: {sample.get('source', 'N/A')}")
            print(f"  Title: {sample.get('title', 'N/A')[:50]}...")
            print(f"  æ—¶é—´: {dt} ({hours_ago:.1f} å°æ—¶å‰)")
        else:
            print(f"  ID: {sample.get('id', 'N/A')}")
            print(f"  âŒ ç¼ºå°‘æ—¶é—´æˆ³")
        print()
    
    # 2. æ¨é€åˆ° Redis
    print("ğŸ“¤ æ¨é€åˆ° Redis...")
    print("-" * 80)
    print(f"è¿æ¥: {REDIS_HOST}:{REDIS_PORT} (DB{DB_IN})")
    print(f"é˜Ÿåˆ—: {QUEUE_IN}")
    print()
    
    stats = push_to_redis(test_data)
    
    if stats:
        print(f"âœ… æ¨é€å®Œæˆ!")
        print(f"  - æ€»æ•°: {stats['total']}")
        print(f"  - æˆåŠŸ: {stats['pushed']}")
        print(f"  - å¤±è´¥: {stats['failed']}")
        
        if stats['errors']:
            print(f"  - é”™è¯¯ä¿¡æ¯:")
            for error in stats['errors'][:5]:
                print(f"    â€¢ {error}")
    print()
    
    # 3. éªŒè¯æ•°æ®
    print("âœ… éªŒè¯...")
    print("-" * 80)
    try:
        r = redis.Redis(
            host=REDIS_HOST,
            port=REDIS_PORT,
            db=DB_IN,
            decode_responses=True
        )
        queue_len = r.llen(QUEUE_IN)
        print(f"âœ“ é˜Ÿåˆ— {QUEUE_IN} ä¸­æœ‰ {queue_len} æ¡æ•°æ®")
        
        if queue_len > 0:
            # éšæœºæŠ½æ ·éªŒè¯
            sample_indices = random.sample(range(min(queue_len, 100)), min(3, queue_len))
            print(f"âœ“ éšæœºæŠ½æ ·éªŒè¯ (ä½ç½®: {sample_indices}):")
            
            for idx in sample_indices:
                sample = r.lindex(QUEUE_IN, idx)
                sample_data = json.loads(sample)
                print(f"  ä½ç½® {idx}:")
                print(f"    - ID: {sample_data.get('id')}")
                print(f"    - Source: {sample_data.get('source', 'âŒ ç¼ºå¤±')}")
                print(f"    - æœ‰æ–‡æœ¬: {'âœ“' if sample_data.get('text') or sample_data.get('title') or sample_data.get('content') else 'âŒ'}")
                
                timestamp = sample_data.get('timestamp')
                if timestamp:
                    hours_ago = (time.time() - timestamp) / 3600
                    print(f"    - æ—¶é—´: {hours_ago:.1f} å°æ—¶å‰")
        
    except Exception as e:
        print(f"âŒ éªŒè¯å¤±è´¥: {e}")
    
    print()
    
    # 4. å‘é€æ¸…æ´—é€šçŸ¥
    print("ğŸ“¢ å‘é€æ¸…æ´—é€šçŸ¥...")
    print("-" * 80)
    print("æ­£åœ¨å‘ Cleaner å‘é€ 'crawler_complete' é€šçŸ¥...")
    print()
    
    if send_crawler_complete_notification():
        print("âœ… é€šçŸ¥å·²å‘é€ï¼")
        print("  Cleaner åº”è¯¥ç«‹å³å¼€å§‹æ¸…æ´—æ•°æ®")
    else:
        print("âš ï¸  é€šçŸ¥å¯èƒ½æœªè¢«æ¥æ”¶")
        print("  è¯·ç¡®ä¿ Cleaner æ­£åœ¨è¿è¡Œ")
    
    print()
    print("=" * 80)
    print("ğŸ§ª æµ‹è¯•é¢„æœŸç»“æœ:")
    print("=" * 80)
    print("Cleaner å¤„ç†åï¼Œä½ åº”è¯¥çœ‹åˆ°:")
    print(f"âœ“ çº¦ {len(normal_data)} æ¡æ­£å¸¸æ•°æ®è¢«æ¸…æ´—")
    print(f"âŒ çº¦ {len(duplicate_data)} æ¡é‡å¤æ•°æ®è¢«è¿‡æ»¤ï¼ˆå»é‡ï¼‰")
    print(f"âŒ çº¦ {len(invalid_data)} æ¡æ— æ•ˆæ•°æ®è¢«æ‹’ç»ï¼ˆéªŒè¯å¤±è´¥ï¼‰")
    print(f"âŒ çº¦ {len(old_data)} æ¡è¶…æ—¶æ•°æ®å¯èƒ½è¢«æ¸…ç†ï¼ˆå¦‚æœå¯ç”¨æ—¶é—´çª—å£æ¸…ç†ï¼‰")
    print()
    print("ğŸ’¡ ç›‘æ§å‘½ä»¤:")
    print("=" * 80)
    print("1. æŸ¥çœ‹ Cleaner æ—¥å¿—:")
    print("   tail -f cleaner/logs/event_driven_cleaner.log")
    print()
    print("2. æŸ¥çœ‹æ¸…æ´—ç»“æœé˜Ÿåˆ—:")
    print("   redis-cli -n 1 LLEN clean_data_queue")
    print("   redis-cli -n 1 LRANGE clean_data_queue 0 2")
    print()
    print("3. æŸ¥çœ‹å»é‡ç¼“å­˜:")
    print("   redis-cli -n 1 SCARD \"set:cleaned_ids\"")
    print("   redis-cli -n 1 SMEMBERS \"set:cleaned_ids\" | head -10")
    print()
    print("4. æŸ¥çœ‹æ¸…æ´—ç»Ÿè®¡ï¼ˆå¦‚æœå¯ç”¨ï¼‰:")
    print("   redis-cli -n 1 GET \"stats:accepted\"")
    print("   redis-cli -n 1 GET \"stats:discarded\"")
    print()
    print("=" * 80)

if __name__ == "__main__":
    main()
