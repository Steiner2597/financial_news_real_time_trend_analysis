# 🔴 实时流 vs 📊 批量爬虫 - 日志区分说明

## 📺 终端显示示例

当同时运行实时流和批量爬虫时，你会看到：

```
================================================================
🔴 实时流式监听（后台线程）已启动
================================================================
  📡 延迟: < 1分钟
  🔄 模式: 持续监听（与批量爬虫并行）
  🛡️  保护: 自动去重 + 自动重连
  💡 提示: 日志前缀 '🔴 [实时流]' 区分实时捕获的数据
================================================================

开始爬取...
----------------------------------------------------------------
📊 [批量爬虫] 正在抓取子版块: r/wallstreetbets
  🔥 抓取上升趋势帖子（Rising）...
📊 [批量] [1/100] Bitcoin hits new high...
📊 [批量] [2/100] Stock market analysis...
🔴 [实时流] r/CryptoCurrency | ETH breaks $2000... | 👍150  ← 实时捕获！
📊 [批量] [3/100] Federal Reserve announces...
🔴 [实时流] r/wallstreetbets | GME to the moon... | 👍2500  ← 又捕获！
📊 [批量] [4/100] Tesla stock surge...
  📰 抓取最新帖子（New）...
📊 [批量] [5/100] Oil prices drop...
```

---

## 🔍 日志前缀说明

| 前缀 | 含义 | 数据延迟 | 运行频率 |
|-----|------|---------|---------|
| **🔴 [实时流]** | 实时流式监听捕获的新帖子 | < 1分钟 | 持续监听 |
| **📊 [批量]** | 批量爬虫抓取的帖子 | 1-2小时 | 每10分钟 |

---

## ⚙️ 运行机制

### 📊 批量爬虫（主线程）
- **运行频率**: 每 10 分钟一轮
- **数据来源**: 
  - `subreddit.rising()` - 上升趋势帖子（1-2小时前）
  - `subreddit.new()` - 最新帖子（1-2小时前）
  - `subreddit.search()` - 搜索结果（8-24小时前）
- **日志格式**: `📊 [批量] [1/100] 标题...`

### 🔴 实时流（后台线程）
- **运行频率**: 持续监听，实时捕获
- **数据来源**: 
  - `subreddit.stream.submissions()` - Reddit Stream API
- **数据延迟**: **< 1分钟**（真正的实时）
- **日志格式**: `🔴 [实时流] r/子版块 | 标题... | 👍点赞数`

---

## 🛡️ 冲突保护机制

### 1️⃣ **Redis 去重**
两个爬虫都会检查 `redis:post:{post_id}` 键：
- 如果已存在 → 跳过
- 如果不存在 → 写入数据 + 标记已处理

**竞态条件**（极少发生）：
- 两个爬虫可能同时检测到"未处理"
- 同时写入 Redis → 产生重复数据
- **但清洗器会再次去重**，最终数据不会重复

### 2️⃣ **内存缓存**
实时流维护 10,000 条 ID 的内存缓存：
```python
self.processed_ids = set()  # 内存去重
if submission.id in self.processed_ids:
    continue  # 跳过
```

### 3️⃣ **清洗器去重**
即使 Redis 写入了重复数据，清洗器也会：
- 计算文章指纹（标题+内容 hash）
- 过滤重复文章
- 只保留一条

---

## 📊 实际效果

### 场景 1: 新热帖发布
```
时间轴:
T+0:00  → 用户在 r/wallstreetbets 发帖 "GME to the moon"
T+0:30  → 🔴 实时流捕获（30秒延迟）
         日志: 🔴 [实时流] r/wallstreetbets | GME to the moon... | 👍5
T+1:30  → 帖子被顶到 rising
T+10:00 → 📊 批量爬虫检测到（但已被标记为已处理）
         日志: ⏭️ 跳过已抓取帖子: abc123
```

### 场景 2: 历史热帖
```
时间轴:
8小时前 → 用户发帖 "Bitcoin hits new high"（现在很热门）
T+0:00  → 📊 批量爬虫从 rising 抓取
         日志: 📊 [批量] [1/100] Bitcoin hits new high...
T+0:01  → 🔴 实时流检测到（但已被标记为已处理）
         日志: （无输出，内部跳过）
```

---

## 🎯 推荐配置

### 只想要实时数据（< 1分钟延迟）
```yaml
# config.yaml
reddit:
  stream_enabled: true      # 启用实时流
  enabled: false            # 禁用批量爬虫
```

### 想要完整覆盖（实时 + 历史）
```yaml
# config.yaml
reddit:
  stream_enabled: true      # 启用实时流（捕获最新）
  enabled: true             # 启用批量爬虫（补充历史）
```

**推荐**: 同时启用两者，实现"边爬边洗边处理"管道 ✅

---

## 🧪 验证方法

### 1. 检查日志前缀
```bash
cd scraper
python control_center.py
# 选择 3 (循环模式)

# 观察日志:
# ✅ 看到 "🔴 [实时流]" → 实时流正常工作
# ✅ 看到 "📊 [批量]" → 批量爬虫正常工作
```

### 2. 检查延迟
在 Reddit 发布测试帖子，观察多久被捕获：
- 🔴 实时流: 30-60秒
- 📊 批量爬虫: 1-2小时

### 3. 检查去重
```bash
cd scraper
python view_redis_data.py
# 查看 Redis 中的数据，检查是否有重复 ID
```

---

## ❓ 常见问题

### Q1: 日志太多，看不清？
**A**: 可以禁用批量爬虫的详细日志：
```python
# reddit_crawler.py
logger.info(f"📊 [批量] [1/100] ...") 
# 改为
logger.debug(f"📊 [批量] [1/100] ...")  # 只在 debug 模式显示
```

### Q2: 两个爬虫会抢带宽吗？
**A**: 不会，Reddit API 限流是按账号计算：
- 每分钟 60 次请求
- PRAW 自动限流
- 两个爬虫共享配额

### Q3: 实时流会漏掉数据吗？
**A**: 不会，Stream API 保证：
- 服务器推送所有新帖子
- 客户端断线重连后自动补齐
- `skip_existing=True` 跳过启动前的旧帖子

---

## 🎉 总结

✅ **会显示在终端** - 用不同前缀区分  
✅ **有去重保护** - Redis + 内存 + 清洗器三重去重  
✅ **不会真正冲突** - 最终数据不会重复  
✅ **互补效果** - 实时流捕获最新，批量补充历史  

**推荐**: 同时运行，获得最完整的数据覆盖！ 🚀
