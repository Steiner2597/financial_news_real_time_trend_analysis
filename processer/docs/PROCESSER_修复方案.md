# Processer æ¨¡å—ä¿®å¤æ–¹æ¡ˆ

## æ¦‚è§ˆ

æœ¬æ–‡æ¡£æä¾›äº†å°† Processer æ¨¡å—ä»**æœ¬åœ° CSV æ–‡ä»¶å¤„ç†**æ”¹ä¸º**å®æ—¶ Redis æµå¤„ç†**çš„å®Œæ•´ä¿®å¤æ–¹æ¡ˆã€‚

---

## ä¿®å¤æ–¹æ¡ˆè¯¦è§£

### ç¬¬ä¸€æ­¥ï¼šä¿®æ”¹é…ç½®æ–‡ä»¶

**æ–‡ä»¶è·¯å¾„ï¼š** `processer/Analysis/config.py`

**ä¿®æ”¹å†…å®¹ï¼š**

```python
# é…ç½®æ–‡ä»¶
CONFIG = {
    # åŸæœ‰é…ç½®...
    "input_file": "input_data.csv",  # ä¿ç•™ä¸ºå¤‡ä»½/å›é€€æ–¹æ¡ˆ
    "output_file": "output_data.json",
    "trending_keywords_count": 10,
    "word_cloud_count": 20,
    "history_hours": 24,
    "history_interval_minutes": 30,

    # ==================== ä¿®æ”¹ï¼šRedisé…ç½® ====================
    "redis": {
        "host": "localhost",
        "port": 6379,
        
        # ğŸ’¡ æ–°å¢ï¼šæ˜ç¡®çš„æ•°æ®æµé…ç½®
        "input_db": 1,              # ä» DB1 è¯»å–ï¼ˆCleaner çš„è¾“å‡ºï¼‰
        "output_db": 0,             # è¾“å‡ºåˆ° DB0ï¼ˆVisualization çš„è¾“å…¥ï¼‰
        
        # ğŸ’¡ æ–°å¢ï¼šé˜Ÿåˆ—åé…ç½®
        "input_queue": "clean_data_queue",    # ä» Cleaner çš„è¾“å‡ºé˜Ÿåˆ—è¯»å–
        "output_prefix": "processed_data",    # è¾“å‡ºé”®çš„å‰ç¼€
        
        # å¯é€‰ï¼šå‘å¸ƒè®¢é˜…ï¼ˆç”¨äºå®æ—¶é€šçŸ¥ï¼‰
        "publish_channel": "processed_data_updates",
        
        # è¿‡æœŸæ—¶é—´
        "key_ttl_seconds": 86400,  # 24å°æ—¶
        
        "password": None,
    },

    "stop_words": [
        "the", "a", "an", "and", "or", "but", "in", "on", "at", "to", "for",
        # ... ä¿æŒåŸæ · ...
    ]
}
```

---

### ç¬¬äºŒæ­¥ï¼šä¿®æ”¹æ•°æ®åŠ è½½å™¨

**æ–‡ä»¶è·¯å¾„ï¼š** `processer/Analysis/data_loader.py`

**æ›¿æ¢ä¸ºï¼š**

```python
import json
import redis
import pandas as pd
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from config import CONFIG


class DataLoader:
    """æ•°æ®åŠ è½½å™¨ - æ”¯æŒ Redis å®æ—¶æµå’Œæœ¬åœ°æ–‡ä»¶ä¸¤ç§æ¨¡å¼"""

    def __init__(self):
        self.config = CONFIG
        self._init_redis()

    def _init_redis(self):
        """åˆå§‹åŒ– Redis è¿æ¥åˆ° DB1ï¼ˆCleaner çš„è¾“å‡º DBï¼‰"""
        try:
            self.redis_client = redis.Redis(
                host=self.config["redis"]["host"],
                port=self.config["redis"]["port"],
                db=self.config["redis"]["input_db"],  # è¿æ¥åˆ° DB1
                decode_responses=True,
                socket_connect_timeout=5
            )
            self.redis_client.ping()
            print("âœ… Redis è¿æ¥æˆåŠŸï¼ˆDB1 - Cleaner è¾“å‡ºï¼‰")
        except Exception as e:
            print(f"âš ï¸  Redis è¿æ¥å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨æœ¬åœ°æ–‡ä»¶æ¨¡å¼")
            self.redis_client = None

    def load_data_from_redis(self) -> pd.DataFrame:
        """
        ä» Redis é˜Ÿåˆ—è¯»å–æ¸…æ´—åçš„æ•°æ®
        
        Returns:
            pd.DataFrame: æ¸…æ´—åçš„æ•°æ®
        """
        if not self.redis_client:
            print("âŒ Redis æœªè¿æ¥ï¼Œæ— æ³•ä»é˜Ÿåˆ—è¯»å–æ•°æ®")
            return pd.DataFrame()

        queue_name = self.config["redis"]["input_queue"]
        data_list = []
        
        try:
            # ç»Ÿè®¡åˆå§‹é˜Ÿåˆ—é•¿åº¦
            initial_queue_len = self.redis_client.llen(queue_name)
            print(f"ğŸ“Š Redis é˜Ÿåˆ— '{queue_name}' ä¸­æœ‰ {initial_queue_len} æ¡æ•°æ®")
            
            # æ‰¹é‡è¯»å–é˜Ÿåˆ—ä¸­çš„æ‰€æœ‰æ•°æ®
            # æ³¨æ„ï¼šè¿™ä¼šæ¸…ç©ºé˜Ÿåˆ—ï¼Œç¡®ä¿å·²å¤‡ä»½é‡è¦æ•°æ®
            timeout = 300  # 5åˆ†é’Ÿè¶…æ—¶
            start_time = datetime.now()
            
            while (datetime.now() - start_time).total_seconds() < timeout:
                # ä½¿ç”¨ lpop é€æ¡è¯»å–ï¼ˆFIFOï¼‰
                item_json = self.redis_client.lpop(queue_name)
                
                if not item_json:
                    break
                
                try:
                    item_data = json.loads(item_json)
                    data_list.append(item_data)
                except json.JSONDecodeError as e:
                    print(f"âš ï¸  JSON è§£æé”™è¯¯ï¼Œè·³è¿‡è¯¥æ•°æ®: {e}")
                    continue
            
            if data_list:
                print(f"âœ… æˆåŠŸä» Redis é˜Ÿåˆ—è¯»å– {len(data_list)} æ¡æ•°æ®")
                df = pd.DataFrame(data_list)
                return df
            else:
                print(f"âš ï¸  è­¦å‘Šï¼šRedis é˜Ÿåˆ— '{queue_name}' ä¸ºç©º")
                return pd.DataFrame()

        except Exception as e:
            print(f"âŒ ä» Redis è¯»å–æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()

    def load_data_from_file(self, file_path: str) -> pd.DataFrame:
        """
        ä»æœ¬åœ° CSV æ–‡ä»¶è¯»å–æ•°æ®ï¼ˆå¤‡ä»½æ–¹æ¡ˆï¼‰
        
        Args:
            file_path: CSV æ–‡ä»¶è·¯å¾„
            
        Returns:
            pd.DataFrame: æ•°æ®
        """
        try:
            print(f"ğŸ“‚ ä»æœ¬åœ°æ–‡ä»¶åŠ è½½æ•°æ®: {file_path}")
            df = pd.read_csv(file_path)
            print(f"âœ… æˆåŠŸåŠ è½½æœ¬åœ°æ–‡ä»¶ï¼Œå…± {len(df)} æ¡æ•°æ®")
            return df
        except FileNotFoundError:
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return pd.DataFrame()
        except Exception as e:
            print(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥: {e}")
            return pd.DataFrame()

    def load_data(self, input_file: str = None) -> pd.DataFrame:
        """
        åŠ è½½æ•°æ®ï¼ˆä¼˜å…ˆ Redisï¼Œå›é€€æœ¬åœ°æ–‡ä»¶ï¼‰
        
        Args:
            input_file: æœ¬åœ° CSV æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            pd.DataFrame: åŠ è½½çš„æ•°æ®
        """
        # ç­–ç•¥ 1: ä¼˜å…ˆå°è¯•ä» Redis è¯»å–
        print("\nğŸ”„ å°è¯•ä» Redis é˜Ÿåˆ—è¯»å–æ•°æ®...")
        df_redis = self.load_data_from_redis()
        
        if not df_redis.empty:
            print("âœ¨ ä½¿ç”¨ Redis å®æ—¶æ•°æ®å¤„ç†æ¨¡å¼")
            return df_redis
        
        # ç­–ç•¥ 2: å›é€€åˆ°æœ¬åœ° CSV æ–‡ä»¶
        csv_path = input_file or self.config.get("input_file", "input_data.csv")
        print(f"\nğŸ”„ Redis é˜Ÿåˆ—ä¸ºç©ºï¼Œå°è¯•æœ¬åœ°æ–‡ä»¶...")
        df_file = self.load_data_from_file(csv_path)
        
        if not df_file.empty:
            print("âœ¨ ä½¿ç”¨æœ¬åœ°æ–‡ä»¶å¤„ç†æ¨¡å¼")
            return df_file
        
        # éƒ½å¤±è´¥åˆ™è¿”å›ç©º
        print("âŒ æ— æ³•åŠ è½½ä»»ä½•æ•°æ®")
        return pd.DataFrame()

    def preprocess_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ•°æ®é¢„å¤„ç†ï¼ˆä¿æŒåŸé€»è¾‘ï¼‰
        
        Args:
            df: è¾“å…¥æ•°æ®æ¡†
            
        Returns:
            pd.DataFrame: é¢„å¤„ç†åçš„æ•°æ®
        """
        if df.empty:
            return df

        # è½¬æ¢æ—¶é—´æ ¼å¼
        if 'created_at' in df.columns:
            df['created_at'] = pd.to_datetime(df['created_at'], errors='coerce')
        
        if 'timestamp' in df.columns:
            df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')

        # è½¬æ¢æƒ…æ„Ÿæ ‡ç­¾ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'sentiment' in df.columns:
            sentiment_mapping = {
                'æ­£é¢': 'positive',
                'ä¸­æ€§': 'neutral',
                'è´Ÿé¢': 'negative',
                'positive': 'positive',
                'neutral': 'neutral',
                'negative': 'negative',
            }
            df['sentiment'] = df['sentiment'].map(sentiment_mapping)
            df['sentiment'] = df['sentiment'].fillna('neutral')

        # æ¸…ç†æ–‡æœ¬æ•°æ®
        if 'text' in df.columns:
            df['clean_text'] = df['text'].fillna('').apply(self._clean_text)
        elif 'content' in df.columns:
            df['clean_text'] = df['content'].fillna('').apply(self._clean_text)
        else:
            df['clean_text'] = ''

        return df

    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬"""
        if not isinstance(text, str):
            return ""

        import re
        
        # ç§»é™¤URL
        text = re.sub(r'http\S+', '', text)

        # ç§»é™¤è‚¡ç¥¨ä»£ç ï¼ˆå¦‚$ETH.Xï¼‰
        text = re.sub(r'\$\w+\.\w+', '', text)

        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™å­—æ¯æ•°å­—å’Œç©ºæ ¼
        text = re.sub(r'[^\w\s]', ' ', text)

        # è½¬æ¢ä¸ºå°å†™
        text = text.lower()

        # ç§»é™¤å¤šä½™ç©ºæ ¼
        text = ' '.join(text.split())

        return text

    def get_time_windows(self, df: pd.DataFrame) -> Dict[str, datetime]:
        """è·å–æ—¶é—´çª—å£"""
        if df.empty:
            # è¿”å›é»˜è®¤æ—¶é—´çª—å£
            now = datetime.now()
            return {
                'latest_time': now,
                'current_window_start': now - timedelta(minutes=30),
                'history_window_start': now - timedelta(hours=24)
            }

        # ä»æ•°æ®ä¸­è·å–æ—¶é—´
        time_column = 'created_at' if 'created_at' in df.columns else 'timestamp'
        
        if time_column not in df.columns:
            now = datetime.now()
            return {
                'latest_time': now,
                'current_window_start': now - timedelta(minutes=30),
                'history_window_start': now - timedelta(hours=24)
            }

        latest_time = pd.to_datetime(df[time_column]).max()
        current_window_start = latest_time - timedelta(minutes=30)
        history_window_start = latest_time - timedelta(hours=24)

        return {
            'latest_time': latest_time,
            'current_window_start': current_window_start,
            'history_window_start': history_window_start
        }
```

---

### ç¬¬ä¸‰æ­¥ï¼šä¿®æ”¹ Redis ç®¡ç†å™¨

**æ–‡ä»¶è·¯å¾„ï¼š** `processer/Analysis/redis_manager.py`

**å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨ String ç»“æ„è€Œä¸æ˜¯ Hash ç»“æ„**

```python
import json
import redis
import os
from datetime import datetime
from config import CONFIG


class RedisManager:
    def __init__(self):
        # Redisè¿æ¥é…ç½® - è¿æ¥åˆ° DB0ï¼ˆè¾“å‡ºæ•°æ®åº“ï¼‰
        self.redis_host = CONFIG["redis"]["host"]
        self.redis_port = CONFIG["redis"]["port"]
        self.redis_db = CONFIG["redis"]["output_db"]  # ä½¿ç”¨ output_db
        self.redis_password = CONFIG["redis"]["password"]
        self.output_prefix = CONFIG["redis"]["output_prefix"]
        self.key_ttl = CONFIG["redis"]["key_ttl_seconds"]

        # è¿æ¥Redis
        self.r = self._connect_redis()

    def _connect_redis(self):
        """è¿æ¥ Redis æ•°æ®åº“"""
        try:
            r = redis.Redis(
                host=self.redis_host,
                port=self.redis_port,
                db=self.redis_db,
                password=self.redis_password,
                decode_responses=True
            )
            r.ping()
            print(f"âœ… Redis è¿æ¥æˆåŠŸ (DB{self.redis_db})")
            return r
        except redis.ConnectionError as e:
            print(f"âŒ Redis è¿æ¥å¤±è´¥: {e}")
            return None

    def publish_processed_data(self, output_file_path=None):
        """
        å°†å¤„ç†åçš„æ•°æ®å‘å¸ƒåˆ° Redis
        
        ä½¿ç”¨ String ç»“æ„ï¼ˆè€Œé Hashï¼‰ç¡®ä¿ä¸ Visualization å…¼å®¹
        """
        if not self.r:
            print("âŒ Redis æœªè¿æ¥ï¼Œæ— æ³•å‘å¸ƒæ•°æ®")
            return False

        try:
            # è¯»å–å¤„ç†åçš„æ•°æ®
            if output_file_path is None:
                output_file_path = "output_data.json"

            with open(output_file_path, 'r', encoding='utf-8') as f:
                processed_data = json.load(f)

            # æ·»åŠ å‘å¸ƒæ—¶é—´æˆ³
            processed_data['metadata']['redis_publish_time'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

            # ==================== ä½¿ç”¨ String ç»“æ„å­˜å‚¨ ====================
            # è¿™æ · Visualization å¯ä»¥é€šè¿‡ redis.get("processed_data:metadata") è¯»å–
            
            print("\nğŸ“¤ å‘å¸ƒå¤„ç†åçš„æ•°æ®åˆ° Redis...")
            
            # 1. å‘å¸ƒå…ƒæ•°æ®
            metadata_key = f"{self.output_prefix}:metadata"
            self.r.set(
                metadata_key,
                json.dumps(processed_data['metadata'], ensure_ascii=False)
            )
            self.r.expire(metadata_key, self.key_ttl)
            print(f"  âœ“ {metadata_key}")

            # 2. å‘å¸ƒçƒ­è¯æ•°æ®
            keywords_key = f"{self.output_prefix}:trending_keywords"
            self.r.set(
                keywords_key,
                json.dumps(processed_data['trending_keywords'], ensure_ascii=False)
            )
            self.r.expire(keywords_key, self.key_ttl)
            print(f"  âœ“ {keywords_key}")

            # 3. å‘å¸ƒè¯äº‘æ•°æ®
            wordcloud_key = f"{self.output_prefix}:word_cloud"
            self.r.set(
                wordcloud_key,
                json.dumps(processed_data['word_cloud'], ensure_ascii=False)
            )
            self.r.expire(wordcloud_key, self.key_ttl)
            print(f"  âœ“ {wordcloud_key}")

            # 4. å‘å¸ƒæ–°é—»æ•°æ®
            news_key = f"{self.output_prefix}:news_feed"
            self.r.set(
                news_key,
                json.dumps(processed_data['news_feed'], ensure_ascii=False)
            )
            self.r.expire(news_key, self.key_ttl)
            print(f"  âœ“ {news_key}")

            # 5. å‘å¸ƒå†å²æ•°æ®
            history_data = processed_data.get('history_data', {})
            for keyword, data in history_data.items():
                history_key = f"{self.output_prefix}:history_data:{keyword}"
                self.r.set(
                    history_key,
                    json.dumps(data, ensure_ascii=False)
                )
                self.r.expire(history_key, self.key_ttl)
            
            print(f"  âœ“ {len(history_data)} æ¡å†å²æ•°æ®")

            # å¯é€‰ï¼šå‘å¸ƒé€šçŸ¥æ¶ˆæ¯
            if "publish_channel" in CONFIG["redis"]:
                channel = CONFIG["redis"]["publish_channel"]
                self.r.publish(channel, json.dumps({
                    "event": "data_updated",
                    "timestamp": datetime.now().isoformat(),
                    "keywords_count": len(processed_data['trending_keywords']),
                    "history_count": len(history_data)
                }))
                print(f"  âœ“ å‘å¸ƒæ›´æ–°é€šçŸ¥åˆ° {channel}")

            print("âœ… æ‰€æœ‰æ•°æ®å·²æˆåŠŸå‘å¸ƒåˆ° Redis (DB0)")
            return True

        except FileNotFoundError:
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {output_file_path}")
            return False
        except Exception as e:
            print(f"âŒ å‘å¸ƒæ•°æ®æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return False

    def get_processed_data(self) -> dict:
        """ä» Redis è¯»å–å¤„ç†åçš„æ•°æ®ï¼ˆéªŒè¯ï¼‰"""
        if not self.r:
            print("âŒ Redis æœªè¿æ¥")
            return None

        try:
            metadata_key = f"{self.output_prefix}:metadata"
            metadata_json = self.r.get(metadata_key)
            
            if not metadata_json:
                print(f"âš ï¸  Redis ä¸­æœªæ‰¾åˆ°æ•°æ®ï¼ˆé”®ï¼š{metadata_key}ï¼‰")
                return None

            keywords_json = self.r.get(f"{self.output_prefix}:trending_keywords")
            wordcloud_json = self.r.get(f"{self.output_prefix}:word_cloud")
            news_json = self.r.get(f"{self.output_prefix}:news_feed")

            result = {
                "metadata": json.loads(metadata_json) if metadata_json else {},
                "trending_keywords": json.loads(keywords_json) if keywords_json else [],
                "word_cloud": json.loads(wordcloud_json) if wordcloud_json else [],
                "news_feed": json.loads(news_json) if news_json else []
            }

            print(f"âœ… æˆåŠŸè¯»å–å¤„ç†æ•°æ®ï¼š{len(result['trending_keywords'])} ä¸ªçƒ­è¯")
            return result

        except Exception as e:
            print(f"âŒ è¯»å–å¤„ç†æ•°æ®å¤±è´¥: {e}")
            return None

    def verify_redis_connection(self) -> bool:
        """éªŒè¯ Redis è¿æ¥"""
        if self.r:
            try:
                self.r.ping()
                info = self.r.info()
                print(f"âœ… Redis å·²è¿æ¥ (DB{self.redis_db})")
                print(f"   Redis ç‰ˆæœ¬: {info.get('redis_version')}")
                print(f"   å·²è¿æ¥å®¢æˆ·ç«¯: {info.get('connected_clients')}")
                print(f"   å·²ç”¨å†…å­˜: {info.get('used_memory_human')}")
                return True
            except Exception as e:
                print(f"âŒ Redis è¿æ¥å·²æ–­å¼€: {e}")
                return False
        return False

    def check_output_keys(self) -> dict:
        """æ£€æŸ¥è¾“å‡ºé”®æ˜¯å¦å­˜åœ¨"""
        keys_info = {}
        for suffix in ['metadata', 'trending_keywords', 'word_cloud', 'news_feed']:
            key = f"{self.output_prefix}:{suffix}"
            exists = self.r.exists(key)
            if exists:
                size = len(self.r.get(key) or "")
                keys_info[key] = f"âœ“ å­˜åœ¨ ({size} bytes)"
            else:
                keys_info[key] = "âœ— ä¸å­˜åœ¨"
        
        return keys_info
```

---

### ç¬¬å››æ­¥ï¼šä¿®æ”¹ä¸»å¤„ç†ç¨‹åº

**æ–‡ä»¶è·¯å¾„ï¼š** `processer/Analysis/main.py`

**å…³é”®ä¿®æ”¹ï¼šæ·»åŠ  Redis è¿æ¥æ£€æŸ¥å’Œæ”¹è¿›çš„æ—¥å¿—**

```python
import json
from datetime import datetime
from data_loader import DataLoader
from text_analyzer import TextAnalyzer
from sentiment_analyzer import SentimentAnalyzer
from history_analyzer import HistoryAnalyzer
from news_processor import NewsProcessor
from redis_manager import RedisManager  # æ–°å¢å¯¼å…¥
from config import CONFIG
import pandas as pd


class MainProcessor:
    def __init__(self):
        self.data_loader = DataLoader()
        self.text_analyzer = TextAnalyzer()
        self.sentiment_analyzer = SentimentAnalyzer()
        self.history_analyzer = HistoryAnalyzer()
        self.news_processor = NewsProcessor()
        self.redis_manager = RedisManager()  # æ–°å¢
        self.config = CONFIG

    def process(self, input_file: str = None, output_file: str = None):
        """
        ä¸»å¤„ç†æµç¨‹
        
        Args:
            input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œä¼˜å…ˆä½¿ç”¨ Redisï¼‰
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        print("\n" + "="*60)
        print("ğŸš€ Processer å¤„ç†å¼€å§‹")
        print("="*60)

        # ä½¿ç”¨é»˜è®¤é…ç½®
        if input_file is None:
            input_file = self.config.get("input_file", "input_data.csv")
        if output_file is None:
            output_file = self.config.get("output_file", "output_data.json")

        # âœ… éªŒè¯ Redis è¿æ¥
        if not self.redis_manager.verify_redis_connection():
            print("âš ï¸  è­¦å‘Šï¼šRedis è¿æ¥å¤±è´¥ï¼Œç³»ç»Ÿå°†ä½¿ç”¨æœ¬åœ°æ–‡ä»¶æ¨¡å¼")
            print("         å¤„ç†åçš„æ•°æ®å°†æ— æ³•æ¨é€åˆ° Redis")

        # 1. åŠ è½½æ•°æ®
        print("\nğŸ“¥ åŠ è½½æ•°æ®...")
        raw_data = self.data_loader.load_data(input_file)
        
        if raw_data.empty:
            print("âŒ åŠ è½½æ•°æ®å¤±è´¥ï¼Œé€€å‡ºå¤„ç†")
            return False

        df = self.data_loader.preprocess_data(raw_data)
        time_windows = self.data_loader.get_time_windows(df)

        print(f"âœ“ åŠ è½½äº† {len(df)} æ¡æ•°æ®")

        # 2. è·å–æ—¶é—´çª—å£æ•°æ®
        current_df = df[df['created_at'] >= time_windows['current_window_start']]
        history_df = df[
            (df['created_at'] >= time_windows['history_window_start']) &
            (df['created_at'] < time_windows['current_window_start'])
        ]

        print(f"âœ“ å½“å‰çª—å£æ•°æ®: {len(current_df)} æ¡")
        print(f"âœ“ å†å²çª—å£æ•°æ®: {len(history_df)} æ¡")

        # 3. è¯é¢‘åˆ†æ
        print("\nğŸ” æ‰§è¡Œæ–‡æœ¬åˆ†æ...")
        current_keywords = self.text_analyzer.extract_keywords(current_df['clean_text'].tolist())

        # è®¡ç®—å†å²24å°æ—¶å¹³å‡é¢‘ç‡
        history_keywords_freq = {}
        for keyword, _ in current_keywords[:self.config['trending_keywords_count']]:
            keyword_history_df = history_df[history_df['clean_text'].str.contains(keyword, case=False, na=False)]
            total_intervals = 48
            history_avg_freq = len(keyword_history_df) / total_intervals
            history_keywords_freq[keyword] = history_avg_freq

        # 4. ç”Ÿæˆçƒ­è¯æ’è¡Œæ¦œ
        print("ğŸ“Š ç”Ÿæˆçƒ­è¯æ’è¡Œæ¦œ...")
        trending_keywords = self._generate_trending_keywords(
            current_keywords, history_keywords_freq, df
        )

        # 5. ç”Ÿæˆè¯äº‘æ•°æ®
        print("â˜ï¸  ç”Ÿæˆè¯äº‘æ•°æ®...")
        word_cloud = self._generate_word_cloud_data(current_keywords)

        # 6. ç”Ÿæˆå†å²æ•°æ®
        print("ğŸ“ˆ ç”Ÿæˆå†å²æ•°æ®...")
        top_keywords = [keyword for keyword, _ in current_keywords[:self.config['trending_keywords_count']]]
        history_data = self.history_analyzer.generate_history_data(df, top_keywords)

        # 7. ç”Ÿæˆæ–°é—»æµ
        print("ğŸ“° ç”Ÿæˆæ–°é—»æµ...")
        news_feed = self.news_processor.generate_news_feed(df, top_keywords)

        # 8. ç”Ÿæˆè¾“å‡ºæ•°æ®
        print("\nğŸ’¾ ç”Ÿæˆè¾“å‡ºæ•°æ®...")
        output_data = self._generate_output_data(
            trending_keywords, word_cloud, history_data, news_feed
        )

        # 9. ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶
        print(f"ğŸ’¾ ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶: {output_file}")
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        print(f"âœ… æœ¬åœ°æ–‡ä»¶ä¿å­˜å®Œæˆ")

        # 10. å‘å¸ƒåˆ° Redis
        print("\nğŸ“¤ å‘å¸ƒåˆ° Redis...")
        if self.redis_manager.publish_processed_data(output_file):
            print("âœ… æ•°æ®å·²æˆåŠŸå‘å¸ƒåˆ° Redis")
        else:
            print("âš ï¸  æ•°æ®å‘å¸ƒåˆ° Redis å¤±è´¥")

        print("\n" + "="*60)
        print("âœ¨ Processer å¤„ç†å®Œæˆï¼")
        print("="*60)
        return True

    def _generate_trending_keywords(self, current_keywords: list, history_keywords_freq: dict,
                                    df: pd.DataFrame) -> list:
        """ç”Ÿæˆçƒ­è¯æ’è¡Œæ¦œï¼ˆä¿æŒåŸé€»è¾‘ï¼‰"""
        trending_data = []
        max_frequency = max([freq for _, freq in current_keywords]) if current_keywords else 1

        for rank, (keyword, current_freq) in enumerate(current_keywords[:self.config['trending_keywords_count']], 1):
            history_avg_freq = history_keywords_freq.get(keyword, 0)
            growth_rate = self.text_analyzer.calculate_growth_rate(current_freq, history_avg_freq)
            trend_score = self.text_analyzer.calculate_trend_score(current_freq, growth_rate, max_frequency)
            sentiment_data = self.sentiment_analyzer.analyze_sentiment_distribution(df, keyword)

            trending_data.append({
                "keyword": keyword,
                "rank": rank,
                "current_frequency": current_freq,
                "growth_rate": round(growth_rate, 1),
                "trend_score": trend_score,
                "sentiment": sentiment_data
            })

        return trending_data

    def _generate_word_cloud_data(self, keywords: list) -> list:
        """ç”Ÿæˆè¯äº‘æ•°æ®ï¼ˆä¿æŒåŸé€»è¾‘ï¼‰"""
        return [
            {"text": keyword, "value": freq}
            for keyword, freq in keywords[:self.config['word_cloud_count']]
        ]

    def _generate_output_data(self, trending_keywords: list, word_cloud: list,
                              history_data: dict, news_feed: list) -> dict:
        """ç”Ÿæˆæœ€ç»ˆè¾“å‡ºæ•°æ®ï¼ˆä¿æŒåŸé€»è¾‘ï¼‰"""
        return {
            "metadata": {
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "update_interval": self.config['history_interval_minutes'],
                "data_version": "1.0"
            },
            "trending_keywords": trending_keywords,
            "word_cloud": word_cloud,
            "history_data": history_data,
            "news_feed": news_feed
        }


if __name__ == "__main__":
    processor = MainProcessor()
    processor.process(
        input_file=CONFIG.get("input_file"),
        output_file=CONFIG.get("output_file")
    )
```

---

## éªŒè¯æ¸…å•

ä¿®æ”¹åï¼Œè¯·æŒ‰ä»¥ä¸‹æ­¥éª¤éªŒè¯ï¼š

```bash
# 1. éªŒè¯é…ç½®æ–‡ä»¶
cd processer/Analysis
python -c "from config import CONFIG; print(CONFIG['redis'])"

# 2. æµ‹è¯•æ•°æ®åŠ è½½
python -c "from data_loader import DataLoader; dl = DataLoader(); df = dl.load_data(); print(f'åŠ è½½äº† {len(df)} æ¡æ•°æ®')"

# 3. è¿è¡Œå®Œæ•´å¤„ç†
python main.py

# 4. éªŒè¯ Redis è¾“å‡º
redis-cli
> SELECT 0
> KEYS processed_data:*
> GET processed_data:metadata
```

---

## å¸¸è§é—®é¢˜æ’æŸ¥

| é—®é¢˜ | ç—‡çŠ¶ | è§£å†³æ–¹æ¡ˆ |
|------|------|---------|
| Redis è¿æ¥å¤±è´¥ | "Redis è¿æ¥å¤±è´¥" é”™è¯¯ | æ£€æŸ¥ Redis æ˜¯å¦è¿è¡Œï¼š`redis-cli ping` |
| é˜Ÿåˆ—ä¸ºç©º | "Redis é˜Ÿåˆ—ä¸ºç©º" è­¦å‘Š | æ£€æŸ¥ Cleaner æ˜¯å¦æ­£ç¡®è¾“å‡ºæ•°æ® |
| æ•°æ®ç»“æ„ä¸åŒ¹é… | Visualization è¯»å–ä¸åˆ°æ•°æ® | ç¡®è®¤ä½¿ç”¨äº† `redis.set()` è€Œé `redis.hset()` |
| æ–‡ä»¶æ¨¡å¼å›é€€ | å§‹ç»ˆä½¿ç”¨æœ¬åœ° CSV | æ£€æŸ¥ Redis å’Œ Cleaner æ¨¡å—çŠ¶æ€ |

