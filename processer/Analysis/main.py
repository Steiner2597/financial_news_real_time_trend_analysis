import json
from datetime import datetime
from data_loader import DataLoader
from text_analyzer import TextAnalyzer
from sentiment_analyzer import SentimentAnalyzer
from history_analyzer import HistoryAnalyzer
from redis_manager import RedisManager  # æ–°å¢å¯¼å…¥
from config import CONFIG
import pandas as pd


class MainProcessor:
    def __init__(self):
        self.data_loader = DataLoader()
        self.text_analyzer = TextAnalyzer()
        self.sentiment_analyzer = SentimentAnalyzer()
        self.history_analyzer = HistoryAnalyzer()
        self.redis_manager = RedisManager()  # æ–°å¢
        self.config = CONFIG

    def process(self, input_file: str = None, output_file: str = None):
        """
        ä¸»å¤„ç†æµç¨‹
        
        Args:
            input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œä¼˜å…ˆä½¿ç”¨ Redisï¼‰
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        print("\n" + "="*60)
        print("ğŸš€ Processer å¤„ç†å¼€å§‹")
        print("="*60)

        # ä½¿ç”¨é»˜è®¤é…ç½®
        if input_file is None:
            input_file = self.config.get("input_file", "input_data.csv")
        if output_file is None:
            output_file = self.config.get("output_file", "output_data.json")

        # âœ… éªŒè¯ Redis è¿æ¥
        print("\nğŸ”Œ æ£€æŸ¥ Redis è¿æ¥...")
        if not self.redis_manager.verify_redis_connection():
            print("âš ï¸  è­¦å‘Šï¼šRedis è¿æ¥å¤±è´¥ï¼Œç³»ç»Ÿå°†ä½¿ç”¨æœ¬åœ°æ–‡ä»¶æ¨¡å¼")
            print("         å¤„ç†åçš„æ•°æ®å°†æ— æ³•æ¨é€åˆ° Redis")

        # 1. åŠ è½½æ•°æ®
        print("\nğŸ“¥ åŠ è½½æ•°æ®...")
        raw_data = self.data_loader.load_data(input_file)
        
        if raw_data.empty:
            print("âŒ åŠ è½½æ•°æ®å¤±è´¥ï¼Œé€€å‡ºå¤„ç†")
            return False

        df = self.data_loader.preprocess_data(raw_data)
        time_windows = self.data_loader.get_time_windows(df)

        print(f"âœ“ åŠ è½½äº† {len(df)} æ¡æ•°æ®")

        # 2. è·å–æ—¶é—´çª—å£æ•°æ®
        current_df = df[df['timestamp'] >= time_windows['current_window_start']]
        history_df = df[
            (df['timestamp'] >= time_windows['history_window_start']) &
            (df['timestamp'] < time_windows['current_window_start'])
        ]

        print(f"âœ“ å½“å‰çª—å£æ•°æ®: {len(current_df)} æ¡")
        print(f"âœ“ å†å²çª—å£æ•°æ®: {len(history_df)} æ¡")

        # 3. è¯é¢‘åˆ†æ
        print("\nğŸ” æ‰§è¡Œæ–‡æœ¬åˆ†æ...")
        current_keywords = self.text_analyzer.extract_keywords(current_df['clean_text'].tolist())

        # è®¡ç®—å†å²24å°æ—¶å¹³å‡é¢‘ç‡
        history_keywords_freq = {}
        for keyword, _ in current_keywords[:self.config['trending_keywords_count']]:
            keyword_history_df = history_df[history_df['clean_text'].str.contains(keyword, case=False, na=False)]
            total_intervals = 48
            history_avg_freq = len(keyword_history_df) / total_intervals
            history_keywords_freq[keyword] = history_avg_freq

        # 4. ç”Ÿæˆçƒ­è¯æ’è¡Œæ¦œ
        print("ğŸ“Š ç”Ÿæˆçƒ­è¯æ’è¡Œæ¦œ...")
        trending_keywords = self._generate_trending_keywords(
            current_keywords, history_keywords_freq, df
        )

        # 5. ç”Ÿæˆè¯äº‘æ•°æ®
        print("â˜ï¸  ç”Ÿæˆè¯äº‘æ•°æ®...")
        word_cloud = self._generate_word_cloud_data(current_keywords)

        # 6. ç”Ÿæˆå†å²æ•°æ®
        print("ğŸ“ˆ ç”Ÿæˆå†å²æ•°æ®...")
        top_keywords = [keyword for keyword, _ in current_keywords[:self.config['trending_keywords_count']]]
        history_data = self.history_analyzer.generate_history_data(df, top_keywords)

        # 7. ç”Ÿæˆæ–°é—»æµ
        print("ğŸ“° ç”Ÿæˆæ–°é—»æµ...")
        news_feed = self.news_processor.generate_news_feed(df, top_keywords)

        # 8. ç»Ÿè®¡æ–°é—»æ¥æº
        print("ğŸ“Š ç»Ÿè®¡æ–°é—»æ¥æºåˆ†å¸ƒ...")
        news_sources = self._calculate_news_sources(df)

        # 9. ç”Ÿæˆè¾“å‡ºæ•°æ®
        print("\nğŸ’¾ ç”Ÿæˆè¾“å‡ºæ•°æ®...")
        output_data = self._generate_output_data(
            trending_keywords, word_cloud, history_data, news_feed, news_sources
        )

        # 10. ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶
        print(f"ğŸ’¾ ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶: {output_file}")
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        print(f"âœ… æœ¬åœ°æ–‡ä»¶ä¿å­˜å®Œæˆ")

        # 11. å‘å¸ƒåˆ° Redis
        print("\nğŸ“¤ å‘å¸ƒåˆ° Redis...")
        if self.redis_manager.publish_processed_data(output_file):
            print("âœ… æ•°æ®å·²æˆåŠŸå‘å¸ƒåˆ° Redis")
            
            # éªŒè¯ Redis ä¸­çš„æ•°æ®
            print("\nğŸ” éªŒè¯ Redis è¾“å‡ºé”®...")
            keys_info = self.redis_manager.check_output_keys()
            for key, status in keys_info.items():
                print(f"  {status} {key}")
        else:
            print("âš ï¸  æ•°æ®å‘å¸ƒåˆ° Redis å¤±è´¥")

        print("\n" + "="*60)
        print("âœ¨ Processer å¤„ç†å®Œæˆï¼")
        print("="*60)
        return True

    def _generate_trending_keywords(self, current_keywords: list, history_keywords_freq: dict,
                                    df: pd.DataFrame) -> list:
        """ç”Ÿæˆçƒ­è¯æ’è¡Œæ¦œ"""
        trending_data = []
        max_frequency = max([freq for _, freq in current_keywords]) if current_keywords else 1

        for rank, (keyword, current_freq) in enumerate(current_keywords[:self.config['trending_keywords_count']], 1):
            history_avg_freq = history_keywords_freq.get(keyword, 0)

            growth_rate = self.text_analyzer.calculate_growth_rate(current_freq, history_avg_freq)
            trend_score = self.text_analyzer.calculate_trend_score(current_freq, growth_rate, max_frequency)

            sentiment_data = self.sentiment_analyzer.analyze_sentiment_distribution(df, keyword)

            trending_data.append({
                "keyword": keyword,
                "rank": rank,
                "current_frequency": current_freq,
                "growth_rate": round(growth_rate, 1),
                "trend_score": trend_score,
                "sentiment": sentiment_data
            })

        return trending_data

    def _generate_word_cloud_data(self, keywords: list) -> list:
        """ç”Ÿæˆè¯äº‘æ•°æ®"""
        return [
            {"text": keyword, "value": freq}
            for keyword, freq in keywords[:self.config['word_cloud_count']]
        ]

    def _calculate_news_sources(self, df: pd.DataFrame) -> dict:
        """
        ç»Ÿè®¡æ–°é—»æ¥æºåˆ†å¸ƒ
        
        Args:
            df: æ•°æ®æ¡†
            
        Returns:
            dict: æ–°é—»æ¥æºç»Ÿè®¡æ•°æ®ï¼Œæ ¼å¼ä¸º {"source_name": count, ...}
        """
        if 'source' not in df.columns:
            print("âš ï¸  è­¦å‘Šï¼šæ•°æ®ä¸­æ²¡æœ‰ 'source' å­—æ®µ")
            return {}
        
        # ç»Ÿè®¡æ¯ä¸ªæ¥æºçš„æ•°é‡
        source_counts = df['source'].value_counts().to_dict()
        
        # å¤„ç†ç©ºå€¼æˆ–æœªçŸ¥æ¥æº
        if pd.isna(list(source_counts.keys())[0]) if source_counts else False:
            source_counts['Unknown'] = source_counts.pop(list(source_counts.keys())[0])
        
        # æŒ‰æ•°é‡æ’åº
        source_counts = dict(sorted(source_counts.items(), key=lambda x: x[1], reverse=True))
        
        print(f"âœ“ ç»Ÿè®¡åˆ° {len(source_counts)} ä¸ªæ–°é—»æ¥æº")
        for source, count in list(source_counts.items())[:5]:
            print(f"  - {source}: {count} æ¡")
        
        return source_counts

    def _generate_output_data(self, trending_keywords: list, word_cloud: list,
                              history_data: dict, news_feed: list, news_sources: dict) -> dict:
        """ç”Ÿæˆæœ€ç»ˆè¾“å‡ºæ•°æ®"""
        return {
            "metadata": {
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "update_interval": self.config['history_interval_minutes'],
                "data_version": "1.0",
                "news_sources": news_sources  # æ·»åŠ æ–°é—»æ¥æºç»Ÿè®¡
            },
            "trending_keywords": trending_keywords,
            "word_cloud": word_cloud,
            "history_data": history_data,
            "news_feed": news_feed
        }


if __name__ == "__main__":
    processor = MainProcessor()
    processor.process(
        input_file=CONFIG.get("input_file"),
        output_file=CONFIG.get("output_file")
    )