# 数据保留策略说明

## 数据流

```
Scraper → data_queue (DB0) → Cleaner → clean_data_queue (DB1) → Processor
          保留 24h              保留 24h                              处理并输出到 DB2
```

## 非消费模式

### 改进点
1. **Cleaner** 使用 `LRANGE` 读取数据，不删除原始数据
2. **Processor** 使用 `LRANGE` 读取数据，不删除清洗后的数据
3. 数据保留在 Redis 中，用于趋势分析

### 优点
- ✅ 保留 24 小时历史数据
- ✅ 支持时间窗口分析（最近 30 分钟 vs 过去 24 小时）
- ✅ 数据不会因为处理而丢失

### 注意事项
- ⚠️ Redis 内存占用会增加
- ⚠️ 需要定期清理旧数据

## 数据清理

### 手动清理
```cmd
python clean_old_data.py
```

或

```cmd
clean_redis_data.bat
```

### 自动清理（Windows 任务计划程序）

1. 打开"任务计划程序"
2. 创建基本任务
3. 名称：`清理 Redis 旧数据`
4. 触发器：每天凌晨 3:00
5. 操作：启动程序
   - 程序：`d:\SE\workspace\financial_real_time_trend_analysis\clean_redis_data.bat`
6. 完成

### 自动清理（Linux crontab）

```bash
# 每天凌晨 3:00 执行清理
0 3 * * * cd /path/to/project && python clean_old_data.py >> logs/clean_data.log 2>&1
```

## 配置说明

### 保留时间
默认保留 **24 小时** 的数据，可以修改 `clean_old_data.py` 中的 `hours` 参数：

```python
cleaner.clean_old_data(
    db=0,
    queue_name='data_queue',
    hours=48  # 改为 48 小时
)
```

### 清理频率建议
- 数据量小（< 1 万条/天）：每周清理一次
- 数据量中（1-10 万条/天）：每天清理一次
- 数据量大（> 10 万条/天）：每 6 小时清理一次

## 监控

### 检查数据量
```bash
# 原始数据
redis-cli -n 0 llen data_queue

# 清洗后数据
redis-cli -n 1 llen clean_data_queue
```

### 检查 Redis 内存
```bash
redis-cli info memory
```

## 故障排除

### Q: 数据量太大，Redis 内存不足
**A**: 减少保留时间或增加清理频率

### Q: 处理速度变慢
**A**: 数据量大时，LRANGE 操作会变慢，考虑：
1. 增加 Redis 内存
2. 优化数据结构（使用 Sorted Set）
3. 分批读取数据

### Q: 想要永久保存数据
**A**: 使用 Processor 的文件导出功能，数据会保存到 `output_data.json`
