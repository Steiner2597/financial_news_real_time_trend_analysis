"""
æµ‹è¯•æ–°é—»å’Œè¯„è®ºçš„å»é‡é€»è¾‘
éªŒè¯åŒä¸€ç¯‡æ–°é—»ä¸‹çš„å¤šæ¡è¯„è®ºä¸ä¼šè¢«é”™è¯¯å»é‡
"""
import json
import redis
import time
from datetime import datetime


def test_news_with_comments():
    """æµ‹è¯•æ–°é—»å’Œå…¶è¯„è®ºèƒ½å¤Ÿæ­£ç¡®åŒºåˆ†"""
    
    print("=" * 70)
    print("æµ‹è¯•åœºæ™¯ï¼šä¸€ç¯‡æ–°é—» + å¤šæ¡è¯„è®º")
    print("=" * 70)
    
    # è¿æ¥ Redis
    r = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)
    queue_name = "financial_news_queue"
    
    # æ¸…ç©ºæµ‹è¯•é˜Ÿåˆ—
    r.delete(queue_name)
    
    # 1. åˆ›å»ºä¸€ç¯‡æ–°é—»
    news = {
        "id": "news_12345",
        "title": "Tesla Stock Surges on Earnings Beat",
        "text": "Tesla Inc. reported better-than-expected earnings...",
        "source": "reuters",
        "url": "https://reuters.com/article/tesla-earnings",
        "created_at": datetime.now().isoformat(),
        "timestamp": int(time.time())
    }
    
    # 2. åˆ›å»ºè¯¥æ–°é—»çš„ 3 æ¡è¯„è®º
    comments = [
        {
            "post_id": "news_12345",  # çˆ¶æ–°é—» ID
            "comment_id": "comment_001",
            "text": "Great news for TSLA investors!",
            "author": "investor_joe",
            "source": "reddit",
            "created_at": datetime.now().isoformat(),
            "timestamp": int(time.time())
        },
        {
            "post_id": "news_12345",  # åŒä¸€çˆ¶æ–°é—» ID
            "comment_id": "comment_002",
            "text": "Time to buy more shares",
            "author": "trader_mike",
            "source": "reddit",
            "created_at": datetime.now().isoformat(),
            "timestamp": int(time.time())
        },
        {
            "post_id": "news_12345",  # åŒä¸€çˆ¶æ–°é—» ID
            "comment_id": "comment_003",
            "text": "Bearish sentiment continues despite earnings",
            "author": "bear_analyst",
            "source": "reddit",
            "created_at": datetime.now().isoformat(),
            "timestamp": int(time.time())
        }
    ]
    
    # æ¨é€åˆ° Redis
    print("\nğŸ“¤ æ¨é€æµ‹è¯•æ•°æ®åˆ° Redis...")
    r.lpush(queue_name, json.dumps(news, ensure_ascii=False))
    for comment in comments:
        r.lpush(queue_name, json.dumps(comment, ensure_ascii=False))
    
    print(f"âœ“ å·²æ¨é€ 1 ç¯‡æ–°é—» + 3 æ¡è¯„è®º")
    print(f"âœ“ é˜Ÿåˆ—é•¿åº¦: {r.llen(queue_name)}")
    
    # æ˜¾ç¤ºé¢„æœŸç»“æœ
    print("\n" + "=" * 70)
    print("é¢„æœŸ ID åˆ†é…:")
    print("=" * 70)
    print(f"æ–°é—»: post_news_12345")
    print(f"è¯„è®º1: comment_comment_001")
    print(f"è¯„è®º2: comment_comment_002")
    print(f"è¯„è®º3: comment_comment_003")
    
    print("\n" + "=" * 70)
    print("é¢„æœŸæ¸…æ´—ç»“æœ:")
    print("=" * 70)
    print("âœ“ æ¸…æ´—æˆåŠŸ: 4 æ¡ï¼ˆ1ç¯‡æ–°é—» + 3æ¡è¯„è®ºï¼‰")
    print("âœ“ å»é‡è¿‡æ»¤: 0 æ¡ï¼ˆæ‰€æœ‰æ•°æ®éƒ½åº”è¯¥ä¿ç•™ï¼‰")
    print("âœ“ æ— æ•ˆæ•°æ®: 0 æ¡")
    
    print("\n" + "=" * 70)
    print("è¯·è¿è¡Œ cleaner æ¥æµ‹è¯•:")
    print("=" * 70)
    print("cd cleaner")
    print("python run_cleaner.py --mode once")
    print()


def test_duplicate_comments():
    """æµ‹è¯•é‡å¤è¯„è®ºèƒ½å¤Ÿæ­£ç¡®å»é‡"""
    
    print("\n" + "=" * 70)
    print("æµ‹è¯•åœºæ™¯ï¼šé‡å¤çš„è¯„è®º")
    print("=" * 70)
    
    # è¿æ¥ Redis
    r = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)
    queue_name = "financial_news_queue"
    
    # æ¸…ç©ºæµ‹è¯•é˜Ÿåˆ—
    r.delete(queue_name)
    
    # åˆ›å»º 2 æ¡ç›¸åŒçš„è¯„è®ºï¼ˆç›¸åŒ comment_idï¼‰
    comment1 = {
        "post_id": "news_99999",
        "comment_id": "comment_duplicate",
        "text": "This is a duplicate comment",
        "author": "test_user",
        "source": "reddit",
        "created_at": datetime.now().isoformat(),
        "timestamp": int(time.time())
    }
    
    comment2 = comment1.copy()  # å®Œå…¨ç›¸åŒçš„è¯„è®º
    
    # æ¨é€åˆ° Redis
    print("\nğŸ“¤ æ¨é€æµ‹è¯•æ•°æ®åˆ° Redis...")
    r.lpush(queue_name, json.dumps(comment1, ensure_ascii=False))
    r.lpush(queue_name, json.dumps(comment2, ensure_ascii=False))
    
    print(f"âœ“ å·²æ¨é€ 2 æ¡ç›¸åŒè¯„è®ºï¼ˆç›¸åŒ comment_idï¼‰")
    print(f"âœ“ é˜Ÿåˆ—é•¿åº¦: {r.llen(queue_name)}")
    
    print("\n" + "=" * 70)
    print("é¢„æœŸæ¸…æ´—ç»“æœ:")
    print("=" * 70)
    print("âœ“ æ¸…æ´—æˆåŠŸ: 1 æ¡ï¼ˆç¬¬ä¸€æ¡è¯„è®ºï¼‰")
    print("âœ“ å»é‡è¿‡æ»¤: 1 æ¡ï¼ˆç¬¬äºŒæ¡é‡å¤è¯„è®ºï¼‰")
    print("âœ“ æ— æ•ˆæ•°æ®: 0 æ¡")
    
    print("\n" + "=" * 70)
    print("è¯·è¿è¡Œ cleaner æ¥æµ‹è¯•:")
    print("=" * 70)
    print("cd cleaner")
    print("python run_cleaner.py --mode once")
    print()


def test_mixed_data():
    """æµ‹è¯•æ··åˆæ•°æ®ï¼ˆæ–°é—» + è¯„è®º + é‡å¤ï¼‰"""
    
    print("\n" + "=" * 70)
    print("æµ‹è¯•åœºæ™¯ï¼šæ··åˆæ•°æ®ï¼ˆæ–°é—» + è¯„è®º + é‡å¤ï¼‰")
    print("=" * 70)
    
    # è¿æ¥ Redis
    r = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)
    queue_name = "financial_news_queue"
    
    # æ¸…ç©ºæµ‹è¯•é˜Ÿåˆ—
    r.delete(queue_name)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    data = []
    
    # 2 ç¯‡æ–°é—»
    for i in range(1, 3):
        news = {
            "id": f"news_{i}",
            "title": f"Market News #{i}",
            "text": f"This is news article {i}",
            "source": "reuters",
            "url": f"https://reuters.com/article/{i}",
            "created_at": datetime.now().isoformat(),
            "timestamp": int(time.time())
        }
        data.append(news)
    
    # æ¯ç¯‡æ–°é—» 2 æ¡è¯„è®º
    for news_id in range(1, 3):
        for comment_id in range(1, 3):
            comment = {
                "post_id": f"news_{news_id}",
                "comment_id": f"comment_{news_id}_{comment_id}",
                "text": f"Comment {comment_id} on news {news_id}",
                "author": f"user_{comment_id}",
                "source": "reddit",
                "created_at": datetime.now().isoformat(),
                "timestamp": int(time.time())
            }
            data.append(comment)
    
    # 1 æ¡é‡å¤æ–°é—»
    duplicate_news = {
        "id": "news_1",  # ä¸ç¬¬ä¸€ç¯‡æ–°é—»ç›¸åŒ ID
        "title": "Market News #1 (Duplicate)",
        "text": "This is a duplicate of news 1",
        "source": "reuters",
        "url": "https://reuters.com/article/1",
        "created_at": datetime.now().isoformat(),
        "timestamp": int(time.time())
    }
    data.append(duplicate_news)
    
    # 1 æ¡é‡å¤è¯„è®º
    duplicate_comment = {
        "post_id": "news_1",
        "comment_id": "comment_1_1",  # ä¸ä¹‹å‰çš„è¯„è®ºç›¸åŒ ID
        "text": "Duplicate comment",
        "author": "user_1",
        "source": "reddit",
        "created_at": datetime.now().isoformat(),
        "timestamp": int(time.time())
    }
    data.append(duplicate_comment)
    
    # æ¨é€åˆ° Redis
    print("\nğŸ“¤ æ¨é€æµ‹è¯•æ•°æ®åˆ° Redis...")
    for item in data:
        r.lpush(queue_name, json.dumps(item, ensure_ascii=False))
    
    print(f"âœ“ å·²æ¨é€:")
    print(f"  - 2 ç¯‡æ–°é—»")
    print(f"  - 4 æ¡è¯„è®º (æ¯ç¯‡æ–°é—»2æ¡)")
    print(f"  - 1 æ¡é‡å¤æ–°é—»")
    print(f"  - 1 æ¡é‡å¤è¯„è®º")
    print(f"âœ“ é˜Ÿåˆ—é•¿åº¦: {r.llen(queue_name)}")
    
    print("\n" + "=" * 70)
    print("é¢„æœŸæ¸…æ´—ç»“æœ:")
    print("=" * 70)
    print("âœ“ æ¸…æ´—æˆåŠŸ: 6 æ¡ï¼ˆ2ç¯‡æ–°é—» + 4æ¡è¯„è®ºï¼‰")
    print("âœ“ å»é‡è¿‡æ»¤: 2 æ¡ï¼ˆ1æ¡é‡å¤æ–°é—» + 1æ¡é‡å¤è¯„è®ºï¼‰")
    print("âœ“ æ— æ•ˆæ•°æ®: 0 æ¡")
    
    print("\n" + "=" * 70)
    print("è¯·è¿è¡Œ cleaner æ¥æµ‹è¯•:")
    print("=" * 70)
    print("cd cleaner")
    print("python run_cleaner.py --mode once")
    print()


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='æµ‹è¯•æ–°é—»å’Œè¯„è®ºçš„å»é‡é€»è¾‘')
    parser.add_argument(
        '--test',
        choices=['basic', 'duplicate', 'mixed', 'all'],
        default='all',
        help='é€‰æ‹©æµ‹è¯•åœºæ™¯'
    )
    args = parser.parse_args()
    
    if args.test in ['basic', 'all']:
        test_news_with_comments()
    
    if args.test in ['duplicate', 'all']:
        test_duplicate_comments()
    
    if args.test in ['mixed', 'all']:
        test_mixed_data()
    
    print("\n" + "=" * 70)
    print("æµ‹è¯•æ•°æ®ç”Ÿæˆå®Œæˆï¼")
    print("=" * 70)
