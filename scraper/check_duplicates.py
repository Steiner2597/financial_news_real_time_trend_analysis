"""
Redis æ•°æ®æŸ¥é‡è„šæœ¬
æ£€æŸ¥ Redis é˜Ÿåˆ—ä¸­æ˜¯å¦å­˜åœ¨é‡å¤æ•°æ®
"""
import json
import yaml
from collections import defaultdict
from typing import Dict, List, Tuple
from utils.redis_client import RedisClient
from utils.logger import setup_logger

logger = setup_logger('check_duplicates')


class DuplicateChecker:
    """æ•°æ®æŸ¥é‡å™¨"""
    
    def __init__(self, redis_client: RedisClient):
        self.redis_client = redis_client
        self.queue_name = redis_client.queue_name
    
    def check_duplicates(self) -> Dict:
        """
        æ£€æŸ¥é˜Ÿåˆ—ä¸­çš„é‡å¤æ•°æ®
        
        Returns:
            dict: æŸ¥é‡ç»Ÿè®¡ç»“æœ
        """
        logger.info("=" * 70)
        logger.info("å¼€å§‹æ£€æŸ¥ Redis é˜Ÿåˆ—æ•°æ®é‡å¤æƒ…å†µ...")
        logger.info("=" * 70)
        
        # è·å–é˜Ÿåˆ—é•¿åº¦
        queue_length = self.redis_client.client.llen(self.queue_name)
        logger.info(f"ğŸ“Š é˜Ÿåˆ—æ€»é•¿åº¦: {queue_length} æ¡æ•°æ®")
        
        if queue_length == 0:
            logger.warning("âš ï¸  é˜Ÿåˆ—ä¸ºç©ºï¼Œæ— æ•°æ®å¯æŸ¥")
            return {'total': 0, 'unique': 0, 'duplicates': 0}
        
        # ç”¨äºå­˜å‚¨å”¯ä¸€æ ‡è¯†
        seen_ids = defaultdict(list)  # {å”¯ä¸€ID: [ç´¢å¼•åˆ—è¡¨]}
        seen_texts = defaultdict(list)  # {æ–‡æœ¬hash: [ç´¢å¼•åˆ—è¡¨]}
        
        # æŒ‰æ¥æºç»Ÿè®¡
        source_count = defaultdict(int)
        source_duplicates = defaultdict(int)
        
        logger.info("ğŸ” æ­£åœ¨æ‰«æé˜Ÿåˆ—æ•°æ®...")
        
        # éå†é˜Ÿåˆ—ï¼ˆä¸åˆ é™¤æ•°æ®ï¼‰
        for i in range(queue_length):
            try:
                # è·å–æ•°æ®ä½†ä¸åˆ é™¤
                json_data = self.redis_client.client.lindex(self.queue_name, i)
                if not json_data:
                    continue
                
                data = json.loads(json_data)
                source = data.get('source', 'unknown')
                source_count[source] += 1
                
                # ç”Ÿæˆå”¯ä¸€æ ‡è¯†
                unique_id = self._generate_unique_id(data)
                text_hash = hash(data.get('text', ''))
                
                # è®°å½•ä½ç½®
                seen_ids[unique_id].append(i)
                seen_texts[text_hash].append(i)
                
                # è¿›åº¦æ˜¾ç¤º
                if (i + 1) % 1000 == 0:
                    logger.info(f"  å·²æ‰«æ: {i + 1}/{queue_length} ({(i+1)/queue_length*100:.1f}%)")
                    
            except Exception as e:
                logger.error(f"  âœ— è§£ææ•°æ®å¤±è´¥ (ç´¢å¼• {i}): {e}")
        
        logger.info(f"âœ“ æ‰«æå®Œæˆ: {queue_length} æ¡æ•°æ®")
        print()
        
        # ç»Ÿè®¡é‡å¤æƒ…å†µ
        duplicate_ids = {uid: indices for uid, indices in seen_ids.items() if len(indices) > 1}
        duplicate_texts = {th: indices for th, indices in seen_texts.items() if len(indices) > 1}
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        total_items = queue_length
        unique_ids = len(seen_ids)
        duplicate_id_count = sum(len(indices) - 1 for indices in duplicate_ids.values())
        duplicate_text_count = sum(len(indices) - 1 for indices in duplicate_texts.values())
        
        # è¾“å‡ºç»Ÿè®¡ç»“æœ
        self._print_statistics(
            total_items,
            unique_ids,
            duplicate_id_count,
            duplicate_text_count,
            source_count,
            duplicate_ids,
            duplicate_texts
        )
        
        return {
            'total': total_items,
            'unique_ids': unique_ids,
            'duplicate_ids': duplicate_id_count,
            'duplicate_texts': duplicate_text_count,
            'source_count': dict(source_count),
            'duplicate_details': {
                'by_id': len(duplicate_ids),
                'by_text': len(duplicate_texts)
            }
        }
    
    def _generate_unique_id(self, data: Dict) -> str:
        """
        ç”Ÿæˆæ•°æ®å”¯ä¸€æ ‡è¯†
        
        Args:
            data: æ•°æ®å­—å…¸
        
        Returns:
            str: å”¯ä¸€æ ‡è¯†
        """
        source = data.get('source', '')
        
        # æ ¹æ®æ¥æºä½¿ç”¨ä¸åŒçš„å”¯ä¸€æ ‡è¯†ç­–ç•¥
        if source == 'reddit_post':
            return f"reddit:post:{data.get('post_id', '')}"
        elif source == 'reddit_comment':
            return f"reddit:comment:{data.get('comment_id', '')}"
        elif source == 'newsapi':
            return f"newsapi:url:{data.get('url', '')}"
        elif source == 'rss':
            return f"rss:url:{data.get('url', '')}"
        elif source == 'stocktwits':
            return f"stocktwits:id:{data.get('message_id', '')}"
        elif source == 'twitter':
            return f"twitter:id:{data.get('tweet_id', '')}"
        else:
            # ä½¿ç”¨ URL æˆ–æ–‡æœ¬å“ˆå¸Œä½œä¸ºå¤‡ç”¨
            url = data.get('url', '')
            if url:
                return f"{source}:url:{url}"
            return f"{source}:hash:{hash(data.get('text', ''))}"
    
    def _print_statistics(
        self,
        total: int,
        unique: int,
        dup_ids: int,
        dup_texts: int,
        source_count: Dict,
        duplicate_ids: Dict,
        duplicate_texts: Dict
    ):
        """æ‰“å°ç»Ÿè®¡ç»“æœ"""
        
        print()
        logger.info("=" * 70)
        logger.info("ğŸ“ˆ æŸ¥é‡ç»Ÿè®¡ç»“æœ")
        logger.info("=" * 70)
        
        # æ€»ä½“ç»Ÿè®¡
        logger.info(f"ğŸ“Š æ€»æ•°æ®é‡: {total} æ¡")
        logger.info(f"âœ“ å”¯ä¸€IDæ•°: {unique} ä¸ª")
        logger.info(f"âœ— IDé‡å¤æ•°: {dup_ids} æ¡ ({dup_ids/total*100:.2f}%)")
        logger.info(f"âœ— æ–‡æœ¬é‡å¤: {dup_texts} æ¡ ({dup_texts/total*100:.2f}%)")
        
        print()
        logger.info("-" * 70)
        logger.info("ğŸ“‘ æŒ‰æ¥æºç»Ÿè®¡")
        logger.info("-" * 70)
        
        for source, count in sorted(source_count.items(), key=lambda x: x[1], reverse=True):
            percentage = count / total * 100
            logger.info(f"  {source:20s}: {count:6d} æ¡ ({percentage:5.2f}%)")
        
        # é‡å¤è¯¦æƒ…ï¼ˆæ˜¾ç¤ºå‰10ä¸ªï¼‰
        if duplicate_ids:
            print()
            logger.info("-" * 70)
            logger.info(f"ğŸ” IDé‡å¤è¯¦æƒ… (å…± {len(duplicate_ids)} ç»„ï¼Œæ˜¾ç¤ºå‰10ç»„)")
            logger.info("-" * 70)
            
            for i, (uid, indices) in enumerate(list(duplicate_ids.items())[:10], 1):
                logger.info(f"  {i}. ID: {uid[:60]}...")
                logger.info(f"     é‡å¤ {len(indices)} æ¬¡ï¼Œä½ç½®: {indices[:5]}{'...' if len(indices) > 5 else ''}")
        
        if duplicate_texts:
            print()
            logger.info("-" * 70)
            logger.info(f"ğŸ” æ–‡æœ¬é‡å¤è¯¦æƒ… (å…± {len(duplicate_texts)} ç»„ï¼Œæ˜¾ç¤ºå‰5ç»„)")
            logger.info("-" * 70)
            
            for i, (th, indices) in enumerate(list(duplicate_texts.items())[:5], 1):
                # è¯»å–ç¬¬ä¸€æ¡æ•°æ®æŸ¥çœ‹å†…å®¹
                try:
                    json_data = self.redis_client.client.lindex(self.queue_name, indices[0])
                    data = json.loads(json_data)
                    text_preview = data.get('text', '')[:80]
                    logger.info(f"  {i}. æ–‡æœ¬é¢„è§ˆ: {text_preview}...")
                    logger.info(f"     é‡å¤ {len(indices)} æ¬¡ï¼Œä½ç½®: {indices[:5]}{'...' if len(indices) > 5 else ''}")
                except:
                    pass
        
        print()
        logger.info("=" * 70)
        
        # å»ºè®®
        if dup_ids > 0 or dup_texts > 0:
            logger.warning("âš ï¸  å‘ç°é‡å¤æ•°æ®ï¼å»ºè®®:")
            logger.warning("   1. æ£€æŸ¥çˆ¬è™«å»é‡é€»è¾‘æ˜¯å¦æ­£å¸¸å·¥ä½œ")
            logger.warning("   2. å¯ä»¥è¿è¡Œ clean_duplicates.py æ¸…ç†é‡å¤æ•°æ®")
            logger.warning("   3. ç¡®è®¤ Redis å»é‡é”®æ˜¯å¦è®¾ç½®äº†æ­£ç¡®çš„è¿‡æœŸæ—¶é—´")
        else:
            logger.info("âœ“ æœªå‘ç°é‡å¤æ•°æ®ï¼Œæ•°æ®è´¨é‡è‰¯å¥½ï¼")
        
        logger.info("=" * 70)
    
    def get_duplicate_details(self, show_content: bool = False) -> List[Dict]:
        """
        è·å–é‡å¤æ•°æ®çš„è¯¦ç»†ä¿¡æ¯
        
        Args:
            show_content: æ˜¯å¦æ˜¾ç¤ºæ•°æ®å†…å®¹
        
        Returns:
            list: é‡å¤æ•°æ®åˆ—è¡¨
        """
        queue_length = self.redis_client.client.llen(self.queue_name)
        seen = defaultdict(list)
        
        for i in range(queue_length):
            try:
                json_data = self.redis_client.client.lindex(self.queue_name, i)
                data = json.loads(json_data)
                unique_id = self._generate_unique_id(data)
                seen[unique_id].append((i, data if show_content else None))
            except:
                pass
        
        duplicates = []
        for uid, items in seen.items():
            if len(items) > 1:
                duplicates.append({
                    'unique_id': uid,
                    'count': len(items),
                    'indices': [idx for idx, _ in items],
                    'data': [d for _, d in items] if show_content else None
                })
        
        return duplicates


def main():
    """ä¸»å‡½æ•°"""
    # åŠ è½½é…ç½®
    with open('config.yaml', 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # åˆå§‹åŒ– Redis å®¢æˆ·ç«¯
    redis_client = RedisClient(**config['redis'])
    
    # åˆ›å»ºæŸ¥é‡å™¨
    checker = DuplicateChecker(redis_client)
    
    # æ‰§è¡ŒæŸ¥é‡
    result = checker.check_duplicates()
    
    print()
    logger.info("æŸ¥é‡å®Œæˆï¼")
    
    # è¯¢é—®æ˜¯å¦å¯¼å‡ºè¯¦æƒ…
    try:
        export = input("\næ˜¯å¦å¯¼å‡ºé‡å¤æ•°æ®è¯¦æƒ…åˆ°æ–‡ä»¶ï¼Ÿ(y/n): ").strip().lower()
        if export == 'y':
            duplicates = checker.get_duplicate_details(show_content=True)
            
            output_file = 'duplicate_details.json'
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(duplicates, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ“ é‡å¤æ•°æ®è¯¦æƒ…å·²å¯¼å‡ºåˆ°: {output_file}")
    except KeyboardInterrupt:
        print()
        logger.info("å·²å–æ¶ˆ")
    
    # å…³é—­è¿æ¥
    redis_client.close()


if __name__ == '__main__':
    main()
