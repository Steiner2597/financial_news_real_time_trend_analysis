# backend/visualization_app/services/data_loader.py
import json
import redis
import os
import sys

# ä½¿ç”¨ç›¸å¯¹å¯¼å…¥
from ..config import settings
from .mock_data_generator import MockDataGenerator
from .scheduler import get_scheduler


class DataLoader:
    """æ•°æ®åŠ è½½å™¨ - æ”¯æŒå•æ¬¡åŠ è½½å’Œå®šæ—¶å‘å¸ƒä¸¤ç§æ¨¡å¼"""

    def __init__(self):
        self.redis_client = redis.Redis(
            host=settings.REDIS_HOST,
            port=settings.REDIS_PORT,
            db=settings.REDIS_DB,  # ä½¿ç”¨é…ç½®çš„DBï¼ˆDB2ï¼‰
            password=settings.REDIS_PASSWORD,
            decode_responses=True
        )
        self.generator = MockDataGenerator()
        self.scheduler = get_scheduler()

    def load_mock_data_to_redis(self):
        """å•æ¬¡æ¨¡å¼ï¼šå°†æ¨¡æ‹Ÿæ•°æ®å­˜å‚¨åˆ°å‘å¸ƒè€…å‘½åç©ºé—´"""
        try:
            mock_data = self.generator.generate_complete_data()

            # ä½¿ç”¨æ–°çš„å‘å¸ƒè€…æ–¹æ³•
            success = self.redis_client.update_publisher_data(mock_data)

            if success:
                print("âœ… æ¨¡æ‹Ÿæ•°æ®å·²æˆåŠŸåŠ è½½åˆ°å‘å¸ƒè€…å‘½åç©ºé—´!")
                print(f"ğŸ“Š æ•°æ®ç‰ˆæœ¬: {mock_data['metadata']['data_version']}")
                print(f"ğŸ”‘ å‘å¸ƒè€…ç»“æ„: processed_data_publisher:*")
                print(f"ğŸ”” è®¢é˜…è€…ç»“æ„: processed_data:* (é€šè¿‡å‘å¸ƒè®¢é˜…åŒæ­¥)")
                print(f"ğŸ“ˆ çƒ­è¯æ•°é‡: {len(mock_data['trending_keywords'])}")
                print(f"ğŸ“° æ–°é—»æ•°é‡: {len(mock_data['news_feed'])}")

                # ç«‹å³åŒæ­¥ä¸€æ¬¡
                self.redis_client.sync_to_processed_data()
                print("âœ… æ•°æ®å·²åŒæ­¥åˆ°è®¢é˜…è€…å‘½åç©ºé—´")

            return success

        except Exception as e:
            print(f"âŒ åŠ è½½æ•°æ®åˆ°Rediså¤±è´¥: {e}")
            return False

    def start_pipeline_mode(self):
        """å¯åŠ¨ç®¡é“æ¨¡å¼ï¼šå®šæ—¶å‘å¸ƒæ•°æ®"""
        print("ğŸš€ å¯åŠ¨æ•°æ®ç®¡é“æ¨¡å¼...")
        return self.scheduler.start(initial_push=True)

    def stop_pipeline_mode(self):
        """åœæ­¢ç®¡é“æ¨¡å¼"""
        return self.scheduler.stop()

    def get_pipeline_status(self):
        """è·å–ç®¡é“æ¨¡å¼çŠ¶æ€"""
        return self.scheduler.get_status()

    def trigger_manual_update(self):
        """æ‰‹åŠ¨è§¦å‘æ•°æ®æ›´æ–°"""
        return self.scheduler.trigger_manual_update()

    def get_redis_info(self):
        """è·å–Redisä¿¡æ¯"""
        try:
            # æ£€æŸ¥ä¸»è¦é”®æ˜¯å¦å­˜åœ¨
            main_keys = [
                "processed_data:metadata",
                "processed_data:trending_keywords",
                "processed_data:word_cloud",
                "processed_data:news_feed"
            ]
            existing_keys = []

            for key in main_keys:
                if self.redis_client.exists(key):
                    existing_keys.append(key)

            # æ£€æŸ¥history_dataå­é”®
            history_keys = []
            all_keys = self.redis_client.keys("processed_data:history_data:*")
            for key in all_keys:
                if self.redis_client.exists(key):
                    history_keys.append(key)

            if existing_keys:
                print("ğŸ“Š Redis DB0ä¸­çš„æ•°æ®ä¿¡æ¯:")
                print(f"  processed_dataå­é”®: {[key.replace('processed_data:', '') for key in existing_keys]}")
                print(f"  history_dataå­é”®æ•°é‡: {len(history_keys)}")

                # æ˜¾ç¤ºmetadataä¿¡æ¯
                if "processed_data:metadata" in existing_keys:
                    metadata_json = self.redis_client.get("processed_data:metadata")
                    if metadata_json:
                        metadata = json.loads(metadata_json)
                        print(f"  æœ€åæ›´æ–°æ—¶é—´: {metadata['timestamp']}")
                        print(f"  æ›´æ–°é—´éš”: {metadata['update_interval']}åˆ†é’Ÿ")
                        print(f"  æ•°æ®ç‰ˆæœ¬: {metadata['data_version']}")

                # æ˜¾ç¤ºçƒ­è¯æ•°é‡
                if "processed_data:trending_keywords" in existing_keys:
                    trending_json = self.redis_client.get("processed_data:trending_keywords")
                    if trending_json:
                        trending = json.loads(trending_json)
                        print(f"  çƒ­è¯æ•°é‡: {len(trending)}")

                # æ˜¾ç¤ºæ–°é—»æ•°é‡
                if "processed_data:news_feed" in existing_keys:
                    news_json = self.redis_client.get("processed_data:news_feed")
                    if news_json:
                        news = json.loads(news_json)
                        print(f"  æ–°é—»æ•°é‡: {len(news)}")
            else:
                print("âš ï¸ Redis DB0ä¸­æ²¡æœ‰æ‰¾åˆ°æ•°æ®")
        except Exception as e:
            print(f"âŒ è·å–Redisä¿¡æ¯å¤±è´¥: {e}")


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='æ•°æ®åŠ è½½å™¨')
    parser.add_argument('--mode', choices=['once', 'pipeline'], default='once',
                        help='è¿è¡Œæ¨¡å¼: once(å•æ¬¡) æˆ– pipeline(ç®¡é“æ¨¡å¼)')

    args = parser.parse_args()

    loader = DataLoader()

    if args.mode == 'once':
        # å•æ¬¡æ¨¡å¼
        print("ğŸ¯ å•æ¬¡æ•°æ®åŠ è½½æ¨¡å¼")
        loader.load_mock_data_to_redis()
        loader.get_redis_info()
    else:
        # ç®¡é“æ¨¡å¼
        print("ğŸ”„ æ•°æ®ç®¡é“æ¨¡å¼")
        loader.start_pipeline_mode()

        try:
            # ä¿æŒç¨‹åºè¿è¡Œ
            import time

            while True:
                time.sleep(1)
        except KeyboardInterrupt:
            print("\nğŸ›‘ åœæ­¢æ•°æ®ç®¡é“...")
            loader.stop_pipeline_mode()