"""
å•æ¬¡æ¸…æ´—å¤„ç†å™¨
æä¾›ä¸€æ¬¡æ€§æ¸…æ´—æ•°æ®çš„åŠŸèƒ½ï¼Œä¸é˜»å¡ä¸»å¾ªç¯
"""
import redis
import logging
from typing import Dict, Any, List
from pathlib import Path

logger = logging.getLogger(__name__)


class SinglePassCleaner:
    """å•æ¬¡æ¸…æ´—å¤„ç†å™¨"""
    
    def __init__(self, redis_host: str, redis_port: int, db_in: int, db_out: int,
                 queue_in: str, queue_out: str, id_cache_key: str):
        """
        åˆå§‹åŒ–å•æ¬¡æ¸…æ´—å¤„ç†å™¨
        
        Args:
            redis_host: Redis ä¸»æœº
            redis_port: Redis ç«¯å£
            db_in: è¾“å…¥æ•°æ®åº“
            db_out: è¾“å‡ºæ•°æ®åº“
            queue_in: è¾“å…¥é˜Ÿåˆ—
            queue_out: è¾“å‡ºé˜Ÿåˆ—
            id_cache_key: ID ç¼“å­˜é”®
        """
        self.redis_host = redis_host
        self.redis_port = redis_port
        self.db_in = db_in
        self.db_out = db_out
        self.queue_in = queue_in
        self.queue_out = queue_out
        self.id_cache_key = id_cache_key
        
        # è¿æ¥ Redis
        self.r_in = redis.Redis(
            host=redis_host,
            port=redis_port,
            db=db_in,
            decode_responses=True
        )
        
        self.r_out = redis.Redis(
            host=redis_host,
            port=redis_port,
            db=db_out,
            decode_responses=True
        )
    
    def clean_once(self, batch_size: int = 100) -> Dict[str, Any]:
        """
        æ‰§è¡Œä¸€æ¬¡æ¸…æ´—æ“ä½œ
        
        Args:
            batch_size: æ¯æ‰¹å¤„ç†çš„æ•°é‡
            
        Returns:
            æ¸…æ´—ç»“æœç»Ÿè®¡
        """
        import json
        import time
        from datetime import datetime
        
        logger.info("\nğŸ§¹ å¼€å§‹å•æ¬¡æ¸…æ´—...")
        
        stats = {
            'total_processed': 0,
            'cleaned': 0,
            'duplicates': 0,
            'invalid': 0,
            'start_time': datetime.now().isoformat()
        }
        
        try:
            # è·å–é˜Ÿåˆ—å½“å‰é•¿åº¦ï¼ˆåªå¤„ç†è¿™äº›æ•°æ®ï¼Œä¸ç­‰å¾…æ–°æ•°æ®ï¼‰
            queue_length = self.r_in.llen(self.queue_in)
            logger.info(f"ğŸ“Š å¾…æ¸…æ´—æ•°æ®é‡: {queue_length}")
            
            if queue_length == 0:
                logger.info("â„¹ï¸  é˜Ÿåˆ—ä¸ºç©ºï¼Œæ— éœ€æ¸…æ´—")
                stats['end_time'] = datetime.now().isoformat()
                return stats
            
            # æ‰¹é‡å¤„ç†ï¼ˆä½¿ç”¨ LRANGE è¯»å–ï¼Œä¸åˆ é™¤åŸå§‹æ•°æ®ï¼‰
            processed = 0
            while processed < queue_length:
                # è®¡ç®—æœ¬æ‰¹æ¬¡å¤§å°
                current_batch = min(batch_size, queue_length - processed)
                
                # æ‰¹é‡è¯»å–æ•°æ®ï¼ˆä¸åˆ é™¤ï¼‰
                start_index = processed
                end_index = processed + current_batch - 1
                batch_data = self.r_in.lrange(self.queue_in, start_index, end_index)
                
                # å¤„ç†æ‰¹æ¬¡æ•°æ®
                for data_str in batch_data:
                    try:
                        # è§£ææ•°æ®
                        data = json.loads(data_str)
                        
                        # æ£€æŸ¥å¿…è¦å­—æ®µ
                        if not self._validate_data(data):
                            stats['invalid'] += 1
                            continue
                        
                        # æ£€æŸ¥å»é‡
                        item_id = self._get_item_id(data)
                        if self._is_duplicate(item_id):
                            stats['duplicates'] += 1
                            continue
                        
                        # æ¸…æ´—æ•°æ®
                        cleaned_data = self._clean_data(data)
                        
                        # æ¨é€åˆ°è¾“å‡ºé˜Ÿåˆ—
                        self.r_out.lpush(self.queue_out, json.dumps(cleaned_data, ensure_ascii=False))
                        
                        # æ·»åŠ åˆ°ç¼“å­˜
                        self._add_to_cache(item_id)
                        
                        stats['cleaned'] += 1
                        
                    except json.JSONDecodeError as e:
                        logger.warning(f"JSON è§£æå¤±è´¥: {e}")
                        stats['invalid'] += 1
                    except Exception as e:
                        logger.error(f"å¤„ç†æ•°æ®æ—¶å‡ºé”™: {e}")
                        stats['invalid'] += 1
                
                processed += len(batch_data)
                stats['total_processed'] = processed
                
                # æ˜¾ç¤ºè¿›åº¦
                if processed % 100 == 0 or processed >= queue_length:
                    logger.info(f"è¿›åº¦: {processed}/{queue_length} "
                               f"(æ¸…æ´—: {stats['cleaned']}, å»é‡: {stats['duplicates']}, æ— æ•ˆ: {stats['invalid']})")
            
            stats['end_time'] = datetime.now().isoformat()
            
            logger.info("\nâœ¨ å•æ¬¡æ¸…æ´—å®Œæˆ")
            logger.info(f"æ€»å¤„ç†: {stats['total_processed']}")
            logger.info(f"æ¸…æ´—æˆåŠŸ: {stats['cleaned']}")
            logger.info(f"å»é‡è¿‡æ»¤: {stats['duplicates']}")
            logger.info(f"æ— æ•ˆæ•°æ®: {stats['invalid']}")
            
            return stats
            
        except Exception as e:
            logger.error(f"æ¸…æ´—è¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            stats['error'] = str(e)
            stats['end_time'] = datetime.now().isoformat()
            return stats
    
    def _validate_data(self, data: Dict[str, Any]) -> bool:
        """
        éªŒè¯æ•°æ®æ˜¯å¦æœ‰æ•ˆ
        
        Args:
            data: æ•°æ®å­—å…¸
            
        Returns:
            æ˜¯å¦æœ‰æ•ˆ
        """
        # æ£€æŸ¥å¿…è¦å­—æ®µï¼šsource å¿…é¡»æœ‰ï¼Œæ–‡æœ¬å­—æ®µè‡³å°‘æœ‰ä¸€ä¸ª
        if 'source' not in data or not data['source']:
            return False
        
        # æ–‡æœ¬å­—æ®µï¼štextã€contentã€title è‡³å°‘æœ‰ä¸€ä¸ªä¸”éç©º
        has_text = any(
            field in data and data[field] and str(data[field]).strip()
            for field in ['text', 'content', 'title']
        )
        
        return has_text
    
    def _get_item_id(self, data: Dict[str, Any]) -> str:
        """
        è·å–æ•°æ®çš„å”¯ä¸€æ ‡è¯†
        
        Args:
            data: æ•°æ®å­—å…¸
            
        Returns:
            å”¯ä¸€æ ‡è¯†
        """
        # ä½¿ç”¨ ID æˆ–ç”Ÿæˆå“ˆå¸Œ
        if 'id' in data:
            return str(data['id'])
        
        # ä½¿ç”¨æ ‡é¢˜å’Œæ¥æºçš„ç»„åˆ
        import hashlib
        content = f"{data.get('title', '')}_{data.get('source', '')}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def _is_duplicate(self, item_id: str) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦é‡å¤
        
        Args:
            item_id: æ•°æ®ID
            
        Returns:
            æ˜¯å¦é‡å¤
        """
        import time
        
        # æ£€æŸ¥ç¼“å­˜ç±»å‹
        cache_type = self.r_out.type(self.id_cache_key)
        
        if cache_type == 'set':
            # SET ç±»å‹ï¼ˆæ°¸ä¹…æ¨¡å¼ï¼‰
            return self.r_out.sismember(self.id_cache_key, item_id)
        
        elif cache_type == 'zset':
            # ZSET ç±»å‹ï¼ˆæ—¶é—´çª—å£æ¨¡å¼ï¼‰
            score = self.r_out.zscore(self.id_cache_key, item_id)
            if score is None:
                return False
            
            # æ£€æŸ¥æ˜¯å¦åœ¨æ—¶é—´çª—å£å†…
            current_time = time.time()
            return score > (current_time - 86400)  # 24å°æ—¶çª—å£
        
        else:
            # ç¼“å­˜ä¸å­˜åœ¨æˆ–å…¶ä»–ç±»å‹
            return False
    
    def _add_to_cache(self, item_id: str):
        """
        æ·»åŠ åˆ°ç¼“å­˜
        
        Args:
            item_id: æ•°æ®ID
        """
        import time
        
        # æ£€æŸ¥ç¼“å­˜ç±»å‹
        cache_type = self.r_out.type(self.id_cache_key)
        
        if cache_type == 'none' or cache_type == 'zset':
            # ä½¿ç”¨ ZSETï¼ˆæ—¶é—´çª—å£æ¨¡å¼ï¼‰
            current_time = time.time()
            self.r_out.zadd(self.id_cache_key, {item_id: current_time})
            
            # æ¸…ç†è¿‡æœŸæ•°æ®
            expiry_time = current_time - 86400  # 24å°æ—¶å‰
            self.r_out.zremrangebyscore(self.id_cache_key, 0, expiry_time)
        else:
            # ä½¿ç”¨ SETï¼ˆæ°¸ä¹…æ¨¡å¼ï¼‰
            self.r_out.sadd(self.id_cache_key, item_id)
    
    def _clean_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ¸…æ´—æ•°æ®
        
        Args:
            data: åŸå§‹æ•°æ®
            
        Returns:
            æ¸…æ´—åçš„æ•°æ®
        """
        import re
        from datetime import datetime
        
        cleaned = {}
        
        # 1. æå– id å­—æ®µï¼ˆä»å¤šä¸ªå¯èƒ½çš„å­—æ®µåä¸­æå–ï¼‰
        id_value = (data.get("id") or data.get("post_id") or data.get("comment_id") or 
                    data.get("tweet_id") or data.get("guid") or data.get("message_id"))
        if id_value:
            cleaned['id'] = str(id_value)
        elif data.get('url'):
            # å¦‚æœæ²¡æœ‰ idï¼Œä½¿ç”¨ URL ä½œä¸ºå”¯ä¸€æ ‡è¯†
            cleaned['id'] = data['url']
        else:
            # æœ€åä½¿ç”¨æ—¶é—´æˆ³ä½œä¸º id
            import time
            cleaned['id'] = f"generated_{int(time.time() * 1000)}"
        
        # 2. æå– created_at å­—æ®µï¼ˆæ–°é—»/è¯„è®ºçš„å‘å¸ƒæ—¶é—´ï¼‰
        created_at = None
        for field in ["created_at", "created_utc", "published", "published_at",
                      "timestamp", "time", "datetime", "date"]:
            if field in data and data[field]:
                created_at = self._parse_time_field(data[field])
                if created_at:
                    break
        
        if created_at:
            cleaned['created_at'] = created_at
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ—¶é—´å­—æ®µï¼Œä½¿ç”¨å½“å‰æ—¶é—´
            cleaned['created_at'] = datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ")
        
        # 3. æ¸…æ´—æ–‡æœ¬å­—æ®µï¼ˆå¤„ç† textã€titleã€contentï¼‰
        for text_field in ['text', 'title', 'content']:
            if text_field in data and data[text_field]:
                text = str(data[text_field]).strip()
                # ç§»é™¤å¤šä½™ç©ºæ ¼
                text = re.sub(r'\s+', ' ', text)
                # ç§»é™¤HTMLæ ‡ç­¾ï¼ˆå¦‚æœæœ‰ï¼‰
                text = re.sub(r'<[^>]+>', '', text)
                cleaned[text_field] = text
        
        # 4. ä¿ç•™å…¶ä»–é‡è¦å­—æ®µ
        for key in ['source', 'url', 'author', 'score', 'comments', 
                    'sentiment', 'tags', 'subreddit', 'symbol', 'symbols']:
            if key in data:
                cleaned[key] = data[key]
        
        # 5. æ·»åŠ æ¸…æ´—æ—¶é—´æˆ³
        cleaned['cleaned_at'] = datetime.now().isoformat()
        
        return cleaned
    
    def _parse_time_field(self, value) -> str:
        """
        è§£ææ—¶é—´å­—æ®µï¼Œè½¬æ¢ä¸ºç»Ÿä¸€çš„ ISO æ ¼å¼å­—ç¬¦ä¸²
        
        Args:
            value: æ—¶é—´å€¼ï¼ˆå¯èƒ½æ˜¯ Unix æ—¶é—´æˆ³ã€å­—ç¬¦ä¸²ç­‰ï¼‰
            
        Returns:
            str: ISO æ ¼å¼æ—¶é—´å­—ç¬¦ä¸²ï¼Œå¦‚ "2024-01-01T12:00:00Z"
        """
        from datetime import datetime
        
        if value is None:
            return None
        
        try:
            # 1. å¦‚æœæ˜¯ Unix æ—¶é—´æˆ³ï¼ˆæ•´æ•°æˆ–æµ®ç‚¹æ•°ï¼‰
            if isinstance(value, (int, float)):
                dt = datetime.utcfromtimestamp(float(value))
                return dt.strftime("%Y-%m-%dT%H:%M:%SZ")
            
            # 2. å¦‚æœæ˜¯æ•°å­—å­—ç¬¦ä¸²
            if isinstance(value, str) and value.strip().replace('.', '').isdigit():
                dt = datetime.utcfromtimestamp(float(value))
                return dt.strftime("%Y-%m-%dT%H:%M:%SZ")
            
            # 3. å¦‚æœæ˜¯ ISO æ ¼å¼å­—ç¬¦ä¸²
            s = str(value).strip()
            try:
                dt = datetime.fromisoformat(s.replace("Z", "+00:00"))
                return dt.strftime("%Y-%m-%dT%H:%M:%SZ")
            except:
                pass
            
            # 4. å°è¯•å¸¸è§æ ¼å¼
            formats = [
                "%Y-%m-%d %H:%M:%S",
                "%Y/%m/%d %H:%M:%S",
                "%Y-%m-%dT%H:%M:%S",
                "%Y-%m-%d",
            ]
            for fmt in formats:
                try:
                    dt = datetime.strptime(s, fmt)
                    return dt.strftime("%Y-%m-%dT%H:%M:%SZ")
                except:
                    continue
            
            return None
        except Exception as e:
            logger.warning(f"æ—¶é—´è§£æå¤±è´¥: {value}, é”™è¯¯: {e}")
            return None
    
    def export_to_file(self, output_dir: Path) -> str:
        """
        å¯¼å‡ºæ¸…æ´—ç»“æœåˆ°æ–‡ä»¶
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        import json
        from datetime import datetime
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y-%m-%d")
        output_file = output_dir / f"cleaned_{timestamp}.jsonl"
        
        # è·å–é˜Ÿåˆ—ä¸­çš„æ‰€æœ‰æ•°æ®
        queue_length = self.r_out.llen(self.queue_out)
        
        if queue_length == 0:
            logger.info("â„¹ï¸  è¾“å‡ºé˜Ÿåˆ—ä¸ºç©ºï¼Œæ— æ•°æ®å¯¼å‡º")
            return str(output_file)
        
        logger.info(f"ğŸ“¦ å¯¼å‡º {queue_length} æ¡æ•°æ®åˆ°æ–‡ä»¶...")
        
        # å¯¼å‡ºæ•°æ®
        with open(output_file, 'w', encoding='utf-8') as f:
            # ä½¿ç”¨ LRANGE è¯»å–æ‰€æœ‰æ•°æ®ï¼ˆä¸åˆ é™¤ï¼‰
            data_list = self.r_out.lrange(self.queue_out, 0, -1)
            
            for data_str in data_list:
                try:
                    data = json.loads(data_str)
                    f.write(json.dumps(data, ensure_ascii=False) + '\n')
                except:
                    pass
        
        logger.info(f"âœ… æ•°æ®å·²å¯¼å‡ºåˆ°: {output_file}")
        return str(output_file)
    
    def close(self):
        """å…³é—­è¿æ¥"""
        try:
            self.r_in.close()
            self.r_out.close()
        except:
            pass
