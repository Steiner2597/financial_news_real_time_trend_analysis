"""
Reddit çˆ¬è™«æ¨¡å— - ä¼˜åŒ–ç‰ˆï¼ˆä¼˜å…ˆçˆ¬æœ€æ–°å†…å®¹ï¼‰
æŠ“å–è´¢ç»ç›¸å…³å­ç‰ˆå—çš„å¸–å­å’Œè¯„è®º
"""
import time
import praw
import prawcore
from datetime import datetime
from typing import List, Dict, Any
from utils.logger import setup_logger
from utils.redis_client import RedisClient

logger = setup_logger('reddit_crawler')


class RedditCrawler:
    """Reddit çˆ¬è™«ç±»ï¼ˆå·²ä¼˜åŒ–å®æ—¶æ€§ï¼‰"""
    
    def __init__(self, config: dict, redis_client: RedisClient):
        """
        åˆå§‹åŒ– Reddit çˆ¬è™«
        
        Args:
            config: Reddit é…ç½®å­—å…¸
            redis_client: Redis å®¢æˆ·ç«¯å®ä¾‹
        """
        self.config = config
        self.redis_client = redis_client
        
        try:
            self.reddit = praw.Reddit(
                client_id=config['client_id'],
                client_secret=config['client_secret'],
                user_agent=config['user_agent']
            )
            logger.info("Reddit API åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"Reddit API åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
        
        self.subreddits = config.get('subreddits', ['investing', 'finance'])
        self.posts_limit = config.get('posts_limit', 50)
        self.comments_limit = config.get('comments_limit', 30)
        self.post_filters = config.get('post_filters', {})
    
    def crawl(self) -> Dict[str, int]:
        """
        æ‰§è¡ŒæŠ“å–ä»»åŠ¡ï¼ˆå·²ä¼˜åŒ–å®æ—¶æ€§ï¼‰
        
        Returns:
            dict: æŠ“å–ç»Ÿè®¡ä¿¡æ¯ {'posts': æ•°é‡, 'comments': æ•°é‡, 'errors': æ•°é‡}
        """
        stats = {'posts': 0, 'comments': 0, 'errors': 0}
        
        logger.info("å¼€å§‹æŠ“å– Reddit æ•°æ®ï¼ˆå®æ—¶ä¼˜å…ˆï¼‰...")
        
        # 1. ä¼˜å…ˆæŠ“å–å…³é”®è¯æœç´¢ï¼ˆæœ€æ–°å†…å®¹ï¼‰
        if self.config.get('search_enabled', False):
            search_stats = self._crawl_search_keywords()
            stats['posts'] += search_stats.get('posts', 0)
            stats['comments'] += search_stats.get('comments', 0)
            stats['errors'] += search_stats.get('errors', 0)
        
        # 2. æŠ“å–å­ç‰ˆå—æœ€æ–°å†…å®¹
        for subreddit_name in self.subreddits:
            sub_posts = 0
            sub_comments = 0
            
            try:
                logger.info(f"æ­£åœ¨æŠ“å–å­ç‰ˆå—: r/{subreddit_name}ï¼ˆæœ€æ–°å†…å®¹ï¼‰")
                subreddit = self.reddit.subreddit(subreddit_name)
                
                # æŠ“å–æ–°å¸–å­
                post_index = 0
                for submission in subreddit.new(limit=self.posts_limit):
                    post_index += 1
                    
                    # 2.1 å»é‡æ£€æŸ¥ï¼ˆRedisï¼‰
                    if self._is_post_processed(submission.id):
                        logger.debug(f"  â­ï¸ è·³è¿‡å·²æŠ“å–å¸–å­: {submission.id}")
                        continue
                    
                    # æ˜¾ç¤ºæ­£åœ¨å¤„ç†çš„å¸–å­
                    logger.info(f"  [{post_index}/{self.posts_limit}] æŠ“å–å¸–å­: {submission.title[:60]}...")
                    
                    # 2.2 æå–å¸–å­ä¿¡æ¯
                    post_data = self._extract_post_data(submission, subreddit_name)
                    if not post_data:
                        continue
                    
                    # 2.3 åº”ç”¨è¿‡æ»¤æ¡ä»¶ï¼ˆå…³é”®ä¼˜åŒ–ï¼ï¼‰
                    if not self._apply_post_filters(post_data):
                        continue
                    
                    # 2.4 ä¿å­˜åˆ° Redis
                    if self.redis_client.push_data(post_data):
                        stats['posts'] += 1
                        sub_posts += 1
                        # æ ‡è®°ä¸ºå·²å¤„ç†ï¼ˆ7å¤©æœ‰æ•ˆæœŸï¼‰
                        self._mark_post_processed(submission.id)
                        logger.debug(f"    âœ“ æ–°å¸–ä¿å­˜ (ID: {submission.id}, upvotes: {post_data['score']})")
                    
                    # 2.5 æŠ“å–è¯„è®º
                    logger.info(f"    â†’ æ­£åœ¨æŠ“å–è¯„è®º (ç›®æ ‡: {self.comments_limit} æ¡)...")
                    comments_count = self._crawl_comments(submission, subreddit_name)
                    stats['comments'] += comments_count
                    sub_comments += comments_count
                    logger.info(f"    âœ“ è¯„è®ºæŠ“å–å®Œæˆ: {comments_count} æ¡")
                    
                    # é¿å…è¯·æ±‚è¿‡å¿«
                    time.sleep(0.5)
                
                logger.info(
                    f"âœ“ å­ç‰ˆå— r/{subreddit_name} æŠ“å–å®Œæˆ - "
                    f"æ–°å¸–: {sub_posts}, è¯„è®º: {sub_comments}"
                )
                
            except prawcore.ResponseException as e:
                if e.response.status_code == 429:
                    logger.warning(f"Reddit API é€Ÿç‡é™åˆ¶: {e}, ç­‰å¾…60ç§’åé‡è¯•...")
                    time.sleep(60)
                else:
                    logger.error(f"Reddit API å“åº”é”™è¯¯ ({e.response.status_code}): {e}")
                stats['errors'] += 1
                
            except Exception as e:
                logger.error(f"æŠ“å–å­ç‰ˆå— r/{subreddit_name} æ—¶å‡ºé”™: {e}")
                stats['errors'] += 1
        
        logger.info(f"æŠ“å–å®Œæˆ - æ–°å¸–: {stats['posts']}, è¯„è®º: {stats['comments']}, é”™è¯¯: {stats['errors']}")
        return stats
    
    def _crawl_search_keywords(self) -> Dict[str, int]:
        """
        æŠ“å–å…³é”®è¯æœç´¢ï¼ˆæœ€æ–°å†…å®¹ï¼‰
        
        Returns:
            dict: æœç´¢ç»Ÿè®¡ä¿¡æ¯
        """
        stats = {'posts': 0, 'comments': 0, 'errors': 0}
        keywords = self.config.get('search_keywords', [])
        search_settings = self.config.get('search_settings', {})
        time_filter = search_settings.get('time_filter', 'day')
        sort = search_settings.get('sort', 'new')
        posts_per_kw = search_settings.get('posts_per_keyword', 15)
        
        logger.info(f"ğŸ” å¼€å§‹å…³é”®è¯å®æ—¶ç›‘æ§ï¼ˆ{len(keywords)} ä¸ªå…³é”®è¯ï¼Œ{time_filter}å†…æœ€æ–°å†…å®¹ï¼‰")
        
        for kw in keywords:
            try:
                logger.info(f"  â”œâ”€ æœç´¢å…³é”®è¯: '{kw}' (time={time_filter}, sort={sort})")
                submissions = self.reddit.subreddit('all').search(
                    query=kw,
                    sort=sort,
                    time_filter=time_filter,
                    limit=posts_per_kw
                )
                
                count = 0
                for submission in submissions:
                    # å»é‡æ£€æŸ¥
                    if self._is_post_processed(submission.id):
                        continue
                    
                    post_data = self._extract_post_data(submission, 'search')
                    if not post_data or not self._apply_post_filters(post_data):
                        continue
                    
                    if self.redis_client.push_data(post_data):
                        count += 1
                        stats['posts'] += 1
                        self._mark_post_processed(submission.id)
                        
                        # æŠ“å–æœç´¢ç»“æœçš„è¯„è®º
                        comments_count = self._crawl_comments(submission, 'search')
                        stats['comments'] += comments_count
                    
                    time.sleep(0.3)  # æœç´¢è¯·æ±‚æ›´æ•æ„Ÿ
                
                logger.info(f"  â””â”€ âœ“ å…³é”®è¯ '{kw}' æŠ“å– {count} æ¡æ–°å¸–")
                
            except Exception as e:
                logger.error(f"  âœ— å…³é”®è¯ '{kw}' æœç´¢å¤±è´¥: {e}")
                stats['errors'] += 1
        
        return stats
    
    def _is_post_processed(self, post_id: str) -> bool:
        """
        æ£€æŸ¥å¸–å­æ˜¯å¦å·²å¤„ç†
        
        Args:
            post_id: Reddit å¸–å­ID
        
        Returns:
            bool: æ˜¯å¦å·²å¤„ç†
        """
        try:
            key = f"reddit:post:{post_id}"
            return self.redis_client.client.exists(key) > 0
        except Exception as e:
            logger.warning(f"æ£€æŸ¥å¸–å­å»é‡çŠ¶æ€å¤±è´¥: {e}")
            return False
    
    def _mark_post_processed(self, post_id: str):
        """
        æ ‡è®°å¸–å­ä¸ºå·²å¤„ç†
        
        Args:
            post_id: Reddit å¸–å­ID
        """
        try:
            key = f"reddit:post:{post_id}"
            # 7å¤©æœ‰æ•ˆæœŸï¼ˆ604800ç§’ï¼‰
            self.redis_client.client.setex(key, 604800, "1")
        except Exception as e:
            logger.warning(f"æ ‡è®°å¸–å­å·²å¤„ç†å¤±è´¥: {e}")
    
    def _is_comment_processed(self, comment_id: str) -> bool:
        """
        æ£€æŸ¥è¯„è®ºæ˜¯å¦å·²å¤„ç†
        
        Args:
            comment_id: Reddit è¯„è®ºID
        
        Returns:
            bool: æ˜¯å¦å·²å¤„ç†
        """
        try:
            key = f"reddit:comment:{comment_id}"
            return self.redis_client.client.exists(key) > 0
        except Exception as e:
            logger.warning(f"æ£€æŸ¥è¯„è®ºå»é‡çŠ¶æ€å¤±è´¥: {e}")
            return False
    
    def _mark_comment_processed(self, comment_id: str):
        """
        æ ‡è®°è¯„è®ºä¸ºå·²å¤„ç†
        
        Args:
            comment_id: Reddit è¯„è®ºID
        """
        try:
            key = f"reddit:comment:{comment_id}"
            # 7å¤©æœ‰æ•ˆæœŸï¼ˆ604800ç§’ï¼‰
            self.redis_client.client.setex(key, 604800, "1")
        except Exception as e:
            logger.warning(f"æ ‡è®°è¯„è®ºå·²å¤„ç†å¤±è´¥: {e}")
    
    def _is_valid_comment(self, comment_data: Dict) -> bool:
        """
        æ£€æŸ¥è¯„è®ºæ˜¯å¦æœ‰æ•ˆï¼ˆè¿‡æ»¤åˆ é™¤/ç§»é™¤çš„è¯„è®ºå’Œæ— æ„ä¹‰å†…å®¹ï¼‰
        
        Args:
            comment_data: è¯„è®ºæ•°æ®
        
        Returns:
            bool: æ˜¯å¦æœ‰æ•ˆ
        """
        text = comment_data.get('text', '').strip()
        
        # è¿‡æ»¤å·²åˆ é™¤/ç§»é™¤çš„è¯„è®º
        if text in ['[deleted]', '[removed]']:
            logger.debug(f"        â­ï¸ è·³è¿‡å·²åˆ é™¤è¯„è®º")
            return False
        
        # è¿‡æ»¤è¿‡çŸ­çš„è¯„è®ºï¼ˆå°‘äº3ä¸ªå­—ç¬¦ï¼‰
        if len(text) < 3:
            logger.debug(f"        â­ï¸ è·³è¿‡è¿‡çŸ­è¯„è®º: {text}")
            return False
        
        # å¯é€‰ï¼šè¿‡æ»¤çº¯è¡¨æƒ…æˆ–ç¬¦å·çš„è¯„è®º
        if text in ['Lol', 'lol', 'LOL', '...', '???', '!!!', 'ğŸ‘', 'ğŸ˜‚']:
            logger.debug(f"        â­ï¸ è·³è¿‡æ— æ„ä¹‰è¯„è®º: {text}")
            return False
        
        return True
    
    def _apply_post_filters(self, post_data: Dict) -> bool:
        """
        åº”ç”¨è¿‡æ»¤æ¡ä»¶ï¼ˆæ–°å¸–éœ€æ”¾å®½ï¼‰
        
        Args:
            post_data: å¸–å­æ•°æ®
        
        Returns:
            bool: æ˜¯å¦é€šè¿‡è¿‡æ»¤
        """
        min_upvotes = self.post_filters.get('min_upvotes', 0)
        min_comments = self.post_filters.get('min_comments', 0)
        
        if post_data['score'] < min_upvotes or post_data['num_comments'] < min_comments:
            logger.debug(
                f"  â­ï¸ è·³è¿‡ä½çƒ­åº¦å¸–å­ (upvotes={post_data['score']}, "
                f"comments={post_data['num_comments']})"
            )
            return False
        return True
    
    def _extract_post_data(self, submission, subreddit_name: str) -> Dict[str, Any]:
        """
        æå–å¸–å­æ•°æ® - å°½å¯èƒ½å®Œæ•´çš„ä¿¡æ¯
        
        Args:
            submission: PRAW submission å¯¹è±¡
            subreddit_name: å­ç‰ˆå—åç§°
        
        Returns:
            dict: å¸–å­æ•°æ®
        """
        try:
            # ç»„åˆæ ‡é¢˜å’Œæ­£æ–‡
            text = submission.title
            if submission.selftext:
                text += "\n\n" + submission.selftext
            
            # è½¬æ¢ created_utc (Unix æ—¶é—´æˆ³) ä¸º ISO 8601 æ ¼å¼å­—ç¬¦ä¸²
            from datetime import datetime
            created_dt = datetime.utcfromtimestamp(submission.created_utc)
            created_at_iso = created_dt.strftime("%Y-%m-%dT%H:%M:%SZ")
            
            data = {
                # åŸºç¡€å­—æ®µ
                'text': text,
                'source': 'reddit_post',
                'created_at': created_at_iso,  # â† ISO 8601 æ ¼å¼å­—ç¬¦ä¸²
                'url': f"https://www.reddit.com{submission.permalink}",
                
                # Reddit ç‰¹æœ‰å­—æ®µ
                'subreddit': subreddit_name,
                'post_id': submission.id,
                'author': str(submission.author) if submission.author else '[deleted]',
                
                # äº’åŠ¨æ•°æ®ï¼ˆç”¨äºè¶‹åŠ¿åˆ†æï¼‰
                'score': submission.score,
                'upvote_ratio': getattr(submission, 'upvote_ratio', 0),
                'num_comments': submission.num_comments,
                
                # å†…å®¹ç±»å‹
                'is_self': submission.is_self,
                'is_video': submission.is_video,
                'link_flair_text': submission.link_flair_text,
                
                # å…¶ä»–å…ƒæ•°æ®
                'gilded': submission.gilded,
                'distinguished': submission.distinguished,
                'stickied': submission.stickied,
                'over_18': submission.over_18,
                'spoiler': submission.spoiler,
                
                # æ ‡é¢˜å•ç‹¬ä¿ç•™ï¼ˆç”¨äºå…³é”®è¯æå–ï¼‰
                'title': submission.title,
                'selftext': submission.selftext if submission.selftext else ''
            }
            return data
        except Exception as e:
            logger.error(f"æå–å¸–å­æ•°æ®å¤±è´¥: {e}")
            return None
    
    def _crawl_comments(self, submission, subreddit_name: str) -> int:
        """
        æŠ“å–å¸–å­çš„è¯„è®ºï¼ˆå·²æ·»åŠ å»é‡å’Œè¿‡æ»¤ï¼‰
        
        Args:
            submission: PRAW submission å¯¹è±¡
            subreddit_name: å­ç‰ˆå—åç§°
        
        Returns:
            int: æˆåŠŸæŠ“å–çš„è¯„è®ºæ•°é‡
        """
        count = 0
        try:
            # å±•å¼€æ‰€æœ‰è¯„è®ºï¼ˆé™åˆ¶æ•°é‡ä»¥é¿å…è¿‡å¤šè¯·æ±‚ï¼‰
            submission.comments.replace_more(limit=0)
            
            # è·å–è¯„è®ºåˆ—è¡¨
            all_comments = submission.comments.list()
            
            # æŒ‰è¯„åˆ†æ’åºï¼Œå–å‰Næ¡ï¼ˆä¿ç•™åŸé€»è¾‘ï¼‰
            sorted_comments = sorted(
                all_comments, 
                key=lambda c: c.score if hasattr(c, 'score') else 0, 
                reverse=True
            )[:self.comments_limit]
            
            for idx, comment in enumerate(sorted_comments, 1):
                try:
                    # ğŸ”¥ æ–°å¢ï¼šè¯„è®ºå»é‡æ£€æŸ¥
                    if self._is_comment_processed(comment.id):
                        logger.debug(f"        â­ï¸ è·³è¿‡å·²æŠ“å–è¯„è®º: {comment.id}")
                        continue
                    
                    comment_data = self._extract_comment_data(comment, subreddit_name)
                    
                    # ğŸ”¥ æ–°å¢ï¼šè¿‡æ»¤æ— æ•ˆè¯„è®º
                    if not comment_data or not self._is_valid_comment(comment_data):
                        continue
                    
                    if self.redis_client.push_data(comment_data):
                        count += 1
                        # ğŸ”¥ æ–°å¢ï¼šæ ‡è®°è¯„è®ºä¸ºå·²å¤„ç†
                        self._mark_comment_processed(comment.id)
                except Exception as e:
                    logger.warning(f"        âœ— ä¿å­˜è¯„è®ºå¤±è´¥: {e}")
            
        except prawcore.ResponseException as e:
            logger.error(
                f"      âœ— Reddit API é”™è¯¯ (å¸–å­: {submission.id}): "
                f"{e.response.status_code} - {e.response.reason}"
            )
        except Exception as e:
            logger.error(f"      âœ— æŠ“å–è¯„è®ºæ—¶å‡ºé”™: {e}")
        
        return count
    
    def _extract_comment_data(self, comment, subreddit_name: str) -> Dict[str, Any]:
        """
        æå–è¯„è®ºæ•°æ® - å®Œæ•´ä¿¡æ¯ï¼ˆç”¨äºæƒ…æ„Ÿåˆ†æï¼‰ï¼ˆä¿æŒåŸé€»è¾‘ï¼‰
        
        Args:
            comment: PRAW comment å¯¹è±¡
            subreddit_name: å­ç‰ˆå—åç§°
        
        Returns:
            dict: è¯„è®ºæ•°æ®
        """
        try:
            data = {
                # åŸºç¡€å­—æ®µ
                'text': comment.body,
                'source': 'reddit_comment',
                'timestamp': int(comment.created_utc),
                'url': f"https://www.reddit.com{comment.permalink}",
                
                # Reddit ç‰¹æœ‰å­—æ®µ
                'subreddit': subreddit_name,
                'comment_id': comment.id,
                'author': str(comment.author) if comment.author else '[deleted]',
                
                # äº’åŠ¨æ•°æ®ï¼ˆç”¨äºæƒ…æ„Ÿåˆ†ææƒé‡ï¼‰
                'score': comment.score,
                'gilded': comment.gilded,
                'distinguished': comment.distinguished,
                'stickied': comment.stickied,
                
                # è¯„è®ºä¸Šä¸‹æ–‡
                'is_submitter': comment.is_submitter,  # æ˜¯å¦æ˜¯å¸–å­ä½œè€…
                'parent_id': comment.parent_id,
                'link_id': comment.link_id
            }
            return data
        except Exception as e:
            logger.error(f"æå–è¯„è®ºæ•°æ®å¤±è´¥: {e}")
            return None


def main():
    """æµ‹è¯•å‡½æ•°"""
    import yaml
    
    # åŠ è½½é…ç½®
    with open('config.yaml', 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # åˆå§‹åŒ– Redis å®¢æˆ·ç«¯
    redis_client = RedisClient(**config['redis'])
    
    # åˆå§‹åŒ–å¹¶è¿è¡Œçˆ¬è™«
    crawler = RedditCrawler(config['reddit'], redis_client)
    stats = crawler.crawl()
    
    print(f"æŠ“å–å®Œæˆ: {stats}")
    
    # å…³é—­è¿æ¥
    redis_client.close()


if __name__ == '__main__':
    main()
