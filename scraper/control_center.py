"""
æ•°æ®æŠ“å–æ§åˆ¶ä¸­å¿ƒ
ç»Ÿä¸€ç®¡ç†æ‰€æœ‰çˆ¬è™«çš„å¯åŠ¨ã€åœæ­¢å’Œé…ç½®
"""
import yaml
import sys
import time
import argparse
import redis
import json
from datetime import datetime, timedelta
from typing import Dict, List
from utils.logger import setup_logger
from utils.redis_client import RedisClient
from utils.data_exporter import DataExporter
from crawlers.reddit_crawler import RedditCrawler
from crawlers.rss_crawler import RSSCrawler
from crawlers.newsapi_crawler import NewsAPICrawler
from crawlers.stocktwits_crawler import StockTwitsCrawler
from crawlers.alphavantage_crawler import AlphaVantageCrawler  # âœ… æ–°å¢

logger = setup_logger('control_center')


class CrawlerControlCenter:
    """çˆ¬è™«æ§åˆ¶ä¸­å¿ƒ"""
    
    def __init__(self, config_file='config.yaml'):
        """
        åˆå§‹åŒ–æ§åˆ¶ä¸­å¿ƒ
        
        Args:
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config_file = config_file
        self.config = self._load_config()
        self.redis_client = None
        self.crawlers = {}
        self.statistics = {
            'reddit': {'posts': 0, 'comments': 0, 'errors': 0},
            'newsapi': {'articles': 0, 'errors': 0},
            'rss': {'articles': 0, 'errors': 0},
            'stocktwits': {'messages': 0, 'errors': 0},
            'alphavantage': {'items': 0, 'errors': 0}  # âœ… æ–°å¢
        }
        
        # è®°å½•ä¸Šæ¬¡è¿è¡Œæ—¶é—´ï¼ˆç”¨äºç‹¬ç«‹é—´éš”æ§åˆ¶ï¼‰
        self.last_run_times = {
            'reddit': 0,
            'newsapi': 0,
            'rss': 0,
            'stocktwits': 0,
            'twitter': 0,
            'alphavantage': 0
        }
        
        logger.info("=" * 60)
        logger.info("æ§åˆ¶ä¸­å¿ƒåˆå§‹åŒ–")
        logger.info("=" * 60)
        
        # åˆå§‹åŒ– Redis è¿æ¥
        self._init_redis()
        
        # åˆå§‹åŒ–çˆ¬è™«
        self._init_crawlers()
    
    def _load_config(self) -> dict:
        """
        åŠ è½½é…ç½®æ–‡ä»¶
        
        Returns:
            dict: é…ç½®å­—å…¸
        """
        try:
            with open(self.config_file, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            logger.info(f"é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {self.config_file}")
            return config
        except FileNotFoundError:
            logger.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.config_file}")
            logger.error("è¯·åˆ›å»º config.yamlï¼ˆå¯å‚è€ƒ README é…ç½®è¯´æ˜ï¼‰")
            sys.exit(1)
        except Exception as e:
            logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            sys.exit(1)
    
    def _init_redis(self):
        """åˆå§‹åŒ– Redis è¿æ¥"""
        try:
            redis_config = self.config.get('redis', {})
            self.redis_client = RedisClient(
                host=redis_config.get('host', 'localhost'),
                port=redis_config.get('port', 6379),
                db=redis_config.get('db', 0),
                password=redis_config.get('password'),
                queue_name=redis_config.get('queue_name', 'financial_data'),
                storage_config=redis_config.get('storage_optimization', {}),
                source_quotas=redis_config.get('source_quotas', {}),
            )
            logger.info("âœ“ Redis è¿æ¥æˆåŠŸ")
        except Exception as e:
            logger.error(f"âœ— Redis è¿æ¥å¤±è´¥: {e}")
            sys.exit(1)
    
    def _init_crawlers(self):
        """åˆå§‹åŒ–æ‰€æœ‰çˆ¬è™«"""
        logger.info("\n" + "=" * 60)
        logger.info("åˆå§‹åŒ–çˆ¬è™«æ¨¡å—")
        logger.info("=" * 60)

        def is_enabled(section_name: str, default_if_present: bool = False) -> bool:
            conf = self.config.get(section_name, None)
            if conf is None:
                return False
            if 'enabled' in conf:
                return bool(conf.get('enabled'))
            # å¦‚æœé…ç½®æ®µå­˜åœ¨ä½†æœªæä¾› enabledï¼šreddit é»˜è®¤å¯ç”¨ï¼Œnewsapi é»˜è®¤ç¦ç”¨ï¼Œå…¶å®ƒéµå¾ª default_if_present
            if section_name == 'reddit':
                return True
            if section_name == 'newsapi':
                return False
            return default_if_present
        
        # Reddit
        if is_enabled('reddit'):
            try:
                self.crawlers['reddit'] = RedditCrawler(
                    self.config.get('reddit', {}),
                    self.redis_client
                )
                logger.info("âœ“ Reddit çˆ¬è™«åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.error(f"âœ— Reddit çˆ¬è™«åˆå§‹åŒ–å¤±è´¥: {e}")
        else:
            logger.info("â—‹ Reddit çˆ¬è™«å·²ç¦ç”¨")
        
        # NewsAPI
        if is_enabled('newsapi'):
            try:
                self.crawlers['newsapi'] = NewsAPICrawler(
                    self.config.get('newsapi', {}),
                    self.redis_client
                )
                logger.info("âœ“ NewsAPI çˆ¬è™«åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.error(f"âœ— NewsAPI çˆ¬è™«åˆå§‹åŒ–å¤±è´¥: {e}")
        else:
            logger.info("â—‹ NewsAPI çˆ¬è™«å·²ç¦ç”¨")
        
        # RSS
        if is_enabled('rss', default_if_present=False):
            try:
                self.crawlers['rss'] = RSSCrawler(
                    self.config.get('rss', {}),
                    self.redis_client
                )
                logger.info("âœ“ RSS çˆ¬è™«åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.error(f"âœ— RSS çˆ¬è™«åˆå§‹åŒ–å¤±è´¥: {e}")
        else:
            logger.info("â—‹ RSS çˆ¬è™«å·²ç¦ç”¨")
        
        # StockTwits
        if is_enabled('stocktwits', default_if_present=False):
            try:
                self.crawlers['stocktwits'] = StockTwitsCrawler(
                    self.config.get('stocktwits', {}),
                    self.redis_client
                )
                logger.info("âœ“ StockTwits çˆ¬è™«åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.error(f"âœ— StockTwits çˆ¬è™«åˆå§‹åŒ–å¤±è´¥: {e}")
        else:
            logger.info("â—‹ StockTwits çˆ¬è™«å·²ç¦ç”¨")
        
        # Alpha Vantage
        if is_enabled('alphavantage', default_if_present=False):
            try:
                self.crawlers['alphavantage'] = AlphaVantageCrawler(
                    self.config.get('alphavantage', {}),
                    self.redis_client
                )
                logger.info("âœ“ Alpha Vantage çˆ¬è™«åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.error(f"âœ— Alpha Vantage çˆ¬è™«åˆå§‹åŒ–å¤±è´¥: {e}")
        else:
            logger.info("â—‹ Alpha Vantage çˆ¬è™«å·²ç¦ç”¨")
        
        logger.info(f"\nå·²å¯ç”¨çˆ¬è™«æ•°é‡: {len(self.crawlers)}/5")
    
    def run_crawler(self, name: str) -> Dict:
        """
        è¿è¡Œå•ä¸ªçˆ¬è™«
        
        Args:
            name: çˆ¬è™«åç§° (reddit/newsapi/rss/stocktwits/alphavantage)
        
        Returns:
            dict: ç»Ÿè®¡ä¿¡æ¯
        """
        if name not in self.crawlers:
            logger.warning(f"çˆ¬è™« {name} æœªå¯ç”¨æˆ–ä¸å­˜åœ¨")
            return {}
        
        logger.info("\n" + "=" * 60)
        logger.info(f"è¿è¡Œ {name.upper()} çˆ¬è™«")
        logger.info("=" * 60)
        
        try:
            stats = self.crawlers[name].crawl()
            
            if name == 'reddit':
                self.statistics['reddit']['posts'] += stats.get('posts', 0)
                self.statistics['reddit']['comments'] += stats.get('comments', 0)
                self.statistics['reddit']['errors'] += stats.get('errors', 0)
            elif name == 'newsapi':
                self.statistics['newsapi']['articles'] += stats.get('articles', 0)
                self.statistics['newsapi']['errors'] += stats.get('errors', 0)
            elif name == 'rss':
                self.statistics['rss']['articles'] += stats.get('articles', 0)
                self.statistics['rss']['errors'] += stats.get('errors', 0)
            elif name == 'stocktwits':
                self.statistics['stocktwits']['messages'] += stats.get('messages', 0)
                self.statistics['stocktwits']['errors'] += stats.get('errors', 0)
            elif name == 'alphavantage':
                self.statistics['alphavantage']['items'] += stats.get('items', 0)
                self.statistics['alphavantage']['errors'] += stats.get('errors', 0)
            
            return stats
        except Exception as e:
            logger.error(f"{name} çˆ¬è™«è¿è¡Œå¤±è´¥: {e}")
            self.statistics[name]['errors'] += 1
            return {'errors': 1}
    
    def run_all_crawlers(self):
        """è¿è¡Œæ‰€æœ‰å¯ç”¨çš„çˆ¬è™«ï¼ˆæ”¯æŒç‹¬ç«‹é—´éš”æ§åˆ¶ï¼‰"""
        logger.info("\n" + "=" * 60)
        logger.info(f"å¼€å§‹æ‰§è¡ŒæŠ“å–ä»»åŠ¡ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info("=" * 60)
        
        current_time = time.time()
        crawler_control = self.config.get('crawler_control', {})
        individual_intervals = crawler_control.get('individual_intervals', {})
        
        for name in ['reddit', 'newsapi', 'rss', 'stocktwits', 'twitter', 'alphavantage']:
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†è¯¥çˆ¬è™«
            if name not in self.crawlers:
                continue
            
            # è·å–è¯¥çˆ¬è™«çš„ç‹¬ç«‹é—´éš”
            crawler_interval = individual_intervals.get(name, 0)
            
            # å¦‚æœè®¾ç½®äº†ç‹¬ç«‹é—´éš”ï¼Œæ£€æŸ¥æ˜¯å¦åˆ°è¾¾è¿è¡Œæ—¶é—´
            if crawler_interval > 0:
                time_since_last_run = current_time - self.last_run_times[name]
                if time_since_last_run < crawler_interval:
                    logger.info(f"â­ï¸  {name.upper()} - è·³è¿‡ï¼ˆè·ä¸Šæ¬¡è¿è¡Œ {time_since_last_run:.0f}ç§’ï¼Œé—´éš” {crawler_interval}ç§’ï¼‰")
                    continue
            
            # è¿è¡Œçˆ¬è™«
            self.run_crawler(name)
            self.last_run_times[name] = current_time
        
        # æ•°æ®å¯¼å‡ºï¼ˆé˜²æ­¢ Redis å†…å­˜å ç”¨è¿‡å¤§ï¼‰
        self._export_data()
        
        # æ‰“å°æ€»ä½“ç»Ÿè®¡
        self._print_statistics()
        
        # å‘é€çˆ¬å–å®Œæˆé€šçŸ¥
        self._send_completion_notification()
    
    def _export_data(self):
        """æ ¹æ®é˜ˆå€¼å¯¼å‡ºæ•°æ®åˆ°æ–‡ä»¶"""
        try:
            data_cfg = self.config.get('data_management', {})
            redis_cfg = self.config.get('redis', {})

            # ä½¿ç”¨ redis.storage_optimization.max_keep ä½œä¸ºä¿ç•™ä¸Šé™
            max_keep = int((redis_cfg.get('storage_optimization') or {}).get('max_keep', 10000))

            auto_export = (data_cfg.get('auto_export') or {})
            queue_threshold = int(auto_export.get('queue_threshold', max_keep))
            memory_threshold_mb = int(auto_export.get('memory_threshold_mb', 0))

            logger.info("\n" + "=" * 60)
            logger.info("æ£€æŸ¥æ˜¯å¦éœ€è¦å¯¼å‡ºæ•°æ®")
            logger.info("=" * 60)

            # æ¡ä»¶1ï¼šé˜Ÿåˆ—é•¿åº¦
            current_length = self.redis_client.get_queue_length()
            need_export = current_length > max(queue_threshold, max_keep)

            # æ¡ä»¶2ï¼šå†…å­˜é˜ˆå€¼
            if memory_threshold_mb > 0:
                mem = self.redis_client.get_memory_usage()
                if mem.get('used_memory_mb', 0) >= memory_threshold_mb:
                    logger.info(f"å†…å­˜é˜ˆå€¼è§¦å‘ï¼š{mem.get('used_memory_mb',0):.2f}MB â‰¥ {memory_threshold_mb}MB")
                    need_export = True

            logger.info(f"å½“å‰ Redis é˜Ÿåˆ—é•¿åº¦: {current_length}")

            if need_export:
                exporter = DataExporter(
                    self.redis_client,
                    export_dir=data_cfg.get('export_dir', 'data_exports'),
                )
                export_stats = exporter.export_and_trim(max_keep=max_keep)

                if export_stats.get('exported', 0) > 0:
                    logger.info(f"âœ“ å·²å¯¼å‡º {export_stats['exported']} æ¡æ•°æ®åˆ° {export_stats['export_file']}")
                    logger.info(f"âœ“ Redis é˜Ÿåˆ—å·²ä¿®å‰ªè‡³ {max_keep} æ¡")
            else:
                logger.info(f"â—‹ é˜Ÿåˆ—é•¿åº¦æœªè¶…è¿‡é˜ˆå€¼ï¼ˆqueue:{queue_threshold} / keep:{max_keep}ï¼‰ï¼Œè·³è¿‡å¯¼å‡º")
        except Exception as e:
            logger.error(f"âœ— æ•°æ®å¯¼å‡ºå¤±è´¥: {e}")
    
    def _print_statistics(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        logger.info("\n" + "=" * 60)
        logger.info("æŠ“å–ä»»åŠ¡ç»Ÿè®¡")
        logger.info("=" * 60)
        
        total_items = 0
        total_errors = 0
        
        # Reddit
        reddit_total = self.statistics['reddit']['posts'] + self.statistics['reddit']['comments']
        total_items += reddit_total
        total_errors += self.statistics['reddit']['errors']
        logger.info(f"Reddit:      å¸–å­ {self.statistics['reddit']['posts']}, "
                   f"è¯„è®º {self.statistics['reddit']['comments']}, "
                   f"é”™è¯¯ {self.statistics['reddit']['errors']}")
        
        # NewsAPI
        total_items += self.statistics['newsapi']['articles']
        total_errors += self.statistics['newsapi']['errors']
        logger.info(f"NewsAPI:     æ–‡ç«  {self.statistics['newsapi']['articles']}, "
                   f"é”™è¯¯ {self.statistics['newsapi']['errors']}")
        
        # RSS
        total_items += self.statistics['rss']['articles']
        total_errors += self.statistics['rss']['errors']
        logger.info(f"RSS:         æ–‡ç«  {self.statistics['rss']['articles']}, "
                   f"é”™è¯¯ {self.statistics['rss']['errors']}")
        
        # StockTwits
        total_items += self.statistics['stocktwits']['messages']
        total_errors += self.statistics['stocktwits']['errors']
        logger.info(f"StockTwits:  æ¶ˆæ¯ {self.statistics['stocktwits']['messages']}, "
                   f"é”™è¯¯ {self.statistics['stocktwits']['errors']}")
        
        # Alpha Vantage
        total_items += self.statistics['alphavantage']['items']
        total_errors += self.statistics['alphavantage']['errors']
        logger.info(f"AlphaVantage: æ•°æ® {self.statistics['alphavantage']['items']}, "
                   f"é”™è¯¯ {self.statistics['alphavantage']['errors']}")
        
        logger.info("-" * 60)
        logger.info(f"æ€»è®¡:        æ•°æ® {total_items}, é”™è¯¯ {total_errors}")
        logger.info(f"é˜Ÿåˆ—é•¿åº¦:    {self.redis_client.get_queue_length()}")
        logger.info("=" * 60)
    
    def _clean_old_data(self, hours=24) -> dict:
        """
        æ¸…ç†è¶…è¿‡æŒ‡å®šå°æ—¶æ•°çš„æ—§æ•°æ®
        
        Args:
            hours: ä¿ç•™æ•°æ®çš„å°æ—¶æ•°
            
        Returns:
            dict: æ¸…ç†ç»Ÿè®¡ä¿¡æ¯
        """
        redis_config = self.config.get('redis', {})
        host = redis_config.get('host', 'localhost')
        port = redis_config.get('port', 6379)
        db = redis_config.get('db', 0)
        queue_name = redis_config.get('queue', 'data_queue')
        
        try:
            r = redis.Redis(host=host, port=port, db=db, decode_responses=True)
            r.ping()
            
            # è®¡ç®—æˆªæ­¢æ—¶é—´
            cutoff_time = datetime.now() - timedelta(hours=hours)
            cutoff_timestamp = int(cutoff_time.timestamp())
            
            total = r.llen(queue_name)
            removed = 0
            
            # ä»é˜Ÿåˆ—å°¾éƒ¨ï¼ˆæœ€æ—§ï¼‰å¼€å§‹æ£€æŸ¥
            while True:
                item = r.lindex(queue_name, -1)
                if not item:
                    break
                
                try:
                    data = json.loads(item)
                    item_timestamp = data.get('timestamp', 0)
                    
                    if item_timestamp < cutoff_timestamp:
                        r.rpop(queue_name)
                        removed += 1
                    else:
                        break
                except:
                    # æ— æ³•è§£æçš„æ•°æ®ï¼Œåˆ é™¤
                    r.rpop(queue_name)
                    removed += 1
            
            remaining = r.llen(queue_name)
            
            logger.info(f"âœ“ æ¸…ç†å®Œæˆ: åˆ é™¤ {removed} æ¡æ—§æ•°æ®ï¼Œä¿ç•™ {remaining} æ¡")
            
            return {
                'removed': removed,
                'remaining': remaining
            }
            
        except Exception as e:
            logger.error(f"æ¸…ç†æ—§æ•°æ®å¤±è´¥: {e}")
            return {'removed': 0, 'remaining': 0, 'error': str(e)}
    
    def _send_completion_notification(self):
        """å‘é€çˆ¬å–å®Œæˆé€šçŸ¥ï¼ˆæ¸…ç†æ—§æ•°æ®åï¼‰"""
        notification_config = self.config.get('redis', {}).get('notification', {})
        
        if not notification_config.get('enabled', False):
            return
        
        # å…ˆæ¸…ç†è¶…è¿‡ 24 å°æ—¶çš„æ—§æ•°æ®
        logger.info("\nğŸ§¹ æ¸…ç†è¶…è¿‡ 24 å°æ—¶çš„æ—§æ•°æ®...")
        clean_result = self._clean_old_data()
        
        channel = notification_config.get('channel', 'crawler_complete')
        
        # ç»Ÿè®¡æ€»æ•°æ®é‡
        total_items = (
            self.statistics['reddit']['posts'] + 
            self.statistics['reddit']['comments'] +
            self.statistics['newsapi']['articles'] +
            self.statistics['rss']['articles'] +
            self.statistics['stocktwits']['messages'] +
            self.statistics['alphavantage']['items']
        )
        
        total_errors = sum(
            self.statistics[source]['errors'] 
            for source in ['reddit', 'newsapi', 'rss', 'stocktwits', 'alphavantage']
        )
        
        # æ„å»ºé€šçŸ¥æ¶ˆæ¯
        message = {
            'event': 'crawl_complete',
            'timestamp': datetime.now().isoformat(),
            'statistics': {
                'total_items': total_items,
                'total_errors': total_errors,
                'queue_length': self.redis_client.get_queue_length(),
                'by_source': {
                    'reddit': {
                        'posts': self.statistics['reddit']['posts'],
                        'comments': self.statistics['reddit']['comments'],
                        'errors': self.statistics['reddit']['errors']
                    },
                    'newsapi': {
                        'articles': self.statistics['newsapi']['articles'],
                        'errors': self.statistics['newsapi']['errors']
                    },
                    'rss': {
                        'articles': self.statistics['rss']['articles'],
                        'errors': self.statistics['rss']['errors']
                    },
                    'stocktwits': {
                        'messages': self.statistics['stocktwits']['messages'],
                        'errors': self.statistics['stocktwits']['errors']
                    },
                    'alphavantage': {
                        'items': self.statistics['alphavantage']['items'],
                        'errors': self.statistics['alphavantage']['errors']
                    }
                }
            }
        }
        
        # å‘é€é€šçŸ¥
        self.redis_client.publish_notification(channel, message)
    
    def get_status(self) -> Dict:
        """
        è·å–æ§åˆ¶ä¸­å¿ƒçŠ¶æ€
        
        Returns:
            dict: çŠ¶æ€ä¿¡æ¯
        """
        return {
            'enabled_crawlers': list(self.crawlers.keys()),
            'statistics': self.statistics,
            'redis_queue_length': self.redis_client.get_queue_length(),
            'timestamp': datetime.now().isoformat()
        }
    
    def close(self):
        """å…³é—­æ‰€æœ‰è¿æ¥"""
        if self.redis_client:
            self.redis_client.close()
            logger.info("Redis è¿æ¥å·²å…³é—­")


def main():
    """ä¸»å‡½æ•° - æ”¯æŒå•æ¬¡è¿è¡Œæˆ–å¾ªç¯æ¨¡å¼"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='è´¢ç»æ•°æ®çˆ¬è™«æ§åˆ¶ä¸­å¿ƒ')
    parser.add_argument('--loop', action='store_true', help='å¾ªç¯è¿è¡Œæ¨¡å¼')
    parser.add_argument('--interval', type=int, default=None, 
                        help='å¾ªç¯é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ crawler_control.loop_interval')
    args = parser.parse_args()
    
    try:
        # åˆ›å»ºæ§åˆ¶ä¸­å¿ƒ
        center = CrawlerControlCenter()
        
        if args.loop:
            # è·å–å¾ªç¯é—´éš”ï¼ˆä¼˜å…ˆä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼Œå…¶æ¬¡ä½¿ç”¨é…ç½®æ–‡ä»¶ï¼‰
            if args.interval is not None:
                interval = args.interval
            else:
                interval = center.config.get('crawler_control', {}).get('loop_interval', 300)
            
            # å¾ªç¯æ¨¡å¼
            logger.info("=" * 60)
            logger.info(f"å¯åŠ¨å¾ªç¯æ¨¡å¼ - é—´éš”: {interval} ç§’ ({interval/60:.1f} åˆ†é’Ÿ)")
            
            # æ˜¾ç¤ºå„çˆ¬è™«çš„ç‹¬ç«‹é—´éš”é…ç½®
            individual_intervals = center.config.get('crawler_control', {}).get('individual_intervals', {})
            if individual_intervals:
                logger.info("\nç‹¬ç«‹é—´éš”æ§åˆ¶:")
                for name, crawler_interval in individual_intervals.items():
                    if crawler_interval > 0 and name in center.crawlers:
                        logger.info(f"  {name:15s}: æ¯ {crawler_interval:4d} ç§’ ({crawler_interval/60:.1f} åˆ†é’Ÿ)")
            
            logger.info("\næŒ‰ Ctrl+C åœæ­¢")
            logger.info("=" * 60)
            
            run_count = 0
            try:
                while True:
                    run_count += 1
                    logger.info(f"\n{'='*60}")
                    logger.info(f"ç¬¬ {run_count} æ¬¡è¿è¡Œ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
                    logger.info(f"{'='*60}\n")
                    
                    # è¿è¡Œæ‰€æœ‰çˆ¬è™«
                    center.run_all_crawlers()
                    
                    # è®¡ç®—ä¸‹æ¬¡è¿è¡Œæ—¶é—´
                    next_run = datetime.now() + timedelta(seconds=interval)
                    logger.info(f"\n{'='*60}")
                    logger.info(f"ç­‰å¾… {interval} ç§’ ({interval/60:.1f} åˆ†é’Ÿ)")
                    logger.info(f"ä¸‹æ¬¡è¿è¡Œæ—¶é—´: {next_run.strftime('%Y-%m-%d %H:%M:%S')}")
                    logger.info(f"æŒ‰ Ctrl+C åœæ­¢å¾ªç¯")
                    logger.info(f"{'='*60}\n")
                    
                    time.sleep(interval)
                    
            except KeyboardInterrupt:
                logger.info("\n\n" + "="*60)
                logger.info("æ”¶åˆ°åœæ­¢ä¿¡å· (Ctrl+C)")
                logger.info(f"æ€»å…±è¿è¡Œäº† {run_count} æ¬¡")
                logger.info("="*60)
        else:
            # å•æ¬¡è¿è¡Œæ¨¡å¼
            center.run_all_crawlers()
        
        # å…³é—­è¿æ¥
        center.close()
        
    except KeyboardInterrupt:
        logger.info("\nç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
